
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>目标检测二十年：一项综述 &#8212; 论文阅读笔记</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/plot_directive.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../../_static/documentation_options.js?v=40d2fe7a"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Notes/detection/目标检测综述20年';</script>
    <link rel="icon" href="../../_static/panda.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="论文阅读笔记 - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="论文阅读笔记 - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">目录</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Method/index.html">论文阅读指南</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Method/efficent_read_paper.html">高效阅读方法及流程</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Method/how_to_read_paper.html">如何阅读论文</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Method/paper_10_question.html">论文速读十问</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Method/read_important_tips.html">读论文与口头报告的几项重点</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Method/reference.html">参考材料</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../List/index.html">论文阅读清单</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../List/basis.html">神经网络基础(basis)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../List/attention.html">注意力部分(attention)</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../List/batch_normalization.html">批量&amp;正则化(batch&amp;normalization)</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../List/classification.html">图像分类(CLAS)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../List/convolutional.html">高级卷积网络知识(Convolutional)</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../List/gan.html">AI合成部分(GAN)</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../List/nlp.html">自然语言处理(NLP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../List/objectdetection.html">目标检测(OBJ)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../List/rnn.html">循环神经网络(RNN)</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../List/segementation.html">目标分割(SEG)</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../List/transformer.html">Transformer</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../List/multimodal.html">多模态(MultiModal Learning)</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../List/llm.html">大语言模型(Large Language Models)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../index.html">论文阅读笔记</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../mm-l/index.html">多模态(MultiModal Machine Learning)</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mm-l/blip-v1.html">BLIP: Bootstrapping Language-Image Pre-training</a></li>









<li class="toctree-l3"><a class="reference internal" href="../mm-l/blip-v2.html">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</a></li>

</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../llm/index.html">大语言模型(Large Language Models)</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../llm/opt.html">OPT: OPT : Open Pre-trained Transformer Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../llm/gpt-v1.html">GPT-v1:Improving Language Understanding by Generative Pre-Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../llm/gpt-v2.html">GPT-v2:Language Models are Unsupervised Multitask Learners</a></li>
<li class="toctree-l3"><a class="reference internal" href="../llm/gpt-v3.html">GPT-v3:Language Models are Few-Shot Learners</a></li>
<li class="toctree-l3"><a class="reference internal" href="../llm/gpt-v4.html">GPT-v4:GPT-4 Technical Report</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Read/index.html">论文阅读记录</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul class="simple">
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Summary/index.html">论文阅读总结</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul class="simple">
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/isLinXu/paper-read-notes" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/isLinXu/paper-read-notes/edit/main/Notes/detection/目标检测综述20年.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/isLinXu/paper-read-notes/issues/new?title=Issue%20on%20page%20%2FNotes/detection/目标检测综述20年.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/Notes/detection/目标检测综述20年.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>目标检测二十年：一项综述</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#"><strong>目标检测二十年：一项综述</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2"><strong>摘要</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id3"><strong>1 引言</strong></a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id4"><strong>2 目标检测二十年</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5"><strong>2.1 目标检测的路线图</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6"><strong>2.1.1 里程碑：传统检测器</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cnn"><strong>2.1.2 里程碑：基于 CNN 的两阶段检测器</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7"><strong>2.2 目标检测数据集和评价指标</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8"><strong>2.2.1 评价指标</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id9"><strong>2.3 目标检测中的技术演变</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10"><strong>2.3.1 早期的暗知识</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11"><strong>2.3.2 多尺度检测的技术演变</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12"><strong>2.3.3 边界框回归的技术演变</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13"><strong>2.3.4 上下文引导的技术演变</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14"><strong>2.3.5 非极大值抑制的技术演变</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id15"><strong>3 加速检测</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id16"><strong>3.1 特征图共享计算</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17"><strong>3.1.1 空间计算冗余和加速</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18"><strong>3.1.2 尺度计算冗余和加速</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id19"><strong>3.2 加速分类器</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id20"><strong>3.3 级联检测</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id21"><strong>3.4 网络剪枝和量化</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id22"><strong>3.4.1 网络剪枝</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id23"><strong>3.4.2 网络量化</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id24"><strong>3.4.3 网络蒸馏</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id25"><strong>3.5 轻量级网络设计</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id26"><strong>3.5.1 分解卷积</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id27"><strong>3.5.2 组卷积</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id28"><strong>3.5.3 深度可分离卷积</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id29"><strong>3.5.4 瓶颈设计</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id30"><strong>3.5.5 神经架构搜索</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id31"><strong>3.6 数值加速</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id32"><strong>3.6.1 使用积分图像加速</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id33"><strong>3.6.2 在频域中加速</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id34"><strong>3.6.3向量量化</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id35"><strong>3.6.4 降秩近似</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id36"><strong>4 最近目标检测的进展</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id37"><strong>4.1 使用更好的引擎进行检测</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id38"><strong>4.2 使用更好的特征进行检测</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id39"><strong>4.2.1 为什么特征融合很重要？</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id40"><strong>4.2.2 不同方式的特征融合</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id41"><strong>4.2.3 学习具有大接受场的高分辨率特征</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id42"><strong>4.3 非滑动窗口检测</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id43"><strong>4.4 提高定位的改进</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id44"><strong>4.4.1 边界框细化</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id45"><strong>4.4.2 改进损失函数以实现精确定位</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id46"><strong>4.5 与分割结合的学习</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id47"><strong>4.5.1 为什么分割改进检测？</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id48"><strong>4.5.2 分割如何改进检测？</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id49"><strong>4.6 旋转和平移变化的鲁棒检测</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id50"><strong>4.6.1 旋转鲁棒检测</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id51"><strong>4.6.2 平移鲁棒检测</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id52"><strong>4.7 从头开始训练</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id53"><strong>4.8 对抗性训练</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id54"><strong>4.9 弱监督目标检测</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id55"><strong>5 应用</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id56"><strong>5.1 行人检测</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id57"><strong>5.1.1 困难和挑战</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id58"><strong>5.1.2 文献综述</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id59"><strong>5.2 面部检测</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id60"><strong>5.2.1 困难和挑战</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id61"><strong>5.2.2 文献综述</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id62"><strong>5.3 文本检测</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id63"><strong>5.3.1 困难和挑战</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id64"><strong>5.3.2 文献综述</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id65"><strong>5.4 交通标志和交通信号检测</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id66"><strong>5.4.1 困难和挑战</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id67"><strong>5.4.2 文献综述</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id68"><strong>5.5 遥感目标检测</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id69"><strong>5.5.1 困难和挑战</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id70"><strong>5.5.2 文献综述</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id71"><strong>6 结论和未来方向</strong></a></li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="id1">
<h1><strong>目标检测二十年：一项综述</strong><a class="headerlink" href="#id1" title="Link to this heading">#</a></h1>
<section id="id2">
<h2><strong>摘要</strong><a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<p>— 目标检测是计算机视觉中最基本的挑战之一，近年来受到了极大的关注。过去二十年的发展可以视为计算机视觉历史的缩影。如果我们将今天的目标检测视为深度学习技术美学的体现，那么回溯到20年前，我们将见证“冷兵器时代”的智慧。本文广泛回顾了400多篇目标检测论文，涵盖了四分之一世纪的时间跨度（从1990年代到2019年），技术演变的各个方面。本文涵盖的主题包括历史上的里程碑式检测器、检测数据集、评价指标、检测系统的基本构建块、加速技术，以及最近的最先进检测方法。本文还回顾了一些重要的检测应用，如行人检测、面部检测、文本检测等，并深入分析了这些应用近年来面临的挑战和技术改进。</p>
<p><strong>索引术语</strong> — 目标检测，计算机视觉，深度学习，卷积神经网络，技术演变。</p>
</section>
</section>
<section id="id3">
<h1><strong>1 引言</strong><a class="headerlink" href="#id3" title="Link to this heading">#</a></h1>
<p>目标检测是计算机视觉中的一个重要任务，涉及在数字图像中检测特定类别的视觉对象实例（如人类、动物或汽车）。目标检测的目标是开发计算模型和技术，提供计算机视觉应用所需的最基本信息：哪些对象在哪里？作为计算机视觉的一个基本问题，目标检测构成了许多其他计算机视觉任务的基础，如实例分割[1-4]、图像字幕[5-7]、目标跟踪[8]等。从应用的角度来看，目标检测可以分为两个研究主题：“通用目标检测”和“检测应用”，前者旨在探索在统一框架下检测不同类型对象的方法以模拟人类视觉和认知，后者指的是在特定应用场景下的检测，如行人检测、面部检测、文本检测等。近年来，深度学习技术的快速发展为目标检测注入了新的活力，取得了显著的突破，并将其推向了前所未有的研究热点。目标检测现在已经广泛应用于许多现实世界的应用中，如自动驾驶、机器人视觉、视频监控等。图1显示了过去几十年与“目标检测”相关的出版物数量的增长。</p>
</section>
<section id="id4">
<h1><strong>2 目标检测二十年</strong><a class="headerlink" href="#id4" title="Link to this heading">#</a></h1>
<p>在本节中，我们将从多个方面回顾目标检测的历史，包括里程碑检测器、目标检测数据集、评价指标和关键技术的演变。</p>
<section id="id5">
<h2><strong>2.1 目标检测的路线图</strong><a class="headerlink" href="#id5" title="Link to this heading">#</a></h2>
<p>在过去的二十年中，人们普遍认为目标检测的进展大致经历了两个历史时期：“传统目标检测时期（2014年之前）”和“基于深度学习的目标检测时期（2014年之后）”，如图2所示。</p>
<section id="id6">
<h3><strong>2.1.1 里程碑：传统检测器</strong><a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<p>如果我们将今天的目标检测视为深度学习技术美学的体现，那么回溯到20年前，我们将见证“冷兵器时代”的智慧。大多数早期的目标检测算法都是基于手工设计的特征构建的。由于当时缺乏有效的图像表示，人们不得不设计复杂的特征表示，以及各种加速技能，以充分利用有限的计算资源。</p>
<ul class="simple">
<li><p><strong>Viola Jones 检测器</strong> 18年前，P. Viola 和 M. Jones 首次实现了无需任何限制（例如，肤色分割）的实时人脸检测[10, 11]。在700MHz的Pentium III CPU上运行，该检测器的检测速度比当时任何其他算法快数十甚至数百倍，具有可比的检测准确性。后来被称为“Viola-Jones (VJ) 检测器”的检测算法，以作者的名字命名，以纪念他们的重要贡献。VJ检测器遵循最直接的检测方式，即滑动窗口：遍历图像中所有可能的位置和尺度，查看任何窗口是否包含人脸。尽管看起来是一个非常简单的过程，但背后的计算远远超出了当时计算机的能力。VJ检测器通过整合三种重要技术显著提高了其检测速度：“积分图像”、“特征选择”和“检测级联”。1) 积分图像：积分图像是一种计算方法，用于加速框滤波或卷积过程。像当时其他目标检测算法[29-31]一样，Haar小波被用作VJ检测器的图像特征表示。积分图像使VJ检测器中每个窗口的计算复杂性与其窗口大小无关。2) 特征选择：与使用一组手动选择的Haar基滤波器不同，作者使用Adaboost算法[32]从大约180k维的随机特征池中选择一小组最有助于面部检测的特征。3) 检测级联：在VJ检测器中引入了多阶段检测范式（又称为“检测级联”），通过在背景窗口上减少计算量，而在面部目标上增加计算量，从而减少了其计算开销。</p></li>
<li><p><strong>HOG 检测器</strong> 梯度直方图（Histogram of Oriented Gradients，HOG）特征描述符最初由 N. Dalal 和 B. Triggs 在 2005 年提出[12]。HOG 可以被认为是当时尺度不变特征变换[33, 34]和形状上下文[35]的重要改进。为了平衡特征不变性（包括平移、尺度、照明等）和非线性（在区分不同对象类别方面），HOG 描述符被设计为在均匀间隔的密集网格单元上计算，并使用重叠的局部对比度归一化（在“块”上）来提高准确性。尽管 HOG 可以用来检测各种对象类别，但它主要是针对行人检测问题提出的。为了检测不同大小的对象，HOG 检测器在保持检测窗口大小不变的情况下多次重新缩放输入图像。HOG 检测器长期以来一直是许多目标检测器[13, 14, 36]和多年来各种计算机视觉应用的重要基础。</p></li>
<li><p><strong>可变形部件模型（Deformable Part-based Model，DPM）</strong> 作为 VOC-07、-08 和 -09 检测挑战的获胜者，DPM 是传统目标检测方法的顶峰。DPM 最初由 P. Felzenszwalb 在 2008 年提出[13]，作为 HOG 检测器的扩展，然后 R. Girshick 对其进行了各种改进[14, 15, 37, 38]。DPM 遵循“分而治之”的检测理念，其中训练可以简单地视为学习适当分解对象的方式，而推理可以被视为对不同对象部分的检测集合。例如，检测“汽车”的问题可以被视为检测其窗户、车身和轮子。这项工作的这一部分，即“星型模型”，由 P. Felzenszwalb 等人完成[13]。后来，R. Girshick 将星型模型进一步扩展到“混合模型”[14, 15, 37, 38]，以处理现实世界中更具显著变化的对象。一个典型的 DPM 检测器由一个根过滤器和多个部分过滤器组成。与手动指定部分过滤器的配置（例如，大小和位置）不同，DPM 开发了一个弱监督学习方法，其中所有部分过滤器的配置都可以作为潜在变量自动学习。R. Girshick 进一步将这一过程形式化为多实例学习的特例[39]，并且一些其他重要技术，如“硬负挖掘”、“边界框回归”和“上下文引导”，也被应用于提高检测准确性（在第2.3节中介绍）。为了加速检测，Girshick 开发了一个技术，用于将检测模型“编译”成一个更快的模型，实现了级联架构，加速了超过10倍而没有任何准确性的牺牲[14, 38]。尽管今天的物体检测器在检测准确性方面已经远远超过了 DPM，但它们仍然深受其宝贵见解的影响，例如混合模型、硬负挖掘、边界框回归等。2010年，P. Felzenszwalb 和 R. Girshick 被 PASCAL VOC 授予“终身成就奖”。</p></li>
</ul>
</section>
<section id="cnn">
<h3><strong>2.1.2 里程碑：基于 CNN 的两阶段检测器</strong><a class="headerlink" href="#cnn" title="Link to this heading">#</a></h3>
<p>随着手工特征的性能饱和，目标检测在 2010 年后达到了一个平台期。R. Girshick 说：“…2010-2012 年间进展缓慢，通过构建集成系统和使用成功方法的微小变种获得了小幅增益”[38]。2012年，世界见证了卷积神经网络的重生[40]。由于深度卷积网络能够学习图像的鲁棒和高层特征表示，一个自然的问题是我们是否可以将其应用于目标检测？R. Girshick 等人在 2014 年通过提出具有 CNN 特征的区域（RCNN）进行目标检测，从而打破了僵局[16, 41]。从那时起，目标检测开始以前所未有的速度发展。在深度学习时代，目标检测可以分为两种类型：“两阶段检测”和“一阶段检测”，前者将检测框架为“粗到细”的过程，而后者则将其框架为“一步完成”。</p>
<ul class="simple">
<li><p><strong>RCNN</strong> RCNN 的理念很简单：它首先通过选择性搜索[42]提取一组对象提议（对象候选框）。然后，将每个提议缩放到固定大小的图像，并输入到在 ImageNet（例如，AlexNet[40]）上训练的 CNN 模型中以提取特征。最后，使用线性 SVM 分类器预测每个区域内是否存在对象，并识别对象类别。RCNN 在 VOC07 上取得了显著的性能提升，平均精度均值（mAP）从 33.7%（</p></li>
<li><p>DPM-v5 [43]）提高到 58.5%。尽管 RCNN 取得了巨大的进步，但它的缺点是显而易见的：对大量重叠提议（一张图片上有超过 2000 个盒子）进行冗余特征计算导致检测速度极慢（每张图片使用 GPU 需要 14 秒）。同年晚些时候，SPPNet [17] 被提出并克服了这个问题。</p>
<ul>
<li><p><strong>SPPNet</strong> 在 2014 年，K. He 等人提出了空间金字塔池化网络（SPPNet）[17]。以前的 CNN 模型需要固定大小的输入，例如 AlexNet [40] 需要 224x224 的图像。SPPNet 的主要贡献是引入了空间金字塔池化（SPP）层，这使得 CNN 能够生成固定长度的表示，而不管图像/感兴趣区域的大小如何，而无需重新缩放它。当使用 SPPNet 进行目标检测时，特征图只需从整个图像计算一次，然后就可以为训练检测器生成任意区域的固定长度表示，避免了重复计算卷积特征。SPPNet 的速度比 R-CNN 快 20 倍以上，而不会牺牲任何检测精度（VOC07 mAP=59.2%）。尽管 SPPNet 有效地提高了检测速度，但仍然存在一些缺点：首先，训练仍然是多阶段的；其次，SPPNet 只微调其全连接层，而简单地忽略了所有之前的层。下一年，Fast RCNN [18] 被提出并解决了这些问题。</p></li>
<li><p><strong>Fast RCNN</strong> 在 2015 年，R. Girshick 提出了 Fast RCNN 检测器 [18]，这是 R-CNN 和 SPPNet [16, 17] 的进一步改进。Fast RCNN 允许我们在同一网络配置下同时训练检测器和边界框回归器。在 VOC07 数据集上，Fast RCNN 将 mAP 从 RCNN 的 58.5% 提高到 70.0%，同时检测速度比 R-CNN 快 200 倍以上。尽管 Fast-RCNN 成功地整合了 R-CNN 和 SPPNet 的优势，但其检测速度仍受提议检测的限制（见第 2.3.2 节的更多细节）。然后，一个自然产生的问题：“我们能否使用 CNN 模型生成对象提议？”后来，Faster R-CNN [19] 回答了这个问题。</p></li>
<li><p><strong>Faster RCNN</strong> 在 2015 年，S. Ren 等人提出了 Faster RCNN 检测器 [19, 44]，紧随 Fast RCNN 之后。Faster RCNN 是第一个端到端的、近乎实时的深度学习检测器（COCO mAP&#64;.5=42.7%，COCO mAP&#64;[.5,.95]=21.9%，VOC07 mAP=73.2%，VOC12 mAP=70.4%，使用 ZFNet [45] 时为 17fps）。Faster RCNN 的主要贡献是引入了区域提议网络（RPN），实现了几乎无成本的区域提议。从 R-CNN 到 Faster RCNN，目标检测系统的大多数单独模块，例如提议检测、特征提取、边界框回归等，已经逐步集成到一个统一的、端到端的学习框架中。尽管 Faster RCNN 突破了 Fast RCNN 的速度瓶颈，但在后续检测阶段仍然存在计算冗余。后来，提出了多种改进，包括 RFCN [46] 和轻头部 RCNN [47]。（详见第 3 节）。</p></li>
<li><p><strong>特征金字塔网络</strong> 在 2017 年，T.-Y. Lin 等人提出了特征金字塔网络（FPN）[22]，基于 Faster RCNN。在 FPN 之前，大多数基于深度学习的检测器只在网络的顶层运行检测。尽管 CNN 更深层的特征对类别识别有利，但不利于定位对象。为此，在 FPN 中开发了一种具有侧向连接的自顶向下架构，用于构建所有尺度上的高层语义。由于 CNN 通过其前向传播自然形成特征金字塔，FPN 在检测各种尺度的对象方面表现出色。在基本的 Faster R-CNN 系统中使用 FPN，它在没有花哨功能的情况下实现了 MSCOCO 数据集上的最先进的单模型检测结果（COCO mAP&#64;.5=59.1%，COCO mAP&#64;[.5, .95]=36.2%）。FPN 现在已成为许多最新检测器的基本构建块。 <strong>2.1.3 里程碑：基于 CNN 的一阶段检测器</strong></p></li>
<li><p><strong>You Only Look Once (YOLO)</strong> YOLO 由 R. Joseph 等人在 2015 年提出。它是深度学习时代的第一个一阶段检测器[20]。YOLO 非常快：一个快速版本的 YOLO 以 155fps 运行，VOC07 mAP=52.7%，而其增强版本以 45fps 运行，VOC07 mAP=63.4% 和 VOC12 mAP=57.9%。YOLO 是 “You Only Look Once” 的缩写。从它的名字可以看出，作者完全放弃了以前的 “提议检测 + 验证” 的检测范式。相反，它遵循一种完全不同的哲学：将一个单一的神经网络应用于完整图像。这个网络将图像划分为区域，并同时为每个区域预测边界框和概率。后来，R. Joseph 在 YOLO 的基础上进行了一系列的改进，并提出了其 v2 和 v3 版本[48, 49]，进一步提高了检测准确性，同时保持了非常高的检测速度。尽管 YOLO 在检测速度上有了很大改进，但与两阶段检测器相比，其定位准确性有所下降，特别是对于一些小对象。YOLO 的后续版本[48, 49]和后来提出的 SSD [21]更加关注了这个问题。</p></li>
<li><p><strong>Single Shot MultiBox Detector (SSD)</strong> SSD [21] 由 W. Liu 等人在 2015 年提出。它是深度学习时代第二个一阶段检测器。SSD 的主要贡献是引入了多参考和多分辨率检测技术（将在第 2.3.2 节中介绍），这显著提高了一阶段检测器的检测准确性，特别是对于一些小对象。SSD 在检测速度和准确性方面都具有优势（VOC07 mAP=76.8%，VOC12 mAP=74.9%，COCO mAP&#64;.5=46.5%，mAP&#64;[.5,.95]=26.8%，一个快速版本以 59fps 运行）。SSD 与任何先前检测器的主要区别在于，前者在网络的不同层检测不同尺度的对象，而后者只在其顶层运行检测。</p></li>
<li><p><strong>RetinaNet</strong> 尽管一阶段检测器的速度很高且简单，但多年来它们的准确性一直落后于两阶段检测器。T.-Y. Lin 等人发现了背后的原因，并在 2017 年提出了 RetinaNet [23]。他们声称，在密集检测器训练期间遇到的极端前景-背景类别不平衡是核心原因。为此，在 RetinaNet 中引入了一种名为 “焦点损失” 的新损失函数，通过重塑标准交叉熵损失，使检测器在训练期间更多地关注难以分类的示例。焦点损失使一阶段检测器能够实现与两阶段检测器相当的准确性，同时保持非常高的检测速度。（COCO mAP&#64;.5=59.1%，mAP&#64;[.5, .95]=39.1%）。</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="id7">
<h2><strong>2.2 目标检测数据集和评价指标</strong><a class="headerlink" href="#id7" title="Link to this heading">#</a></h2>
<p>构建更少偏见的更大数据集对于发展先进的计算机视觉算法至关重要。在目标检测中，过去 10 年中发布了许多著名的数据集和基准，包括 PASCAL VOC Challenges [50, 51]（例如，VOC2007, VOC2012）、ImageNet Large Scale Visual Recognition Challenge（例如，ILSVRC2014）[52]、MS-COCO Detection Challenge [53] 等。这些数据集的统计数据在表 1 中给出。图 4 显示了这些数据集的一些图像示例。图 3 显示了从 2008 年到 2018 年在 VOC07、VOC12 和 MS-COCO 数据集上检测准确性的改进。</p>
<ul class="simple">
<li><p><strong>Pascal VOC</strong> PASCAL 视觉对象类别（VOC）挑战赛[50, 51]（从 2005 年到 2012 年）是早期计算机视觉社区中最重要的比赛之一。PASCAL VOC 包含多个任务，包括图像分类、目标检测、语义分割和动作检测。Pascal-VOC 在目标检测中主要使用两个版本：VOC07 和 VOC12，前者由 5k 训练图像 + 12k 标注对象组成，后者由 11k 训练图像 + 27k 标注对象组成。这两个数据集注释了 20 种在日常生活中常见的对象类别（人：人；动物：鸟、猫、牛、狗、马、羊；车辆：飞机、自行车、船、公共汽车、</p></li>
<li><p>汽车、摩托车、火车；室内：瓶子、椅子、餐桌、盆栽植物、沙发、电视/显示器）。近年来，随着像 ILSVRC 和 MS-COCO 这样的更大数据集的发布，VOC 逐渐过时，现在已成为大多数新检测器的测试平台。</p>
<ul>
<li><p><strong>ILSVRC</strong> ImageNet 大规模视觉识别挑战（ILSVRC）[52] 推动了通用目标检测的最新技术。ILSVRC 从 2010 年到 2017 年每年组织一次。它包含使用 ImageNet 图像[57]的检测挑战。ILSVRC 检测数据集包含 200 个类别的视觉对象。其图像/对象实例的数量比 VOC 大两个数量级。例如，ILSVRC-14 包含 517k 张图像和 534k 个注释对象。</p></li>
<li><p><strong>MS-COCO</strong> MS-COCO [53] 是目前可用的最具挑战性的目标检测数据集。基于 MS-COCO 数据集的年度比赛自 2015 年以来一直举行。与 ILSVRC 相比，它的对象类别数量较少，但对象实例更多。例如，MS-COCO-17 包含 164k 张图像和 897k 个注释对象，涵盖 80 个类别。与 VOC 和 ILSVRC 相比，MS-COCO 的最大进步是，除了边界框注释外，每个对象还使用每个实例的分割进一步标记，以帮助精确定位。此外，MS-COCO 包含的小型对象（其面积小于图像的 1%）和密集放置的对象比 VOC 和 ILSVRC 多。所有这些特点使 MSCOCO 中的对象分布更接近现实世界的对象分布。就像当时的 ImageNet 一样，MS-COCO 已成为目标检测社区的实际标准。</p></li>
<li><p><strong>Open Images</strong> 2018 年见证了 Open Images Detection (OID) 挑战的引入[58]，紧随 MS-COCO 之后，但规模前所未有。Open Images 有两个任务：1）标准目标检测，2）视觉关系检测，后者检测特定关系的成对对象。对于目标检测任务，数据集包含 1,910k 张图像，有 15,440k 个注释边界框，涵盖 600 个对象类别。</p></li>
<li><p><strong>其他检测任务的数据集</strong> 除了通用目标检测之外，过去 20 年也见证了特定领域检测应用的繁荣，如行人检测、面部检测、文本检测、交通标志/信号检测和遥感目标检测。表 2-6 列出了这些检测任务的一些流行数据集。这些任务的检测方法的详细介绍可以在第 5 节中找到。</p></li>
</ul>
</li>
</ul>
<section id="id8">
<h3><strong>2.2.1 评价指标</strong><a class="headerlink" href="#id8" title="Link to this heading">#</a></h3>
<p>我们如何评估目标检测器的有效性？这个问题在不同的时间可能有不同的答案。在早期的检测社区中，没有关于检测性能的广泛接受的评估标准。例如，在早期的行人检测研究[12]中，通常使用“错过率与每窗口的假阳性（FPPW）”作为指标。然而，每个窗口的测量（FPPW）可能存在缺陷，在某些情况下无法预测整幅图像的性能[59]。2009 年，创建了 Caltech 行人检测基准[59, 60]，从那时起，评估指标从每个窗口（FPPW）变为每图像的假阳性（FPPI）。近年来，目标检测最常用的评估是“平均精度（AP）”，最初在 VOC2007 中引入。AP 定义为在不同召回率下的平均检测精度，并且通常以类别特定的方式进行评估。为了比较所有对象类别的性能，通常使用平均所有对象类别的 mAP 作为性能的最终指标。为了测量对象定位的准确性，使用交叉点比（IoU）来检查预测框和真实框之间的 IoU 是否大于预定义的阈值，比如说 0.5。如果是肯定的，对象将被识别为“成功检测”，否则将被识别为“错过”。基于 0.5 IoU 的 mAP 已经成为多年来目标检测问题的实际标准指标。2014 年之后，由于 MS-COCO 数据集的普及，研究人员开始更多地关注边界框位置的准确性。与使用固定的 IoU 阈值不同，MS-COCO AP 是在 0.5（粗略定位）到 0.95（完美定位）之间的多个 IoU 阈值上平均的。这种指标的变化鼓励了更准确的对象定位，对于某些现实世界的应用可能非常重要（例如，想象有一个机器人手臂试图抓取扳手）。最近，在 Open Images 数据集中对评估进行了一些进一步的发展，例如，通过考虑一组盒子和非穷尽的图像级类别层次结构。一些研究人员还提出了一些替代指标，例如“定位召回精度”[94]。尽管最近有所变化，基于 VOC/COCO 的 mAP 仍然是目标检测最常用的评估指标。</p>
</section>
</section>
<section id="id9">
<h2><strong>2.3 目标检测中的技术演变</strong><a class="headerlink" href="#id9" title="Link to this heading">#</a></h2>
<p>在本节中，我们将介绍一些检测系统的重要构建块以及它们在过去 20 年中的技术演变。</p>
<section id="id10">
<h3><strong>2.3.1 早期的暗知识</strong><a class="headerlink" href="#id10" title="Link to this heading">#</a></h3>
<p>在 2000 年之前的目标检测（早期时间）并没有像滑动窗口检测那样遵循统一的检测理念。当时的检测器通常基于低级和中级视觉设计，如下所示。</p>
<ul class="simple">
<li><p><strong>组件、形状和边缘</strong> “通过组件识别”，作为一种重要的认知理论[98]，长期以来一直是图像识别和目标检测的核心思想[13, 99, 100]。一些早期研究人员将目标检测框架化为对象组件、形状和轮廓之间的相似性测量，包括距离变换[101]、形状上下文[35]和边缘元素[102]等。尽管初始结果很有希望，但在更复杂的检测问题上表现不佳，因此基于机器学习的检测方法开始流行。基于机器学习的检测经历了多个时期，包括外观的统计模型（1998 年之前）、小波特征表示（1998-2005）和基于梯度的表示（2005-2012）。构建一个对象的统计模型，如 Eigenfaces[95]（见图 5(a)），是目标检测历史上基于学习的方法的第一波。1991 年，M. Turk 等人通过使用 Eigenface 分解在实验室环境中实现了实时面部检测[95]。与当时的基于规则或基于模板的方法[107, 108]相比，统计模型通过从数据中学习任务特定的知识，更好地提供了对象外观的全面描述。</p></li>
<li><p><strong>早期时间的 CNN 用于目标检测</strong> 使用 CNN 进行目标检测的历史可以追溯到 1990 年代[96]，当时 Y. LeCun 等人做出了巨大贡献。由于计算资源的限制，当时的 CNN 模型比今天的要小得多，也要浅得多。尽管如此，计算效率仍然是早期基于 CNN 的检测模型中一个棘手的问题。Y. LeCun 等人进行了一系列改进，如“共享权重的复制神经网络”[96]和“空间位移网络”[97]，通过扩展卷积网络的每一层以覆盖整个输入图像，从而减少计算量，如图 5(b)-(c) 所示。这样，整个图像的任何位置的特征都可以通过仅进行一次网络的前向传播来提取。这可以被视为今天完全卷积网络（FCN）[110, 111]的原型，后者几乎是 20 年后提出的。CNN 也已经被应用于当时的其他任务，如面部检测[112, 113]和手部跟踪[114]。</p></li>
</ul>
</section>
<section id="id11">
<h3><strong>2.3.2 多尺度检测的技术演变</strong><a class="headerlink" href="#id11" title="Link to this heading">#</a></h3>
<p>多尺度检测不同大小和不同长宽比的对象是目标检测中的主要技术挑战之一。在过去的 20 年中，多尺度检测经历了多个历史时期：“特征金字塔和滑动窗口（2014 年之前）”、“带有对象提议的检测（2010-2015）”、“深度回归（2013-2016）”、“多参考检测（2015 年之后）”和“多分辨率检测（2016 年之后）”，如图 6 所示。</p>
<ul>
<li><p><strong>特征金字塔 + 滑动窗口（2014 年之前）</strong> 随着计算能力的提高，在 VJ 检测器之后，研究人员开始更多地关注通过构建“特征金字塔 + 滑动窗口”的直观检测方式。从 2004 年到 2014 年，许多里程碑式的检测器都基于这种检测范式构建，包括 HOG 检测器、DPM，甚至深度学习时代的 Overfeat 检测器[103]（ILSVRC13 定位任务的获胜者）。早期的检测模型，如 VJ 检测器和 HOG 检测器，专门设计用于检测具有“固定长宽比”的对象（例如，面部和直立的行人），只需在特征金字塔上构建固定大小的检测窗口即可。当时没有考虑检测“各种长宽比”的对象。为了检测外观更复杂、如 PASCAL VOC 中的对象，R. Girshick 等人开始寻求特征金字塔之外的更好解决方案。“混合模型”[15]是当时最好的解决方案之一，通过训练多个模型来检测具有不同长宽比的对象。除此之外，基于示例的检测[36, 115]通过为训练集中的每个对象实例（示例）训练单独的模型提供了另一种解决方案。 随着现代数据集（例如，MS-COCO中的对象变得更加多样化，混合模型或基于示例的方法不可避免地导致更多的杂项检测模型。然后自然出现了一个问题：是否有一个统一的多尺度方法来检测具有不同长宽比的对象？对象提议的引入回答了这个问题。</p>
<ul>
<li><p><strong>带有对象提议的检测（2010-2015）</strong> 对象提议指的是一组类别不可知的候选框，这些框可能包含任何对象。它在 2010 年首次应用于目标检测[116]。带有对象提议的检测有助于避免在整个图像上进行耗时的滑动窗口搜索。一个对象提议检测算法应该满足以下三个要求：
1）高召回率，2）高定位精度，3）在前两个要求的基础上，提高精度并减少处理时间。
现代提议检测方法可以分为三类：1）基于分割的分组方法[42, 117–119]，2）窗口评分方法[116, 120–122]，3）基于神经网络的方法[123–128]。我们建议读者参考以下论文以获得这些方法的全面回顾[129, 130]。早期的提议检测方法遵循自底向上的检测理念[116, 120]，并深受视觉显著性检测的影响。后来，研究人员开始转向低级视觉（例如，边缘检测）和更精心设计的手工技能，以提高候选框的定位[42, 117–119, 122, 131]。2014 年之后，随着深度 CNN 在视觉识别中的普及，自顶向下的基于学习的方法开始在这个问题上显示出更多的优势[19, 121, 123, 124]。从那时起，对象提议检测已经从自底向上的视觉发展到“过度拟合特定一组对象类别”，并且检测器和提议生成器之间的区别变得模糊[132]。由于“对象提议”已经彻底改变了滑动窗口检测，并迅速主导了基于深度学习的检测器，在 2014-2015 年间，许多研究人员开始提出以下问题：对象提议在检测中的主要作用是什么？是为了提高准确性，还是仅仅为了加速检测？为了回答这个问题，一些研究人员尝试削弱提议的作用[133]，或者简单地在 CNN 特征上执行滑动窗口检测[134–138]，但都没有获得令人满意的结果。提议检测很快就在一阶段检测器和“深度回归”技术（将在后面介绍）的崛起之后淡出了人们的视线。</p></li>
<li><p><strong>深度回归（2013-2016）</strong> 近年来，随着 GPU 计算能力的增加，人们处理多尺度检测的方式变得越来越直接和粗暴。使用深度回归解决多尺度问题的想法非常简单，即直接基于深度学习特征预测边界框的坐标[20, 104]。这种方法的优点是简单且易于实现，而缺点是定位可能不够准确，特别是对于一些小对象。“多参考检测”（将在后面介绍）后来解决了这个问题。</p></li>
<li><p><strong>多参考/多分辨率检测（2015 年之后）</strong> 多参考检测是多尺度目标检测最受欢迎的框架[19, 21, 44, 48]。其主要思想是在图像的不同位置预定义一组具有不同大小和长宽比的参考框（又称为锚框），然后基于这些参考预测检测框。每个预定义的锚框的损失通常由两部分组成：1）用于类别识别的交叉熵损失，以及 2）用于对象定位的 L1/L2 回归损失。
损失函数的一般形式可以写为：
$$L(p,p^<em>,t,t^</em>)=L_{cls}(p,p^<em>)+\beta I(t)L_{loc}(t,t^</em>)\I(t)=\begin{cases}1&amp;\mathrm{if~}IOU{a,a^*}&gt;\eta\0&amp;\mathrm{otherwise}&amp;\end{cases}$$</p>
<p>其中 t 和 t* 分别是预测和真实边界框的位置，p 和 p* 是它们的类别概率。IOU{a, a*} 是锚框 a 和其真实对应物 a* 之间的 IoU。η 是一个 IoU 阈值，比如 0.5。如果一个锚框没有覆盖任何对象，它的定位损失在最终损失中不计入。
另一种流行的技术是多分辨率检测[21, 22, 55, 105]，即在网络的不同层检测不同尺度的对象。
由于 CNN 在其前向传播过程中自然形成特征金字塔，因此更容易在更深层检测较大的对象，在较浅层检测较小的对象。多参考和多分辨率检测现在已成为最新目标检测系统的基本构建块。</p>
</li>
</ul>
</li>
</ul>
</section>
<section id="id12">
<h3><strong>2.3.3 边界框回归的技术演变</strong><a class="headerlink" href="#id12" title="Link to this heading">#</a></h3>
<p>边界框（BB）回归是目标检测中的一项重要技术。它的目标是基于初始提议或锚框来细化预测的边界框的位置。在过去的 20 年中，BB 回归的演变经历了三个历史时期：“没有 BB 回归（2008 年之前）”、“从 BB 到 BB（2008-2013）”和“从特征到 BB（2013 年之后）”。图 7 显示了边界框回归的演变。</p>
<ul class="simple">
<li><p><strong>没有 BB 回归（2008 年之前）</strong> 大多数早期的检测方法，如 VJ 检测器和 HOG 检测器，不使用 BB 回归，通常直接将滑动窗口视为检测结果。为了获得对象的准确位置，研究人员别无选择，只能构建非常密集的金字塔并在每个位置上密集地滑动检测器。</p></li>
<li><p><strong>从 BB 到 BB（2008-2013）</strong> BB 回归第一次被引入到目标检测系统是在 DPM[15]中。那时的 BB 回归通常充当后处理模块，因此它是可选的。由于 PASCAL VOC 的目标是为每个对象预测单个边界框，DPM 生成最终检测的最简单方法应该是直接使用其根过滤器的位置。后来，R. Girshick 等人引入了一种更复杂的方法，基于对象假设的完整配置来预测边界框，并将这一过程形式化为线性最小二乘回归问题[15]。这种方法在 PASCAL 标准下显著提高了检测性能。</p></li>
<li><p><strong>从特征到 BB（2013 年之后）</strong> 在 2015 年 Faster RCNN 引入之后，BB 回归不再作为单独的后处理模块，而是与检测器集成在一起，并以端到端的方式进行训练。同时，BB 回归已经发展到直接基于 CNN 特征预测 BB。为了获得更稳健的预测，通常使用平滑 L1 函数[19]作为回归损失，或者使用平方根函数[20]，这些比 DPM 中使用的最小二乘损失对异常值更为稳健。一些研究人员还选择对坐标进行归一化以获得更稳健的结果[18, 19, 21, 23]。</p></li>
</ul>
</section>
<section id="id13">
<h3><strong>2.3.4 上下文引导的技术演变</strong><a class="headerlink" href="#id13" title="Link to this heading">#</a></h3>
<p>视觉对象通常嵌入在周围环境的典型上下文中。我们的大脑利用对象和环境之间的关联来促进视觉感知和认知[160]。上下文引导长期以来被用来改善检测。在其演变历史中有三种常见的方法：1）带局部上下文的检测，2）带全局上下文的检测，3）上下文交互，如图 8 所示。</p>
<ul class="simple">
<li><p><strong>带局部上下文的检测</strong> 局部上下文指的是围绕要检测对象的区域的视觉信息。长期以来，人们已经认识到局部上下文有助于改善目标检测。在 2000 年代初，Sinha 和 Torralba[139] 发现包含面部边界轮廓等局部上下文区域可以显著提高面部检测性能。Dalal 和 Triggs 也发现，包含少量背景信息可以提高行人检测的准确性[12]。最近的基于深度学习的检测器也可以通过简单地扩大网络的接受场或对象提议的大小[140–145, 161]来提高局部上下文。</p></li>
<li><p><strong>带全局上下文的检测</strong> 全局上下文利用场景配置作为目标检测的额外信息源。对于早期的物体检测器，整合全局上下文的常见方法是整合构成场景的元素的统计摘要，如 Gist[160]。对于现代基于深度学习的检测器，有两种方法来整合全局上下文。第一种方法是利用大的感受野（甚至比输入图像还大）或 CNN 特征的全局池化操作。第二种方法是将全局上下文视为一种顺序信息，并使用循环神经网络来学习它[148, 149]。</p></li>
<li><p><strong>上下文交互</strong> 上下文交互指的是通过视觉元素之间的交互传达的信息，例如约束和依赖关系。对于大多数目标检测器而言，对象实例是独立检测和识别的，没有利用它们之间的关系。一些最近的研究建议，通过考虑上下文交互，可以改进现代目标检测器。一些最近的改进可以分为两类，第一类是探索个体对象之间的关系[15, 146, 150, 152, 162]，第二类是探索对象和场景之间的依赖关系建模[151, 151, 153]。</p></li>
</ul>
</section>
<section id="id14">
<h3><strong>2.3.5 非极大值抑制的技术演变</strong><a class="headerlink" href="#id14" title="Link to this heading">#</a></h3>
<p>非极大值抑制（NMS）是目标检测中重要的技术组。由于邻近窗口通常具有相似的检测分数，非极大值抑制在此用作后处理步骤，以移除重复的边界框并获取最终的检测结果。在目标检测的早期，NMS 并不总是被集成[30]。这是因为当时目标检测系统所需的输出并不完全清楚。在过去的 20 年中，NMS 已经逐渐发展成为以下三组方法：1）贪婪选择，2）边界框聚合，3）学习 NMS，如图 9 所示。</p>
<ul class="simple">
<li><p><strong>贪婪选择</strong> 贪婪选择是一种过时但最受欢迎的 NMS 方法。这个过程的背后思想简单直观：对于一组重叠的检测，选择具有最大检测分数的边界框，同时根据预定的重叠阈值（比如说，0.5）移除其邻近的框。上述处理以贪婪方式迭代执行。尽管贪婪选择现在已成为 NMS 的事实上的方法，但它仍有改进空间，如图 11 所示。首先，得分最高的框可能不是最佳拟合。其次，它可能会抑制附近的对象。最后，它不会抑制假阳性。近年来，尽管对提高其性能进行了一些手动修改[158, 159, 163]（见第 4.4 节的更多细节），据我们所知，贪婪选择仍然作为当今目标检测的最强基线。</p></li>
<li><p><strong>BB 聚合</strong> BB 聚合是另一组用于 NMS 的技术[10, 103, 156, 157]，其思想是将多个重叠的边界框组合或聚类为一个最终检测。这种方法的优点是它充分考虑了对象关系及其空间布局。有一些知名的检测器使用这种方法，例如 VJ 检测器[10]和 Overfeat[103]。</p></li>
<li><p><strong>学习 NMS</strong> 最近一组受到广泛关注的 NMS 改进方法是学习 NMS[136, 146, 154, 155]。这类方法的主要思想是将 NMS 视为一个过滤器，重新评分所有原始检测，并以端到端的方式将 NMS 训练为网络的一部分。这些方法在改善遮挡和密集对象检测方面显示出比传统手工制作的 NMS 方法更有效的结果。</p></li>
</ul>
<p><strong>2.3.6 硬负挖掘的技术演变</strong> 目标检测器的训练本质上是一个不平衡数据学习问题。在基于滑动窗口的检测器中，背景和对象之间的不平衡可能极为极端，如每个对象有 10^4 到 10^5 个背景窗口。现代检测数据集要求预测对象的长宽比，进一步增加了不平衡比率到 10^6 到 10^7。在这种情况下，使用所有背景数据将对训练有害，因为大量的简单负例将压倒学习过程。硬负挖掘（HNM）旨在解决训练期间不平衡数据的问题。目标检测中 HNM 的技术演变如图 10 所示。</p>
<ul class="simple">
<li><p><strong>引导启动</strong> 在目标检测中，引导启动指的是一组训练技术，其中训练从一小部分背景样本开始，然后在训练过程中逐步添加新的误分类背景。在早期的目标检测器中，引导启动最初是为了减少对数百万背景样本的计算[10, 29, 164]而引入的。后来，它成为了 DPM 和 HOG 检测器[12, 13]解决数据不平衡问题的标准训练技术。</p></li>
<li><p><strong>深度学习检测器中的 HNM</strong> 在深度学习时代之后，由于计算能力的提升，引导启动在 2014-2016 年期间短暂地在目标检测中被抛弃[16–20]。为了在训练期间缓解数据不平衡问题，像 Faster RCNN 和 YOLO 这样的检测器简单地平衡了正负窗口之间的权重。然而，研究人员后来注意到，权重平衡不能完全解决不平衡数据问题[23]。为此，在 2016 年之后，引导启动被重新引入到基于深度学习的检测器中[21, 165–168]。例如，在 SSD[21]和 OHEM[166]中，只有一小部分样本（具有最大损失值的样本）的梯度将被反向传播。在 ReFineDet[55]中，设计了一个“锚点细化模块”来过滤简单负例。另一种改进是设计新的损失函数[23, 169, 170]，通过重塑标准交叉熵损失，使其在训练期间更多地关注难以分类的示例。</p></li>
</ul>
</section>
</section>
<section id="id15">
<h2><strong>3 加速检测</strong><a class="headerlink" href="#id15" title="Link to this heading">#</a></h2>
<p>目标检测的加速一直是重要但具有挑战性的问题。在过去的 20 年中，目标检测社区开发了复杂的加速技术。这些技术大致可以分为三个层次的组：“检测流水线的加速”，“检测引擎的加速”和“数值计算的加速”，如图 12 所示。</p>
</section>
<section id="id16">
<h2><strong>3.1 特征图共享计算</strong><a class="headerlink" href="#id16" title="Link to this heading">#</a></h2>
<p>在目标检测器的不同计算阶段中，特征提取通常占据了大部分计算量。对于基于滑动窗口的检测器，计算冗余从两个位置和尺度开始，前者是由于相邻窗口之间的重叠，后者是由于相邻尺度之间的特征相关性。</p>
<section id="id17">
<h3><strong>3.1.1 空间计算冗余和加速</strong><a class="headerlink" href="#id17" title="Link to this heading">#</a></h3>
<p>减少空间计算冗余的最常用思想是特征图共享计算，即在滑动窗口之前只计算整个图像的特征图一次。传统检测器中的“图像金字塔”可以被视为“特征金字塔”。例如，为了加速 HOG 行人检测器，研究人员通常累积整个输入图像的“HOG 图”（见图 13）。然而，这种方法的缺点也很明显，即特征图的分辨率（在该特征图上滑动窗口的最小步长）将受到单元格大小的限制。如果一个小对象位于两个单元格之间，它可能被所有检测窗口忽略。解决这个问题的一种方法是构建一个积分特征金字塔，这将在第 3.6 节中介绍。特征图共享计算的思想也已广泛用于基于卷积的检测器。一些相关工作可以追溯到 1990 年代[96, 97]。近年来的大多数基于 CNN 的检测器，例如 SPPNet[17]、Fast-RCNN[18]和 Faster-RCNN[19]，都应用了类似的想法，实现了数十甚至数百倍的加速。</p>
</section>
<section id="id18">
<h3><strong>3.1.2 尺度计算冗余和加速</strong><a class="headerlink" href="#id18" title="Link to this heading">#</a></h3>
<p>减少尺度计算冗余的最成功方法是直接缩放特征而不是图像，这在 VJ 检测器[10]中首次应用。然而，这种方法不能直接应用于 HOG 类特征，因为模糊效应。对于这个问题，P. Dollár 等人通过广泛的统计分析，发现了 HOG 和积分通道特征的相邻尺度之间的强（对数线性）相关性[171]。这种相关性可以用来加速特征金字塔的计算[172]，通过近似相邻尺度的特征图。此外，构建“检测器金字塔”是另一种避免尺度计算冗余的方法，即通过在单个特征图上滑动多个检测器来检测不同尺度的对象，而不是重新缩放图像或特征[173]。</p>
</section>
</section>
<section id="id19">
<h2><strong>3.2 加速分类器</strong><a class="headerlink" href="#id19" title="Link to this heading">#</a></h2>
<p>传统的基于滑动窗口的检测器，例如 HOG 检测器和 DPM，更喜欢使用线性分类器而不是非线性分类器，因为它们的计算复杂性较低。使用非线性分类器，如核 SVM，可能提高准确性，但同时也带来高计算开销。作为标准的非参数方法，传统的核方法没有固定的计算复杂性。当我们有一个非常大的训练集时，检测速度将变得极其缓慢。在目标检测中，有许多方法可以加速基于核的分类器，其中“模型近似”是最常用的[30, 174]。由于经典核 SVM 的决策边界只能由其训练样本的一小部分（支持向量）确定，因此在推理阶段的计算复杂性将与支持向量的数量成比例：O(Nsv)。Reduced Set Vectors[30] 是一种用于核 SVM 的近似方法，旨在通过少量合成向量获得等效的决策边界。在目标检测中加速核 SVM 的另一种方法是将其决策边界近似为分段线性形式，以实现恒定的推理时间[174]。核方法也可以通过稀疏编码方法加速[175]。</p>
</section>
<section id="id20">
<h2><strong>3.3 级联检测</strong><a class="headerlink" href="#id20" title="Link to this heading">#</a></h2>
<p>级联检测是目标检测中常用的技术[10, 176]。它采取从粗到细的检测理念：首先使用简单计算过滤掉大多数简单的背景窗口，然后使用复杂计算处理那些更困难的窗口。VJ 检测器是级联检测的代表。此后，许多后续的经典目标检测器，如 HOG 检测器和 DPM，通过使用这种技术加速[14, 38, 54, 177, 178]。近年来，级联检测也已应用于基于深度学习的检测器，特别是对于“大场景中的小对象”的检测任务，例如面部检测[179, 180]、行人检测[165, 177, 181]等。除了算法加速，级联检测还应用于解决其他问题，例如提高困难示例的检测[182–184]、整合上下文信息[143, 185]和提高定位精度[104, 125]。</p>
</section>
<section id="id21">
<h2><strong>3.4 网络剪枝和量化</strong><a class="headerlink" href="#id21" title="Link to this heading">#</a></h2>
<p>“网络剪枝”和“网络量化”是加速 CNN 模型的两种常用技术，前者指的是剪枝网络结构或权重以减小其大小，后者指的是减少激活或权重的编码长度。</p>
<section id="id22">
<h3><strong>3.4.1 网络剪枝</strong><a class="headerlink" href="#id22" title="Link to this heading">#</a></h3>
<p>网络剪枝的研究可以追溯到 1980 年代。当时，Y. LeCun 等人提出了一种称为“最优脑损伤”的方法来压缩多层感知器网络的参数[186]。在这种方法中，通过取二阶导数来近似网络的损失函数，以便移除一些不重要的权重。遵循这一理念，近年来的网络剪枝方法通常采用迭代训练和剪枝过程，即在每个训练阶段后只移除一小部分不重要的权重，并重复这些操作[187]。由于传统的网络剪枝只是移除不重要的权重，这可能导致卷积滤波器中的一些稀疏连接模式，因此不能直接应用于压缩 CNN 模型。解决这个问题的一个简单方法是移除整个过滤器而不是独立的权重[188, 189]。</p>
</section>
<section id="id23">
<h3><strong>3.4.2 网络量化</strong><a class="headerlink" href="#id23" title="Link to this heading">#</a></h3>
<p>最近关于网络量化的工作主要集中在网络二值化上，其目标是通过将网络的激活或权重量化为二进制变量（例如，0/1），从而将浮点运算转换为 AND、OR、NOT 逻辑运算，以加速网络。网络二值化可以显著加速计算并减少网络的存储，使其更容易部署在移动设备上。上述思想的一种可能实现是通过最小二乘法用二进制变量近似卷积[190]。通过使用多个二进制卷积的线性组合，可以获得更精确的近似[191]。此外，一些研究人员还进一步开发了用于二值化计算的 GPU 加速库，获得了更显著的加速结果[192]。</p>
</section>
<section id="id24">
<h3><strong>3.4.3 网络蒸馏</strong><a class="headerlink" href="#id24" title="Link to this heading">#</a></h3>
<p>网络蒸馏是一个通用框架，用于将大型网络（“教师网络”）的知识压缩到小型网络（“学生网络”）中[193, 194]。最近，这个想法已经被用于目标检测的加速[195, 196]。这个想法的一个直接方法是使用教师网络指导（轻量级）学生网络的训练，以便后者可以用于加速检测[195]。另一种方法是转换候选区域，以最小化学生网络和教师网络之间的特征距离。这种方法使检测模型加速了两倍，同时实现了可比的准确性[196]。</p>
</section>
</section>
<section id="id25">
<h2><strong>3.5 轻量级网络设计</strong><a class="headerlink" href="#id25" title="Link to this heading">#</a></h2>
<p>加速基于 CNN 的检测器的最后一组方法是直接设计轻量级网络，而不是使用现成的检测引擎。研究人员一直在探索网络的正确配置，以便在受限的时间成本下获得准确性。除了一些一般的设计原则，如“更少的通道和更多的层”[197]，近年来还提出了其他一些方法：1）分解卷积，2）组卷积，3）深度可分离卷积，4）瓶颈设计，以及 5）神经架构搜索。</p>
<section id="id26">
<h3><strong>3.5.1 分解卷积</strong><a class="headerlink" href="#id26" title="Link to this heading">#</a></h3>
<p>分解卷积是构建轻量级 CNN 模型的最简单和最直接的方法。有两组分解方法。第一组方法是在空间维度上将大卷积滤波器分解为一组小的[47, 147, 198]，如图 14(b)所示。例如，可以将一个 7x7 滤波器分解为三个 3x3 滤波器，它们共享相同的接受场，但后者更高效。另一个例子是将一个 kxk 滤波器分解为一个 kx1 滤波器和一个 1xk 滤波器[198, 199]，这可能对非常大的滤波器（比如 15x15）更有效。这个想法最近已经应用于目标检测[200]。第二组方法是在通道维度上将一大组卷积分解为两组小组[201, 202]，如图 14(c)所示。例如，一个具有 d 个过滤器和 c 个通道的特征图的卷积层可以通过 d’ 个过滤器 + 一个非线性激活 +另一个 d 个过滤器（d’ &lt; d）来近似。在这种情况下，原始层的复杂度 O(dk^2c) 可以减少到 O(d’k^2c) + O(dk^2d’)。</p>
</section>
<section id="id27">
<h3><strong>3.5.2 组卷积</strong><a class="headerlink" href="#id27" title="Link to this heading">#</a></h3>
<p>组卷积的目标是通过将特征通道分成许多不同的组，然后在每个组上独立进行卷积来减少卷积层中的参数数量[189, 203]，如图 14(d)所示。如果我们将特征通道均匀地分成 m 组，不改变其他配置，卷积的计算复杂性理论上可以减少到之前的 1/m。</p>
</section>
<section id="id28">
<h3><strong>3.5.3 深度可分离卷积</strong><a class="headerlink" href="#id28" title="Link to this heading">#</a></h3>
<p>深度可分离卷积，如图 14(e)所示，是最近流行的构建轻量级卷积网络的方法[204]。它可以被视为组卷积的一个特例，当组的数量设置为通道数时。假设我们有一个具有 d 个过滤器和 c 个通道的特征图的卷积层。每个滤波器的大小是 k×k。对于深度可分离卷积，每个 k×k×c 滤波器首先被分割成 c 片，每片的大小是 k×k×1，然后分别在每个通道上使用每片滤波器进行卷积。最后，使用一些 1x1 滤波器进行维度转换，以便最终输出应具有 d 个通道。通过使用深度可分离卷积，计算复杂性可以从 O(dk^2c) 减少到 O(ck^2) + O(dc)。这个想法最近已经被应用于目标检测和细粒度分类[205–207]。</p>
</section>
<section id="id29">
<h3><strong>3.5.4 瓶颈设计</strong><a class="headerlink" href="#id29" title="Link to this heading">#</a></h3>
<p>神经网络中的瓶颈层包含的节点比前几层少。它可以用于学习输入的高效数据编码，具有降低的维度，这在深度自动编码器中已经很常见[208]。近年来，瓶颈设计已经被广泛用于设计轻量级网络[47, 209– 212]。这些方法中的一种常见方法是压缩检测器的输入层，以从检测管道的一开始就减少计算量[209–211]。另一种方法是压缩检测引擎的输出，使特征图更薄，以便后续检测阶段更高效[47, 212]。</p>
</section>
<section id="id30">
<h3><strong>3.5.5 神经架构搜索</strong><a class="headerlink" href="#id30" title="Link to this heading">#</a></h3>
<p>最近，通过神经架构搜索（NAS）自动设计网络架构引起了极大的兴趣，而不是严重依赖专家的经验和知识。NAS 已经被应用于大规模图像分类[213, 214]、目标检测[215]和图像分割[216]任务。NAS 也在设计轻量级网络方面显示出有希望的结果，其中在搜索过程中同时考虑了预测准确性和计算复杂性的约束[217, 218]。</p>
</section>
</section>
<section id="id31">
<h2><strong>3.6 数值加速</strong><a class="headerlink" href="#id31" title="Link to this heading">#</a></h2>
<p>在本节中，我们主要介绍四种在目标检测中经常使用的数值加速方法：
1）使用积分图像加速，2）在频域中加速，3）向量量化，以及 4）降秩近似。</p>
<section id="id32">
<h3><strong>3.6.1 使用积分图像加速</strong><a class="headerlink" href="#id32" title="Link to this heading">#</a></h3>
<p>积分图像是图像处理中的一个重要方法。它有助于快速计算图像子区域的总和。积分图像的本质是信号处理中卷积的积分-微分可分离性：</p>
<p>$$f(x)<em>g(x)=(\int f(x)dx)</em>(\frac{dg(x)}{dx})$$.
如果$\frac{dg(x)}{dx}$​是一个稀疏信号，那么卷积可以通过该方程的右侧加速。尽管 VJ 检测器[10]因积分图像加速而闻名，但在它出现之前，积分图像已经被用来加速 CNN 模型[219]，并实现了超过 10 倍的加速。除了上述例子，积分图像也可以用来加速目标检测中的更一般特征，例如颜色直方图、梯度直方图[171, 177, 220, 221]等。一个典型的例子是通过计算积分 HOG 图[177, 220]来加速 HOG。与传统的积分图像累积像素值不同，积分 HOG 图在图像中累积梯度方向，如图 15 所示。由于一个单元格的直方图可以被视为某个区域内梯度向量的总和，通过使用积分图像，可以在任意位置和大小的矩形区域内以恒定的计算开销计算直方图。积分 HOG 图已经在行人检测中使用，并实现了数十倍的加速而不失准确性[177]。后来在 2009 年，P. Dollár 等人提出了一种称为积分通道特征（ICF）的新型图像特征，它可以被视为积分图像特征的更一般情况，并已成功应用于行人检测[171]。ICF 在当时实现了最先进的检测准确性，并在接近实时的检测速度下。</p>
</section>
<section id="id33">
<h3><strong>3.6.2 在频域中加速</strong><a class="headerlink" href="#id33" title="Link to this heading">#</a></h3>
<p>卷积是目标检测中的一种重要数值运算。由于线性检测器的检测可以被视为特征图和检测器权重之间的窗口逐个内积，这个过程可以通过卷积来实现。卷积可以通过多种方式加速，其中傅里叶变换是一个非常实用的选择，特别是对于加速那些大滤波器。在频域中加速卷积的理论基础是信号处理中的卷积定理，即在适当的条件下，两个信号卷积的傅里叶变换是它们在傅里叶空间中的逐点乘积：
$$I*W=\mathcal{F}^{-1}(\mathcal{F}(I)\odot\mathcal{F}(W))$$.</p>
</section>
<section id="id34">
<h3><strong>3.6.3向量量化</strong><a class="headerlink" href="#id34" title="Link to this heading">#</a></h3>
<p>向量量化（Vector Quantization, VQ）是信号处理中的一种经典量化方法，旨在通过一组原型向量来近似大量数据的分布。它可以用于数据压缩，并加速目标检测中的内积运算[227, 228]。例如，通过 VQ，可以将 HOG 直方图分组并量化为一组原型直方图向量。然后在检测阶段，特征向量和检测权重之间的内积可以通过表查找操作来实现。由于该过程中没有浮点乘法和除法，DPM 和示例 SVM 检测器的速度可以加速超过一个数量级[227]。</p>
</section>
<section id="id35">
<h3><strong>3.6.4 降秩近似</strong><a class="headerlink" href="#id35" title="Link to this heading">#</a></h3>
<p>在深度网络中，全连接层的计算本质上是两个矩阵的乘法。当参数矩阵 W ∈ R^(u×v) 很大时，检测器的计算负担将变得很重。例如，在 Fast RCNN 检测器[18]中，将近一半的前向传递时间都花在计算全连接层上。降秩近似是一种加速矩阵乘法的方法。它的目的是对矩阵 W 进行低秩分解：W ≈ UΣtV^T，其中 U 是一个 u×t 矩阵，包含 W 的前 t 个左奇异向量，Σt 是一个 t×t 对角矩阵，包含 W 的前 t 个奇异值，V 是一个 v×t 矩阵，包含 W 的前 t 个右奇异向量。上述过程，也称为截断奇异值分解（Truncated SVD），将参数计数从 uv 减少到 t(u + v)，如果 t 远小于 min(u, v)，这可以是显著的。截断 SVD 已被用于加速 Fast RCNN 检测器[18]，并实现了两倍的加速。</p>
</section>
</section>
</section>
<section id="id36">
<h1><strong>4 最近目标检测的进展</strong><a class="headerlink" href="#id36" title="Link to this heading">#</a></h1>
<p>在本节中，我们将回顾最近三年中的最新目标检测方法。</p>
<section id="id37">
<h2><strong>4.1 使用更好的引擎进行检测</strong><a class="headerlink" href="#id37" title="Link to this heading">#</a></h2>
<p>近年来，深度 CNN 在许多计算机视觉任务中发挥了核心作用。由于检测器的准确性在很大程度上取决于其特征提取网络，因此在本文中，我们将背景网络（例如 ResNet 和 VGG）称为检测器的“引擎”。图 17 显示了三种知名检测系统：Faster RCNN[19]、R-FCN[46]和 SSD[21]在不同选择的引擎上的检测准确性[27]。在本节中，我们将介绍深度学习时代一些重要的检测引擎。我们建议读者参考以下调查，以获取有关此主题的更多详细信息[229]。</p>
<ul class="simple">
<li><p><strong>AlexNet</strong>：AlexNet[40]是一个八层深网络，是第一个在计算机视觉中引发深度学习革命的 CNN 模型。AlexNet 在 2012 年 ImageNet LSVRC-2012 竞赛中以较大优势获胜[15.3% 对比 26.2%（第二名）的错误率]。截至 2019 年 2 月，AlexNet 论文已被引用超过 30,000 次。</p></li>
<li><p><strong>VGG</strong>：VGG 由牛津大学的视觉几何小组（Visual Geometry Group, VGG）在 2014 年提出[230]。VGG 将模型深度增加到 16-19 层，并使用非常小的（3x3）卷积滤波器，而不是之前在 AlexNet 中使用的 5x5 和 7x7 滤波器。VGG 在其时代的 ImageNet 数据集上实现了最先进的性能。</p></li>
<li><p><strong>GoogLeNet</strong>：GoogLeNet，也称为 Inception[198, 231–233]，是谷歌公司自 2014 年以来提出的一系列 CNN 模型。GoogLeNet 增加了 CNN 的宽度和深度（高达 22 层）。Inception 家族的主要贡献是引入了分解卷积和批量归一化。</p></li>
<li><p><strong>ResNet</strong>：深度残差网络（ResNet）[234]，由 K. He 等人在 2015 年提出，是一种新型的卷积网络架构，比以前使用的网络深得多（高达 152 层）。ResNet 的目标是通过将网络层重新定义为学习相对于层输入的残差函数，从而简化网络的训练。ResNet 在 2015 年赢得了多个计算机视觉竞赛，包括 ImageNet 检测、ImageNet 定位、COCO 检测和 COCO 分割。</p></li>
<li><p><strong>DenseNet</strong>：DenseNet[235]由 G. Huang 和 Z. Liu 等人在 2017 年提出。ResNet 的成功表明，CNN 中的短路连接使我们能够训练更深入、更准确的模型。作者接受了这一观察结果，并引入了一个密集连接块，该块以前馈方式将每层连接到网络中的所有其他层。</p></li>
<li><p><strong>SENet</strong>：Squeeze and Excitation Networks（SENet）由 J. Hu 和 L. Shen 等人在 2018 年提出[236]。其主要贡献是整合了全局池化和洗牌操作，以学习特征图的通道级重要性。SENet 在 2017 年 ILSVRC 分类竞赛中获得了第一名。</p></li>
</ul>
<p><strong>• 带有新引擎的物体检测器</strong></p>
<p>在最近三年中，许多最新的引擎已经被应用于目标检测。例如，一些最新的目标检测模型，如 STDN[237]、DSOD[238]、TinyDSOD[207] 和 Pelee[209] 选择 DenseNet[235] 作为它们的检测引擎。作为实例分割的最先进模型，Mask RCNN[4] 应用了下一代 ResNet：ResNeXt[239] 作为其检测引擎。此外，为了加速检测，深度可分离卷积操作，这是由 Xception[204] 引入的，是 Inception 的改进版本，也已经在检测器如 MobileNet[205] 和 LightHead RCNN[47] 中使用。</p>
</section>
<section id="id38">
<h2><strong>4.2 使用更好的特征进行检测</strong><a class="headerlink" href="#id38" title="Link to this heading">#</a></h2>
<p>特征表示的质量对目标检测至关重要。近年来，许多研究人员在最新的引擎基础上努力进一步提高图像特征的质量，其中最重要的两组方法是：1）特征融合，2）学习具有大接受场的高分辨率特征。</p>
<section id="id39">
<h3><strong>4.2.1 为什么特征融合很重要？</strong><a class="headerlink" href="#id39" title="Link to this heading">#</a></h3>
<p>不变性和等变性是图像特征表示中的两个重要属性。分类渴望不变的特征表示，因为它旨在学习高级语义信息。目标定位渴望等变表示，因为它旨在区分位置和尺度变化。由于目标检测由目标识别和定位的两个子任务组成，因此对于检测器来说，同时学习不变性和等变性至关重要。特征融合在过去三年中已在目标检测中得到广泛应用。</p>
</section>
<section id="id40">
<h3><strong>4.2.2 不同方式的特征融合</strong><a class="headerlink" href="#id40" title="Link to this heading">#</a></h3>
<p>这里我们介绍一些最近的特征融合方法，从两个方面：1）处理流程，2）逐元素操作。</p>
<p>• 处理流程</p>
<p>最近的目标检测中的特征融合方法可以分为两类：1）自底向上融合，2）自顶向下融合，如图 18(a)-(b)所示。自底向上融合通过跳跃连接将浅层特征向前传递到更深层[237, 240–242]。相比之下，自顶向下融合将深层特征反馈到较浅层[22, 55, 243–246]。除了这些方法，最近还提出了更复杂的方法，例如在不同层之间编织特征[247]。由于不同层的特征图可能在空间和通道维度上具有不同的大小，可能需要调整特征图，例如通过调整通道数、对低分辨率图进行上采样或对高分辨率图进行下采样以适当大小。最简单的方法是使用最近邻或双线性插值[22, 244]。此外，分数步幅卷积（也称为转置卷积）[45, 248]是另一种最近流行的调整特征图大小和通道数的方法。使用分数步幅卷积的优点在于它可以自行学习适当的上采样方式[55, 212, 241– 243, 245, 246, 249]。</p>
<p>• 逐元素操作</p>
<p>从局部角度来看，特征融合可以被视为不同特征图之间的逐元素操作。有三组方法：1）逐元素求和，2）逐元素乘积，以及 3）连接，如图 18(c)-(e)所示。逐元素求和是执行特征融合的最简单方法。它已在许多最近的目标检测器中频繁使用[22, 55, 241, 243, 246]。逐元素乘积[245, 249–251]与逐元素求和非常相似，唯一的区别是使用乘法代替加法。逐元素乘积的优点在于它可以用于抑制或突出显示某个区域内的特征，这可能进一步有利于小目标检测[245, 250, 251]。特征连接是另一种特征融合方式[212, 237, 240, 244]。它的优点在于可以用来整合不同区域的上下文信息，而缺点是增加了内存[235]。</p>
</section>
<section id="id41">
<h3><strong>4.2.3 学习具有大接受场的高分辨率特征</strong><a class="headerlink" href="#id41" title="Link to this heading">#</a></h3>
<p>接受场和特征分辨率是 CNN 基检测器的两个重要特征，前者指的是输入像素对输出单个像素计算的贡献空间范围，后者对应于输入和特征图之间的下采样率。具有较大接受场的网络能够捕获更大规模的上下文信息，而具有较小接受场的网络可能更专注于局部细节。</p>
<p>如前所述，特征分辨率越低，检测小目标就越困难。最直接的提高特征分辨率的方法是移除池化层或减少卷积下采样率。但这会引起一个新问题，由于输出步长的减少，接受场会变得太小。换句话说，这将缩小检测器的“视野”，可能导致一些大目标的漏检。一个实用的提高接受场和特征分辨率的方法是引入扩张卷积（也称为空洞卷积或带孔卷积）。扩张卷积最初在语义分割任务中提出[252, 253]。其主要思想是扩展卷积核并使用稀疏参数。例如，一个 3x3 滤波器，如果扩张率为 2，将具有与 5x5 核相同接受场，但只有 9 个参数。扩张卷积现在已经被广泛应用于目标检测[21, 56, 254, 255]，并证明在不增加任何额外参数和计算成本的情况下提高了准确性[56]。</p>
</section>
</section>
<section id="id42">
<h2><strong>4.3 非滑动窗口检测</strong><a class="headerlink" href="#id42" title="Link to this heading">#</a></h2>
<p>尽管目标检测已经从使用手工特征发展到深度神经网络，但检测仍然遵循“在特征图上滑动窗口”的范式[137]。最近，一些检测器建立在滑动窗口之外。</p>
<ul class="simple">
<li><p><strong>作为子区域搜索的检测</strong> 子区域搜索[184, 256–258]提供了一种新的执行检测的方法。一种最近的方法是将检测视为从初始网格开始的路径规划过程，并最终收敛到所需的真实边界框[256]。另一种方法是将检测视为一个迭代更新过程，以细化预测边界框的角[257]。</p></li>
<li><p><strong>作为关键点定位的检测</strong> 关键点定位是一个重要的计算机视觉任务，具有广泛的应用，例如面部表情识别[259]、人体姿态识别[260]等。由于图像中的任何对象都可以通过其真实边界框的左上角和右下角唯一确定，因此检测任务可以等效地被框架化为成对关键点定位问题。最近实现这一想法的一种方法是预测角落的热图[261]。这种方法的优点是它可以在语义分割框架下实现，并且不需要设计多尺度锚框。</p></li>
</ul>
</section>
<section id="id43">
<h2><strong>4.4 提高定位的改进</strong><a class="headerlink" href="#id43" title="Link to this heading">#</a></h2>
<p>为了提高定位精度，最近的检测器中有两种方法：1）边界框细化，2）为精确定位设计新的损失函数。</p>
<section id="id44">
<h3><strong>4.4.1 边界框细化</strong><a class="headerlink" href="#id44" title="Link to this heading">#</a></h3>
<p>提高定位精度的最直观方法是边界框细化，可以被视为检测结果的后处理。虽然边界框回归已经集成到大多数现代目标检测器中，但仍有一些具有意外尺度的对象不能被任何预定义的锚框很好地捕获。这将不可避免地导致它们的定位预测不准确。为此，最近引入了“迭代边界框细化”[262–264]，通过将检测结果迭代地输入到边界框回归器中，直到预测收敛到正确的位置和大小。然而，一些研究人员也声称这种方法不能保证定位精度的单调性[262]，换句话说，如果多次应用边界框回归，它可能会降低定位精度。</p>
</section>
<section id="id45">
<h3><strong>4.4.2 改进损失函数以实现精确定位</strong><a class="headerlink" href="#id45" title="Link to this heading">#</a></h3>
<p>在大多数现代检测器中，目标定位被视为坐标回归问题。然而，这种范式的两个缺点是：首先，回归损失函数并不对应于定位的最终评估。例如，我们不能保证较低的回归误差总是产生较高的 IoU 预测，特别是当对象具有非常大的长宽比时。其次，传统的边界框回归方法不提供定位的置信度。当多个边界框相互重叠时，这可能导致非最大值抑制（见 2.3.5 小节的更多细节）失败。上述问题可以通过设计新的损失函数来缓解。最直接的设计是直接使用 IoU 作为定位损失函数[265]。一些研究人员进一步提出了 IoU 引导的 NMS，以在训练和检测阶段提高定位[163]。此外，一些研究人员还尝试在概率推断框架下改进定位[266]。与直接预测框坐标的先前方法不同，这种方法预测边界框位置的概率分布。</p>
</section>
</section>
<section id="id46">
<h2><strong>4.5 与分割结合的学习</strong><a class="headerlink" href="#id46" title="Link to this heading">#</a></h2>
<p>目标检测和语义分割都是计算机视觉中的重要任务。最近的研究表明，通过与语义分割结合，可以改进目标检测。</p>
<section id="id47">
<h3><strong>4.5.1 为什么分割改进检测？</strong><a class="headerlink" href="#id47" title="Link to this heading">#</a></h3>
<p>语义分割改进目标检测有三个原因：</p>
<ul class="simple">
<li><p>分割有助于类别识别：边缘和边界是构成人类视觉认知的基本元素[267, 268]。在计算机视觉中，对象（例如，汽车，人）与材质（例如，天空，水，草地）的区别在于前者通常具有封闭且定义明确的边界，而后者则没有。由于语义分割任务的特征很好地捕获了对象的边界，分割可能有助于类别识别。</p></li>
<li><p>分割有助于精确定位：对象的真实边界框由其定义明确的边界确定。对于一些具有特殊形状的对象（例如，想象一只尾巴非常长的猫），预测高 IoU 位置将很困难。由于对象边界可以在语义分割特征中很好地编码，因此通过分割学习将有助于精确的对象定位。</p></li>
<li><p>分割可以作为上下文嵌入：日常生活中的对象被不同的背景所包围，例如天空，水，草地等，所有这些元素构成了对象的上下文。整合语义分割的上下文将有助于目标检测，例如，飞机更有可能出现在天空而不是水上。</p></li>
</ul>
</section>
<section id="id48">
<h3><strong>4.5.2 分割如何改进检测？</strong><a class="headerlink" href="#id48" title="Link to this heading">#</a></h3>
<p>通过分割改进目标检测有两种主要方法：1）学习丰富的特征，2）学习多任务损失函数。</p>
<ul class="simple">
<li><p><strong>学习丰富的特征</strong> 最简单的方法是将分割网络视为固定特征提取器，并将其作为额外特征集成到检测框架中[144, 269, 270]。这种方法的优点是实现简单，缺点是分割网络可能会带来额外的计算。</p></li>
<li><p><strong>学习多任务损失函数</strong> 另一种方法是在原始检测框架的顶部引入一个额外的分割分支，并使用多任务损失函数（分割损失 + 检测损失）训练此模型[4, 269]。在大多数情况下，分割分支将在推理阶段被移除。优点是检测速度不会受到影响，但缺点是训练需要像素级图像注释。为此，一些研究人员遵循了“弱监督学习”的思想：他们不是基于像素级注释掩模进行训练，而是简单地基于边界框级注释训练分割分支[250, 271]。</p></li>
</ul>
</section>
</section>
<section id="id49">
<h2><strong>4.6 旋转和平移变化的鲁棒检测</strong><a class="headerlink" href="#id49" title="Link to this heading">#</a></h2>
<p>对象旋转和平移变化是目标检测中的重要挑战。由于 CNN 学习的特征对旋转和平移变化不具有不变性，因此近年来许多人在这个问题上做出了努力。</p>
<section id="id50">
<h3><strong>4.6.1 旋转鲁棒检测</strong><a class="headerlink" href="#id50" title="Link to this heading">#</a></h3>
<p>目标旋转在检测任务中非常常见，例如面部检测、文本检测等。最直接的解决方案是数据增强，以便任何方向的对象都可以被增强数据很好地覆盖[88]。另一种解决方案是为每个方向训练独立的检测器[272, 273]。除了这些传统方法，最近还有一些新的改进方法。</p>
<ul class="simple">
<li><p><strong>旋转不变损失函数</strong> 学习使用旋转不变损失函数的思想可以追溯到 1990 年代[274]。最近的工作引入了对原始检测损失函数的约束，以使旋转对象的特征保持不变[275, 276]。</p></li>
<li><p><strong>旋转校准</strong> 另一种改进旋转不变检测的方法是对对象候选进行几何变换[277–279]。这将特别有助于多阶段检测器，在早期阶段的相关性将有益于后续检测。这一思想的代表是空间变换网络（STN）[278]。STN 现在已经被用于旋转文本检测[278]和旋转面部检测[279]。</p></li>
<li><p><strong>旋转 RoI 池化</strong> 在两阶段检测器中，特征池化的目的是通过首先将提议均匀地划分为一组网格，然后将网格特征连接起来，为具有任意位置和大小的对象提议提取固定长度的特征表示。由于网格划分是在笛卡尔坐标系中进行的，因此特征对旋转变换不具有不变性。最近的改进是使用极坐标网格，使特征能够对旋转变化具有鲁棒性[272]。</p></li>
</ul>
</section>
<section id="id51">
<h3><strong>4.6.2 平移鲁棒检测</strong><a class="headerlink" href="#id51" title="Link to this heading">#</a></h3>
<p>最近在训练和检测阶段都取得了对平移鲁棒检测的改进。</p>
<ul class="simple">
<li><p><strong>平移自适应训练</strong> 大多数现代检测器将输入图像重新缩放到固定大小，并反向传播所有尺度的对象的损失，如图 19(a)所示。然而，这样做的一个缺点是存在“尺度不平衡”问题。在检测期间构建图像金字塔可以缓解这个问题，但并不能根本解决[46, 234]。最近的改进是图像金字塔的尺度归一化（SNIP）[280]，在训练和检测阶段都构建图像金字塔，并且只反向传播所选尺度的损失，如图 19(b)所示。一些研究人员进一步提出了更有效的训练策略：带有高效重采样的 SNIP（SNIPER）[281]，即裁剪和重新缩放图像到一组子区域，以便从大批量训练中受益。</p></li>
<li><p><strong>平移自适应检测</strong> 大多数现代检测器使用固定配置来检测不同大小的对象。例如，在典型的基于 CNN 的检测器中，我们需要仔细定义锚点的大小。这样做的一个缺点是配置不能适应意外的尺度变化。为了改善小目标的检测，一些“自适应缩放”技术最近被提出，以将小目标自适应地放大到“更大的目标”[184, 258]。另一个最近的改进是学习预测图像中对象的尺度分布，然后根据分布自适应地重新缩放图像[282, 283]。</p></li>
</ul>
</section>
</section>
<section id="id52">
<h2><strong>4.7 从头开始训练</strong><a class="headerlink" href="#id52" title="Link to this heading">#</a></h2>
<p>大多数基于深度学习的目标检测器首先在大规模数据集上进行预训练，比如 ImageNet，然后在特定的检测任务上进行微调。人们一直认为预训练有助于提高泛化能力并加快训练速度，问题是，我们真的需要在 ImageNet 上预训练检测器吗？实际上，采用预训练网络进行目标检测有一些局限性。第一个局限性是 ImageNet 分类和目标检测之间的差异，包括它们的损失函数和尺度/类别分布。第二个局限性是域不匹配。由于 ImageNet 中的图像是 RGB 图像，而检测有时应用于深度图像（RGB-D）或 3D 医学图像，预训练的知识不能很好地转移到这些检测任务中。近年来，一些研究人员尝试从头开始训练目标检测器。为了加快训练并提高稳定性，一些研究人员引入了密集连接和批量归一化，以加速浅层的反向传播[238, 284]。K. He 等人最近的工作的进一步质疑了预训练范式，他们探索了相反的领域：他们报告了在 COCO 数据集上使用标准模型从头开始训练，仅增加训练迭代次数，就可以获得竞争性的结果。随机初始化的模型即使只使用 10% 的训练数据，也表现出惊人的鲁棒性，这表明 ImageNet 预训练可以加快收敛速度，但并不一定提供正则化或提高最终检测准确性。</p>
</section>
<section id="id53">
<h2><strong>4.8 对抗性训练</strong><a class="headerlink" href="#id53" title="Link to this heading">#</a></h2>
<p>生成对抗网络（GAN）[286]，由 A. Goodfellow 等人在 2014 年引入，近年来受到了极大的关注。一个典型的 GAN 由两个神经网络组成：一个生成器网络和一个鉴别器网络，在最小最大优化框架中相互竞争。通常，生成器学习从潜在空间映射到特定数据分布，而鉴别器旨在区分真实数据分布的实例和由生成器产生的实例。GAN 已被广泛应用于许多计算机视觉任务，如图像生成[286, 287]、图像风格迁移[288]和图像超分辨率[289]。在最近两年中，GAN 也被应用于目标检测，特别是提高对小型和遮挡对象的检测。GAN 被用来通过缩小小型和大型对象之间的表示来增强对小型对象的检测[290, 291]。为了提高对遮挡对象的检测，最近的一种想法是使用对抗性训练生成遮挡掩码[292]。与在像素空间生成示例不同，对抗性网络直接修改特征以模仿遮挡。除了这些工作，对检测器进行对抗性攻击的研究[293]，旨在研究如何使用对抗性示例攻击检测器，最近越来越受到关注。对这一主题的研究对于自动驾驶尤为重要，因为在确保对对抗性攻击的鲁棒性之前，不能充分信任它。</p>
</section>
<section id="id54">
<h2><strong>4.9 弱监督目标检测</strong><a class="headerlink" href="#id54" title="Link to this heading">#</a></h2>
<p>现代目标检测器的训练通常需要大量手动标注的数据，而标注过程耗时、昂贵且效率低下。弱监督目标检测（WSOD）旨在通过仅使用图像级注释而不是边界框来训练检测器，从而解决这个问题。最近，多实例学习已被用于 WSOD[294, 295]。多实例学习是一种监督学习方法[39, 296]。与学习一组单独标注的实例不同，多实例学习模型接收一组标记的袋子，每个袋子包含许多实例。如果我们将一张图片中的对象候选视为一个袋子，将图像级注释视为标签，那么 WSOD 就可以被表述为一个多实例学习过程。类激活映射是另一种最近用于 WSOD 的方法[297, 298]。对 CNN 可视化的研究表明，尽管没有对对象位置的监督，CNN 的卷积层表现为对象检测器。类激活映射揭示了如何使 CNN 即使在仅使用图像级标签训练时也具有定位能力[299]。除了上述方法，一些其他研究人员将 WSOD 视为一个提议排名过程，通过选择最具信息量的区域，然后在图像级注释上训练这些区域[300]。WSOD 的一个简单方法是遮盖图像的不同部分。如果检测分数急剧下降，那么一个对象很可能被覆盖[301]。此外，交互式注释[295]在训练期间考虑人类反馈，以改进 WSOD。最近，生成对抗性训练已被用于 WSOD[302]。</p>
</section>
</section>
<section id="id55">
<h1><strong>5 应用</strong><a class="headerlink" href="#id55" title="Link to this heading">#</a></h1>
<p>在本节中，我们将回顾过去 20 年中的一些重要检测应用，包括行人检测、面部检测、文本检测、交通标志/信号检测和遥感目标检测。</p>
<section id="id56">
<h2><strong>5.1 行人检测</strong><a class="headerlink" href="#id56" title="Link to this heading">#</a></h2>
<p>行人检测作为目标检测的一个重要应用，在自动驾驶、视频监控、刑事调查等领域受到了广泛关注。一些早期的行人检测方法，如 HOG 检测器[12]、ICF 检测器[171]，在特征表示[12, 171]、分类器设计[174]和检测加速[177]方面为一般目标检测奠定了坚实的基础。近年来，一些通用目标检测算法，例如 Faster RCNN[19]，已被引入到行人检测中[165]，并大大推动了这一领域的进展。</p>
<section id="id57">
<h3><strong>5.1.1 困难和挑战</strong><a class="headerlink" href="#id57" title="Link to this heading">#</a></h3>
<p>行人检测中的挑战和困难可以总结如下：</p>
<ul class="simple">
<li><p><strong>小行人</strong>：图 20(a) 显示了远离摄像机捕捉到的小行人的一些示例。在 Caltech 数据集[59, 60]中，15% 的行人高度小于 30 像素。</p></li>
<li><p><strong>硬负例</strong>：图 20(b) 显示了一些背景在街景图像中的视觉外观与行人非常相似的示例。</p></li>
<li><p><strong>密集和遮挡的行人</strong>：图 20(c) 显示了一些密集和遮挡的行人的示例。在 Caltech 数据集[59, 60]中，未被遮挡的行人仅占总行人实例的 29%。</p></li>
<li><p><strong>实时检测</strong>：从高清视频中实时检测行人对一些应用（如自动驾驶和视频监控）至关重要。</p></li>
</ul>
</section>
<section id="id58">
<h3><strong>5.1.2 文献综述</strong><a class="headerlink" href="#id58" title="Link to this heading">#</a></h3>
<p>行人检测有着悠久的研究历史[30, 31, 101]。其发展可以分为两个技术时期：1）传统行人检测，2）基于深度学习的行人检测。我们建议读者参考以下综述以获取有关此主题的更多详细信息[60, 303–307]。</p>
<ul class="simple">
<li><p><strong>传统行人检测方法</strong> 由于计算资源的限制，Haar 小波特征在早期行人检测中得到了广泛使用[30, 31, 308]。为了提高对遮挡行人的检测，当时流行的一个想法是“通过组件检测”[31, 102, 220]，即认为检测是多个部分检测器的集合，这些检测器分别针对不同的人体部位（例如头部、腿部和手臂）进行训练。随着计算能力的提升，人们开始设计更复杂的检测模型，自 2005 年以来，基于梯度的表示[12, 37, 177, 220, 309]和 DPM[15, 37, 54]已成为行人检测的主流。2009 年，通过使用积分图像加速，提出了一种有效且轻量级的特征表示：积分通道特征（ICF）[171]。ICF 随后成为当时行人检测的新基准[60]。除了特征表示，一些领域知识也已被考虑，例如外观恒常性和形状对称性[310]和立体信息[173, 311]。</p></li>
<li><p><strong>基于深度学习的行人检测方法</strong> 行人检测是最早应用深度学习的计算机视觉任务之一[312]。为了提高小行人检测：尽管像 Fast/Faster R-CNN 这样的深度学习目标检测器在一般目标检测中表现出最佳性能，但由于它们的卷积特征的低分辨率，它们在检测小行人方面取得了有限的成功[165]。解决这个问题的一些最近解决方案包括特征融合[165]、引入额外的高分辨率手工特征[313, 314]和在多个分辨率上集成检测结果[315]。为了提高硬负例检测：最近的一些改进包括集成提升决策树[165]和语义分割（作为行人的上下文）[316]。此外，“跨模态学习”的想法也被引入，通过使用 RGB 和红外图像来丰富硬负例的特征[317]。为了提高密集和遮挡行人检测：正如我们在第 2.3.2 节中提到的，CNN 的深层特征具有更丰富的语义，但不利于检测密集对象。为此，一些研究人员设计了新的损失函数，通过考虑目标的吸引和周围其他对象的排斥[318]。目标遮挡是与密集行人经常相伴的另一个问题。集成部分检测器[319, 320]和注意力机制[321]是提高遮挡行人检测的最常用方法。</p></li>
</ul>
</section>
</section>
<section id="id59">
<h2><strong>5.2 面部检测</strong><a class="headerlink" href="#id59" title="Link to this heading">#</a></h2>
<p>面部检测是计算机视觉中最古老的应用之一[96, 164]。早期的面部检测，如 VJ 检测器[10]，在目标检测中极大地推动了许多杰出思想的发展，即使在今天的目标检测中，这些思想仍然扮演着重要角色。面部检测现在已经应用于生活的各个方面，例如数字相机中的“微笑”检测、电子商务中的“刷脸”、移动应用中的面部化妆等。</p>
<section id="id60">
<h3><strong>5.2.1 困难和挑战</strong><a class="headerlink" href="#id60" title="Link to this heading">#</a></h3>
<p>面部检测中的困难和挑战可以总结如下：</p>
<ul class="simple">
<li><p><strong>类内变化</strong>：人脸可能呈现各种表情、肤色、姿势和运动，如图 21(a) 所示。</p></li>
<li><p><strong>遮挡</strong>：面部可能被其他物体部分遮挡，如图 21(b) 所示。</p></li>
<li><p><strong>多尺度检测</strong>：在各种尺度上检测面部，特别是对于一些小面部，如图 21(c) 所示。</p></li>
<li><p><strong>实时检测</strong>：在移动设备上的面部检测通常需要 CPU 实时检测速度。</p></li>
</ul>
</section>
<section id="id61">
<h3><strong>5.2.2 文献综述</strong><a class="headerlink" href="#id61" title="Link to this heading">#</a></h3>
<p>面部检测的研究可以追溯到20世纪90年代初[95, 106, 108]。然后经历了多个历史时期：早期面部检测（2001年之前）、传统面部检测（2001-2015年）和基于深度学习的面部检测（2015年至今）。我们建议读者参考以下综述以获取更多详细信息[323, 324]。</p>
<ul class="simple">
<li><p><strong>早期面部检测（2001年之前）</strong> 早期面部检测算法可以分为三组：1) 基于规则的方法。这类方法编码了人类对典型面部特征的知识，并捕捉面部元素之间的关系[107, 108]。2) 基于子空间分析的方法。这类方法分析面部在潜在线性子空间中的分布[95, 106]。Eigenfaces是这类方法的代表[95]。3) 基于学习的方法：将面部检测框架化为滑动窗口 + 二分类（目标 vs 背景）过程。这类方法中常用的模型包括神经网络[96, 164, 325]和SVM[29, 326]。</p></li>
<li><p><strong>传统面部检测（2000-2015年）</strong> 这一时期的面部检测器可以分为两组。第一组方法基于提升决策树[10, 11, 109]。这些方法计算简单，但通常在复杂场景下的检测精度较低。第二组基于早期的卷积神经网络，利用特征的共享计算来加速检测[112, 113, 327]。</p></li>
<li><p><strong>基于深度学习的面部检测（2015年以后）</strong> 在深度学习时代，大多数面部检测算法遵循了一般目标检测器（如Faster RCNN和SSD）的检测思想。为了加速面部检测：级联检测（见第3.3节）是深度学习时代加速面部检测器的常用方法[179, 180]。另一种加速方法是预测图像中面部的尺度分布[283]，然后在选定的尺度上运行检测。为了改进多姿态和遮挡面部检测：面部校准的思想已被用来改进多姿态面部检测，通过估计校准参数[279]或使用多阶段检测的逐步校准[277]。为了改进遮挡面部检测，最近提出了两种方法。第一种是融入“注意力机制”，以便突出显示底层面部目标的特征[250]。第二种是“基于部分的检测”，它继承了DPM的思想[328]。为了改进多尺度面部检测：最近关于多尺度面部检测的工作[322, 329–331]使用了与一般目标检测中类似的检测策略，包括多尺度特征融合和多分辨率检测（见第2.3.2节和4.2.2节的更多细节）。</p></li>
</ul>
</section>
</section>
<section id="id62">
<h2><strong>5.3 文本检测</strong><a class="headerlink" href="#id62" title="Link to this heading">#</a></h2>
<p>文本长期以来一直是人类的主要信息载体，已有数千年历史。文本检测的基本目标是确定给定图像中是否存在文本，如果存在，定位并识别它。文本检测有非常广泛的应用。它可以帮助视障人士“阅读”街道标志和货币[332, 333]。在地理信息系统中，检测和识别房屋号码和街道标志可以更容易地构建数字地图[334, 335]。</p>
<section id="id63">
<h3><strong>5.3.1 困难和挑战</strong><a class="headerlink" href="#id63" title="Link to this heading">#</a></h3>
<p>文本检测的困难和挑战可以总结如下：</p>
<ul class="simple">
<li><p><strong>不同字体和语言</strong>：文本可能有不同的字体、颜色和语言，如图22(a)所示。</p></li>
<li><p><strong>文本旋转和平移畸变</strong>：文本可能有不同的方向，甚至可能有平移畸变，如图22(b)所示。</p></li>
<li><p><strong>密集排列的文本定位</strong>：具有大宽高比和密集布局的文本行难以准确定位，如图22(c)所示。</p></li>
<li><p><strong>断裂和模糊的字符</strong>：断裂和模糊的字符在街景图像中很常见。</p></li>
</ul>
</section>
<section id="id64">
<h3><strong>5.3.2 文献综述</strong><a class="headerlink" href="#id64" title="Link to this heading">#</a></h3>
<p>文本检测由两个相关但相对独立的任务组成：1）文本定位，2）文本识别。现有的文本检测方法可以分为两组：“分步检测”和“集成检测”。我们建议读者参考以下综述以获取更多详细信息[338, 339]。</p>
<ul class="simple">
<li><p><strong>分步检测与集成检测</strong> 分步检测方法[340, 341]由一系列处理步骤组成，包括字符分割、候选区域验证、字符分组和单词识别。这类方法的优点是大部分背景可以在粗分割步骤中被过滤掉，大大减少了后续处理的计算成本。缺点是所有步骤的参数都需要仔细设置，错误会在每个步骤中发生并累积。相比之下，集成方法[342–345]将文本检测框架化为一个联合概率推断问题，其中字符定位、分组和识别步骤在统一框架下处理。这些方法的优点是避免了累积错误，易于集成语言模型，缺点是当考虑大量字符类别和候选窗口时，推断将变得计算昂贵[339]。</p></li>
<li><p><strong>传统方法与深度学习方法</strong> 大多数传统的文本检测方法以无监督的方式生成文本候选项，常用的技术包括最大稳定极值区域（MSER）分割[341]和形态学滤波[346]。这些方法中还考虑了一些领域知识，如文本的对称性和笔画结构[340, 341, 347]。近年来，研究人员更多地关注文本定位问题而非识别。最近提出了两组方法。第一组方法将文本检测视为一般目标检测的特殊情况[251, 348–357]。这些方法具有统一的检测框架，但对检测具有方向或大宽高比的文本效果不佳。第二组方法将文本检测视为图像分割问题[336, 337, 358–360]。这些方法的优点是对于文本的形状和方向没有特殊限制，但缺点是基于分割结果不容易区分密集排列的文本行。最近基于深度学习的文本检测方法提出了解决上述问题的方案。对于文本旋转和平移变化：解决这个问题的常见解决方案是在锚框和RoI池化层中引入与旋转和平移变化相关的额外参数[351–353, 355–357]。为了改进密集排列文本的检测：基于分割的方法在检测密集排列的文本方面显示出更多优势。为了区分相邻的文本行，最近提出了两种解决方案。第一种是“分割和链接”，其中“分割”指的是字符热图，“链接”指的是两个相邻段之间的连接，表明它们属于同一个单词或文本行[336, 358]。第二组是引入额外的角/边界检测任务来帮助分离密集排列的文本，其中一组角或一个封闭边界对应于一个单独的文本行[337, 359, 360]。为了改进断裂和模糊文本的检测：最近处理断裂和模糊文本的想法是使用单词级[77, 361]识别和句子级识别[335]。对于不同字体的文本，最有效的方法是使用合成样本进行训练[77, 348]。</p></li>
</ul>
</section>
</section>
<section id="id65">
<h2><strong>5.4 交通标志和交通信号检测</strong><a class="headerlink" href="#id65" title="Link to this heading">#</a></h2>
<p>随着自动驾驶技术的发展，自动检测交通标志和交通信号近年来引起了极大的关注。在过去的几十年中，尽管计算机视觉社区主要推动了对一般对象的检测，而不是像交通灯和交通标志这样的固定模式，但认为它们的识别并不具有挑战性将是一个错误。</p>
<section id="id66">
<h3><strong>5.4.1 困难和挑战</strong><a class="headerlink" href="#id66" title="Link to this heading">#</a></h3>
<p>交通标志/灯光检测的挑战和困难可以总结如下：</p>
<ul class="simple">
<li><p><strong>照明变化</strong>：当驾驶进入阳光眩光或在夜间时，检测将特别困难，如图23(a)所示。</p></li>
<li><p><strong>运动模糊</strong>：由于汽车的运动，由车载摄像头捕获的图像将变得模糊，如图23(b)所示。</p></li>
<li><p><strong>恶劣天气</strong>：在恶劣天气下，例如下雨和下雪天，图像质量将受到影响，如图23(c)所示。</p></li>
<li><p><strong>实时检测</strong>：这对于自动驾驶尤为重要。</p></li>
</ul>
</section>
<section id="id67">
<h3><strong>5.4.2 文献综述</strong><a class="headerlink" href="#id67" title="Link to this heading">#</a></h3>
<p>现有的交通标志/灯光检测方法可以分为两组：1）传统检测方法，2）基于深度学习的检测方法。我们建议读者参考以下综述以获取有关此主题的更多详细信息[80]。</p>
<ul class="simple">
<li><p><strong>传统检测方法</strong> 基于视觉的交通标志/灯光检测研究可以追溯到20年前[362, 363]。由于交通标志/灯光具有特定的形状和颜色，传统的检测方法通常基于颜色阈值[364–368]、视觉显著性检测[369]、形态学滤波[79]和边缘/轮廓分析[370, 371]。由于上述方法仅基于低级视觉设计，它们通常在复杂环境下失败（如图23所示），因此，一些研究人员开始寻找超越视觉方法的解决方案，例如在交通灯检测中结合GPS和数字地图[372, 373]。尽管“特征金字塔 + 滑动窗口”已经成为当时一般目标检测和行人检测的标准框架，除了极少数工作[374]，交通标志/灯光检测方法的主流直到2010年才遵循这一范式[375–377]。</p></li>
<li><p><strong>基于深度学习的检测方法</strong> 在深度学习时代，一些知名的检测器，如Faster RCNN和SSD，已应用于交通标志/灯光检测任务[83, 84, 378, 379]。在这些检测器的基础上，一些新技术，如注意力机制和对抗性训练，已被用于提高在复杂交通环境下的检测[290, 378]。</p></li>
</ul>
</section>
</section>
<section id="id68">
<h2><strong>5.5 遥感目标检测</strong><a class="headerlink" href="#id68" title="Link to this heading">#</a></h2>
<p>遥感成像技术为人们更好地了解地球打开了一扇门。近年来，随着遥感图像分辨率的提高，遥感目标检测（例如，检测飞机、船只、油罐等）已成为研究热点。遥感目标检测有广泛的应用，如军事调查、灾难救援和城市交通管理。</p>
<section id="id69">
<h3><strong>5.5.1 困难和挑战</strong><a class="headerlink" href="#id69" title="Link to this heading">#</a></h3>
<p>遥感目标检测的挑战和困难可以总结如下：</p>
<ul class="simple">
<li><p><strong>大数据检测</strong>：由于遥感图像的巨大数据量，如何快速准确地检测遥感目标仍然是一个问题。图 24(a) 显示了遥感图像与自然图像之间的数据量比较。</p></li>
<li><p><strong>遮挡目标</strong>：每天有超过 50% 的地球表面被云层覆盖。图 24(b) 展示了一些被遮挡目标的示例。</p></li>
<li><p><strong>领域适应</strong>：由不同传感器捕获的遥感图像（例如，具有不同的模态和分辨率）呈现出高度差异。</p></li>
</ul>
</section>
<section id="id70">
<h3><strong>5.5.2 文献综述</strong><a class="headerlink" href="#id70" title="Link to this heading">#</a></h3>
<p>我们建议读者参考以下综述以获取有关此主题的更多详细信息[90, 382]。</p>
<ul class="simple">
<li><p><strong>传统检测方法</strong> 大多数传统的遥感目标检测方法遵循两阶段检测范式：1）候选提取，2）目标验证。在候选提取阶段，一些常用的方法包括基于灰度值的过滤方法[383, 384]、基于视觉显著性的方法[385–388]、基于小波变换的方法[389]、基于异常检测的方法[390]等。上述方法的一个相似之处在于它们都是无监督方法，因此在复杂环境中通常会失败。在目标验证阶段，一些常用的特征包括 HOG[390, 391]、LBP[384]、SIFT[386, 388, 392]等。此外，还有一些其他方法遵循滑动窗口检测范式[391–394]。为了检测具有特定结构和形状的目标，例如油罐和近海船只，一些领域知识被使用。例如，油罐检测可以被视为圆/弧检测问题[395, 396]。近海船只检测可以被视为检测前甲板和船尾[397, 398]。为了改进遮挡目标检测，一个常用的思路是“通过部分检测”[380, 399]。为了检测具有不同方向的目标，使用“混合模型”通过为不同方向的目标训练不同的检测器[273]。</p></li>
<li><p><strong>基于深度学习的检测方法</strong> 在 RCNN 在 2014 年取得巨大成功后，深度 CNN 很快就被应用于遥感目标检测[275, 276, 400, 401]。像 Faster RCNN 和 SSD 这样的通用目标检测框架在遥感社区引起了越来越多的关注[91, 167, 381, 402–405]。由于遥感图像与日常图像之间存在巨大差异，一些研究已经对深度 CNN 特征对遥感图像的有效性进行了调查[406–408]。人们发现，尽管取得了巨大成功，但深度 CNN 在光谱数据上并不比传统方法更好[406]。为了检测具有不同方向的目标，一些研究人员改进了 ROI 池化层以实现更好的旋转不变性[272, 409]。为了改进领域适应，一些研究人员从贝叶斯视角制定了检测，即在检测阶段，模型根据测试图像的分布自适应更新[91]。此外，注意力机制和特征融合策略也被用于改进小目标检测[410, 411]。</p></li>
</ul>
</section>
</section>
</section>
<section id="id71">
<h1><strong>6 结论和未来方向</strong><a class="headerlink" href="#id71" title="Link to this heading">#</a></h1>
<p>在过去的 20 年中，目标检测取得了显著的成就。本文不仅广泛回顾了一些里程碑检测器（例如 VJ 检测器、HOG 检测器、DPM、Faster-RCNN、YOLO、SSD 等）、关键技术、加速方法、检测应用、数据集和指标，还讨论了社区当前面临的挑战，以及如何进一步扩展和改进这些检测器。目标检测的未来研究可能关注但不限于以下方面：</p>
<ul class="simple">
<li><p><strong>轻量级目标检测</strong>：加速检测算法，使其能够在移动设备上平稳运行。一些重要应用包括移动增强现实、智能相机、面部验证等。尽管近年来已做出巨大努力，但机器与人类眼睛之间的速度差距仍然很大，特别是在检测一些小型对象时。</p></li>
<li><p><strong>检测与 AutoML 的结合</strong>：最近的深度学习基检测器变得越来越复杂，严重依赖经验。未来的一个方向是通过使用神经架构搜索来减少设计检测模型（例如，如何设计引擎和如何设置锚框）时的人工干预。AutoML 可能是目标检测的未来。</p></li>
<li><p><strong>检测与领域适应的结合</strong>：任何目标检测器的训练过程本质上都可以考虑为在独立同分布（i.i.d.）数据假设下的似然估计过程。处理非 i.i.d. 数据的目标检测，特别是对于一些现实世界的应用，仍然是一个挑战。GAN 在领域适应方面显示出了有希望的结果，可能对未来的目标检测大有帮助。</p></li>
<li><p><strong>弱监督检测</strong>：基于深度学习的训练通常依赖于大量手动标注的图像。标注过程耗时、昂贵且效率低下。开发弱监督检测技术，其中检测器仅使用图像级注释或部分使用边界框注释进行训练，对于减少劳动成本和提高检测灵活性具有重要意义。</p></li>
<li><p><strong>小目标检测</strong>：在大场景中检测小目标一直是一个挑战。这一研究方向的一些潜在应用包括使用遥感图像统计野生动物的数量和检测一些重要军事目标的状态。一些进一步的方向可能包括集成视觉注意力机制和设计高分辨率轻量级网络。</p></li>
<li><p><strong>视频检测</strong>：在高清视频中实时进行目标检测/跟踪对视频监控和自动驾驶至关重要。传统的目标检测器通常设计用于图像检测，而忽略了视频帧之间的相关性。通过探索空间和时间相关性来改进检测是一个重要的研究方向。</p></li>
<li><p><strong>信息融合检测</strong>：使用多源/模态数据进行目标检测，例如 RGB-D 图像、3D 点云、LiDAR 等，对自动驾驶和无人机应用至关重要。一些开放问题包括：如何将训练良好的检测器迁移到不同模态的数据上，以及如何进行信息融合以改进检测。</p></li>
</ul>
<p>站在技术演变的高速公路上，我们相信本文将帮助读者构建目标检测的大局观，并找到这个快速发展的研究领域的未来方向。</p>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#"><strong>目标检测二十年：一项综述</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2"><strong>摘要</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id3"><strong>1 引言</strong></a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id4"><strong>2 目标检测二十年</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5"><strong>2.1 目标检测的路线图</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6"><strong>2.1.1 里程碑：传统检测器</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cnn"><strong>2.1.2 里程碑：基于 CNN 的两阶段检测器</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7"><strong>2.2 目标检测数据集和评价指标</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8"><strong>2.2.1 评价指标</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id9"><strong>2.3 目标检测中的技术演变</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10"><strong>2.3.1 早期的暗知识</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11"><strong>2.3.2 多尺度检测的技术演变</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12"><strong>2.3.3 边界框回归的技术演变</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13"><strong>2.3.4 上下文引导的技术演变</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14"><strong>2.3.5 非极大值抑制的技术演变</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id15"><strong>3 加速检测</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id16"><strong>3.1 特征图共享计算</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17"><strong>3.1.1 空间计算冗余和加速</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18"><strong>3.1.2 尺度计算冗余和加速</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id19"><strong>3.2 加速分类器</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id20"><strong>3.3 级联检测</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id21"><strong>3.4 网络剪枝和量化</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id22"><strong>3.4.1 网络剪枝</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id23"><strong>3.4.2 网络量化</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id24"><strong>3.4.3 网络蒸馏</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id25"><strong>3.5 轻量级网络设计</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id26"><strong>3.5.1 分解卷积</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id27"><strong>3.5.2 组卷积</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id28"><strong>3.5.3 深度可分离卷积</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id29"><strong>3.5.4 瓶颈设计</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id30"><strong>3.5.5 神经架构搜索</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id31"><strong>3.6 数值加速</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id32"><strong>3.6.1 使用积分图像加速</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id33"><strong>3.6.2 在频域中加速</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id34"><strong>3.6.3向量量化</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id35"><strong>3.6.4 降秩近似</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id36"><strong>4 最近目标检测的进展</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id37"><strong>4.1 使用更好的引擎进行检测</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id38"><strong>4.2 使用更好的特征进行检测</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id39"><strong>4.2.1 为什么特征融合很重要？</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id40"><strong>4.2.2 不同方式的特征融合</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id41"><strong>4.2.3 学习具有大接受场的高分辨率特征</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id42"><strong>4.3 非滑动窗口检测</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id43"><strong>4.4 提高定位的改进</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id44"><strong>4.4.1 边界框细化</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id45"><strong>4.4.2 改进损失函数以实现精确定位</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id46"><strong>4.5 与分割结合的学习</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id47"><strong>4.5.1 为什么分割改进检测？</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id48"><strong>4.5.2 分割如何改进检测？</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id49"><strong>4.6 旋转和平移变化的鲁棒检测</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id50"><strong>4.6.1 旋转鲁棒检测</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id51"><strong>4.6.2 平移鲁棒检测</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id52"><strong>4.7 从头开始训练</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id53"><strong>4.8 对抗性训练</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id54"><strong>4.9 弱监督目标检测</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id55"><strong>5 应用</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id56"><strong>5.1 行人检测</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id57"><strong>5.1.1 困难和挑战</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id58"><strong>5.1.2 文献综述</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id59"><strong>5.2 面部检测</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id60"><strong>5.2.1 困难和挑战</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id61"><strong>5.2.2 文献综述</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id62"><strong>5.3 文本检测</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id63"><strong>5.3.1 困难和挑战</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id64"><strong>5.3.2 文献综述</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id65"><strong>5.4 交通标志和交通信号检测</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id66"><strong>5.4.1 困难和挑战</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id67"><strong>5.4.2 文献综述</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id68"><strong>5.5 遥感目标检测</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id69"><strong>5.5.1 困难和挑战</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id70"><strong>5.5.2 文献综述</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id71"><strong>6 结论和未来方向</strong></a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By isLinXu
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022, isLinXu.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>