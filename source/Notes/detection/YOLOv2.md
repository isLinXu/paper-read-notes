# YOLOv2

**标题：** YOLO9000: Better, Faster, Stronger

**作者：** Joseph Redmon, Ali Farhadi

**单位：** University of Washington, Allen Institute for AI

**网址：** http://pjreddie.com/yolo9000/

**摘要：**
本文介绍了YOLO9000，一个先进的实时目标检测系统，能够检测超过9000个对象类别。首先，作者提出了对YOLO检测方法的各种改进，包括新颖的和借鉴以往工作的改进。改进后的模型，YOLOv2，在标准的检测任务上如PASCAL VOC和COCO达到了最先进的水平。使用一种新颖的多尺度训练方法，同一个YOLOv2模型可以在不同尺寸下运行，提供了在速度和准确性之间的简单权衡。在67 FPS下，YOLOv2在VOC 2007上达到了76.8 mAP。在40 FPS下，YOLOv2达到了78.6 mAP，超过了如Faster RCNN和SSD等最先进的方法，同时运行速度仍然显著更快。最后，作者提出了一种在目标检测和分类上联合训练的方法。使用这种方法，作者同时在COCO检测数据集和ImageNet分类数据集上训练了YOLO9000。这种联合训练允许YOLO9000预测那些没有标记检测数据的对象类别的检测。作者在ImageNet检测任务上验证了他们的方法，YOLO9000在ImageNet检测验证集上达到了19.7 mAP，尽管只有200个类别中的44个有检测数据。在COCO之外的156个类别上，YOLO9000达到了16.0 mAP。但YOLO能检测的不仅仅是200个类别；它预测了超过9000个不同对象类别的检测，并且仍然实时运行。

**1、这篇论文试图解决的问题：**
论文试图解决的问题是提高目标检测系统的速度、准确性，并扩大其能够检测的对象类别数量。具体来说，作者想要改进YOLO（You Only Look Once）目标检测系统，使其能够实时检测更多的对象类别，同时保持或提高准确性和速度。

**2、这是否是一个新的问题：**
这不是一个全新的问题，因为目标检测是计算机视觉领域的一个长期存在的研究问题。然而，作者在现有基础上提出了新的改进和方法，使得YOLO系统能够检测更多的类别，并且在速度和准确性上有所提升。

**3、这篇文章要验证的科学假设：**
文章的核心科学假设是，通过提出的方法改进，YOLO系统能够在保持实时性能的同时，提高检测的准确性，并且能够扩展到检测更多的对象类别。

**4、有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？**
相关研究包括但不限于：
- R-CNN系列：Region-based Convolutional Neural Networks，包括Fast R-CNN、Faster R-CNN等。
- SSD：Single Shot MultiBox Detector。
- 其他实时目标检测系统。
这些研究可以归类为计算机视觉中的目标检测领域。领域内值得关注的研究员包括但不限于Joseph Redmon、Ali Farhadi、Ross Girshick等。

**5、论文中提到的解决方案之关键是什么：**
解决方案的关键包括：
- 对YOLO检测方法的多项改进，如批归一化、高分辨率分类器、使用锚框、通过k-means确定锚框尺寸等。
- 多尺度训练方法，允许模型在不同尺寸下运行。
- 联合训练目标检测和分类数据集，使用WordTree结构来整合不同数据集的类别标签。

**6、论文中的实验是如何设计的：**
实验设计包括在标准的检测任务上评估YOLOv2的性能，如PASCAL VOC和COCO数据集，并与当时的最先进方法进行比较。此外，还包括使用COCO检测数据集和ImageNet分类数据集来训练YOLO9000，并在ImageNet检测任务上进行评估。

**7、用于定量评估的数据集上什么？代码有没有开源？**
用于定量评估的数据集包括PASCAL VOC 2007、COCO test-dev2015和ImageNet检测验证集。代码和预训练模型已经在论文提供的网址上开源。

**8、论文中的实验及结果有没有很好地支持需要验证的科学假设？**
是的，实验结果支持了作者的科学假设。YOLOv2在多个数据集上达到了最先进的性能，同时在不同FPS下保持了高mAP，证明了其速度和准确性的提升。YOLO9000在没有检测数据的类别上也表现出了良好的性能，证明了其能够通过联合训练检测更多类别的能力。

**9、这篇论文到底有什么贡献？**
论文的贡献包括：
- 提出了改进的YOLOv2模型，该模型在多个数据集上达到了最先进的性能。
- 引入了多尺度训练方法，提高了模型的灵活性。
- 提出了一种联合训练目标检测和分类数据集的方法，显著扩展了检测系统能够识别的类别数量。
- 开源了代码和预训练模型，促进了研究社区的进一步研究。

**10、下一步呢？有什么工作可以继续深入？**
下一步的工作可能包括：
- 探索更强大的模型架构来进一步提升检测性能。
- 研究更精细的标签分配策略，以改善在训练期间对分类数据的弱监督。
- 将这些技术应用于其他视觉任务，如图像分割。
- 使用这些技术来处理不同来源和结构的数据，以构建更强大的视觉世界模型。


---

<img width="689" alt="yolov2-fig1" src="https://github.com/isLinXu/issues/assets/59380685/541b16b4-3e63-4eb0-9a0b-07106b694337">

这个图表展示了在VOC和COCO数据集上进行边界框维度聚类的结果。通过k-means聚类算法，选择不同的聚类数目（k）来获得模型的良好先验。图表分为两部分：左图显示了不同聚类数目下的平均IOU（Intersection over Union），右图显示了VOC和COCO数据集的相对聚类中心。以下是对图表中关键信息的提取和总结分析：

关键信息提取

左图：平均IOU vs. 聚类数目（# Clusters）

- **VOC数据集**：
    - 用灰色三角形表示。
    - 随着聚类数目（k）的增加，平均IOU逐渐上升并趋于平稳。
    - 在k=5时，平均IOU约为0.65，之后增加的幅度较小。
- **COCO数据集**：
    - 用蓝色圆点表示。
    - 随着聚类数目（k）的增加，平均IOU逐渐上升并趋于平稳。
    - 在k=5时，平均IOU约为0.55，之后增加的幅度较小。

右图：VOC和COCO数据集的相对聚类中心

- **VOC数据集**：
    - 聚类中心显示为灰色矩形。
    - 聚类中心的矩形较为均匀，尺寸变化较小，主要集中在较细长的形状。
- **COCO数据集**：
    - 聚类中心显示为蓝色矩形。
    - 聚类中心的矩形尺寸变化较大，显示出更大的尺寸多样性。

总结分析

1. **平均IOU与聚类数目的关系**：
    
    - 对于VOC和COCO数据集，随着聚类数目（k）的增加，平均IOU逐渐上升并趋于平稳。
    - 在k=5时，VOC数据集的平均IOU约为0.65，COCO数据集的平均IOU约为0.55。此时，增加聚类数目对平均IOU的提升效果较小，表明k=5是一个较好的折中选择。
2. **VOC和COCO数据集的聚类中心**：
    
    - VOC数据集的聚类中心较为均匀，尺寸变化较小，主要集中在较细长的形状。这表明VOC数据集中的目标物体尺寸较为一致。
    - COCO数据集的聚类中心尺寸变化较大，显示出更大的尺寸多样性。这表明COCO数据集中的目标物体尺寸变化较大，具有更大的多样性。
3. **模型先验的选择**：
    
    - 通过k-means聚类算法，可以为模型提供良好的先验信息。选择k=5时，可以在召回率和模型复杂度之间取得较好的平衡。
    - VOC和COCO数据集的聚类中心显示了不同的数据集特性，VOC数据集更倾向于细长的目标物体，而COCO数据集则具有更大的尺寸多样性。

结论

- **聚类数目k的选择**：在k=5时，VOC和COCO数据集的平均IOU都达到了较高水平，进一步增加聚类数目对平均IOU的提升效果较小，表明k=5是一个较好的折中选择。
- **数据集特性**：VOC数据集中的目标物体尺寸较为一致，主要集中在较细长的形状；而COCO数据集中的目标物体尺寸变化较大，具有更大的多样性。
- **模型先验信息**：通过k-means聚类算法，可以为模型提供良好的先验信息，有助于提高目标检测模型的性能。

综上所述，通过对VOC和COCO数据集进行边界框维度聚类，可以为目标检测模型提供有效的先验信息，选择合适的聚类数目（如k=5）可以在召回率和模型复杂度之间取得较好的平衡。


---

<img width="656" alt="yolov2-fig2" src="https://github.com/isLinXu/issues/assets/59380685/0412ee20-0a21-46d3-9734-5ebbf29e613d">

这个图表展示了一个目标检测网络中边界框的维度先验和位置预测的机制。通过预测边界框的宽度和高度作为聚类中心的偏移量，并使用sigmoid函数预测边界框的中心坐标。以下是对图表中关键信息的提取和总结分析：

关键信息提取

图表内容
- **边界框（Bounding Box）**：
  - 用蓝色矩形表示，表示预测的边界框。
  - 边界框的宽度和高度分别为 \( b_w \) 和 \( b_h \)。
- **聚类中心（Cluster Centroids）**：
  - 用黑色虚线矩形表示，表示聚类中心的边界框。
  - 聚类中心的宽度和高度分别为 \( p_w \) 和 \( p_h \)。
- **中心坐标（Center Coordinates）**：
  - 边界框的中心坐标为 \( (b_x, b_y) \)。
  - 聚类中心的中心坐标为 \( (C_x, C_y) \)。

公式
- **中心坐标预测**：
  - \( b_x = \sigma(t_x) + C_x \)
  - \( b_y = \sigma(t_y) + C_y \)
  - 其中，\( \sigma \) 表示sigmoid函数，\( t_x \) 和 \( t_y \) 是网络预测的偏移量。
- **宽度和高度预测**：
  - \( b_w = p_w e^{t_w} \)
  - \( b_h = p_h e^{t_h} \)
  - 其中，\( t_w \) 和 \( t_h \) 是网络预测的偏移量。

总结分析

1. **中心坐标预测**：
   - 使用sigmoid函数对网络预测的偏移量 \( t_x \) 和 \( t_y \) 进行处理，得到一个在0到1之间的值。
   - 将这个值加上聚类中心的坐标 \( C_x \) 和 \( C_y \)，得到预测边界框的中心坐标 \( b_x \) 和 \( b_y \)。
   - 这种方法确保了预测的中心坐标相对于聚类中心的位置是合理的，并且在特征图的范围内。

2. **宽度和高度预测**：
   - 使用指数函数对网络预测的偏移量 \( t_w \) 和 \( t_h \) 进行处理，得到一个正值。
   - 将这个值乘以聚类中心的宽度 \( p_w \) 和高度 \( p_h \)，得到预测边界框的宽度 \( b_w \) 和高度 \( b_h \)。
   - 这种方法确保了预测的宽度和高度是正值，并且相对于聚类中心的尺寸进行了调整。

3. **维度先验的作用**：
   - 聚类中心的宽度和高度 \( p_w \) 和 \( p_h \) 作为先验信息，提供了一个初始的边界框尺寸。
   - 通过预测偏移量 \( t_w \) 和 \( t_h \)，可以对边界框的尺寸进行微调，使其更符合实际目标物体的尺寸。

4. **位置预测的合理性**：
   - 使用sigmoid函数对中心坐标的偏移量进行处理，确保了预测的中心坐标在特征图的范围内。
   - 使用指数函数对宽度和高度的偏移量进行处理，确保了预测的边界框尺寸为正值，并且可以根据实际情况进行调整。

结论
- **中心坐标预测**：通过sigmoid函数处理偏移量，确保预测的中心坐标在特征图的范围内，并相对于聚类中心的位置合理。
- **宽度和高度预测**：通过指数函数处理偏移量，确保预测的边界框尺寸为正值，并根据聚类中心的尺寸进行调整。
- **维度先验的作用**：聚类中心的宽度和高度作为先验信息，提供了一个初始的边界框尺寸，通过预测偏移量进行微调，使其更符合实际目标物体的尺寸。

综上所述，这种边界框维度先验和位置预测的方法，通过结合聚类中心的先验信息和网络预测的偏移量，能够有效地预测目标物体的边界框，确保预测结果的合理性和准确性。


---

<img width="655" alt="yolov2-fig3" src="https://github.com/isLinXu/issues/assets/59380685/d897c56d-d9a5-4eaa-95b4-d6a28c216ff0">

| 模型                  | Mean Average Precision (mAP) | Frames Per Second (FPS) |
|-----------------------|------------------------------|-------------------------|
| R-CNN                 | 62                           | 低                      |
| Fast R-CNN            | 70                           | 略有提升                |
| Faster R-CNN          | 73                           | 进一步提升              |
| Faster R-CNN ResNet   | 76                           | 与Faster R-CNN相近      |
| SSD300                | 74                           | 46                      |
| SSD512                | 77                           | 略低于SSD300            |
| YOLO                  | 63                           | 45                      |
| YOLOv2                | 70-78                        | 40-90                   |

速度和精度的权衡

- 图中虚线表示30 FPS的分界线，表示实时处理的标准。
- **R-CNN系列**模型的FPS普遍较低，难以达到实时处理的标准。
- **SSD系列**和**YOLO系列**模型的FPS较高，能够达到或超过实时处理的标准。

总结分析

1. **R-CNN系列**：
    
    - **R-CNN**：虽然精度较低（mAP约为62），但速度非常慢，难以应用于实时场景。
    - **Fast R-CNN**：精度有所提升（mAP约为70），速度也有所提高，但仍未达到实时处理的标准。
    - **Faster R-CNN**：精度进一步提升（mAP约为73），速度有所提高，但仍未达到实时处理的标准。
    - **Faster R-CNN ResNet**：精度最高（mAP约为76），但速度仍未达到实时处理的标准。
2. **SSD系列**：
    
    - **SSD300**：在精度（mAP约为74）和速度（FPS约为46）之间取得了较好的平衡，能够达到实时处理的标准。
    - **SSD512**：精度更高（mAP约为77），但速度略低于SSD300，仍能达到实时处理的标准。
3. **YOLO系列**：
    
    - **YOLO**：精度较低（mAP约为63），但速度较快（FPS约为45），能够达到实时处理的标准。
    - **YOLOv2**：在精度（mAP在70到78之间）和速度（FPS在40到90之间）之间取得了非常好的平衡，显示出较高的速度和较高的精度。

结论

- **R-CNN系列**模型虽然在精度上有所提升，但在速度上难以达到实时处理的标准，适合对速度要求不高的应用场景。
- **SSD系列**模型在精度和速度之间取得了较好的平衡，特别是SSD300，能够达到实时处理的标准，适合对速度和精度都有要求的应用场景。
- **YOLO系列**模型，特别是YOLOv2，在精度和速度之间取得了非常好的平衡，显示出较高的速度和较高的精度，适合对实时处理有较高要求的应用场景。

综上所述，YOLOv2在精度和速度上表现出色，是实时目标检测的一个非常好的选择，而SSD系列模型也在精度和速度之间取得了较好的平衡，适合多种应用场景。


---

<img width="491" alt="yolov2-darknet" src="https://github.com/isLinXu/issues/assets/59380685/3d32b01e-9949-45cd-912f-07da9fa2cdf8">

Darknet-19 网络结构总结

Darknet-19 是一种卷积神经网络（CNN），主要用于图像分类任务。它由多个卷积层、最大池化层、全局平均池化层和一个Softmax层组成。以下是对其网络结构的详细总结和输入输出流程的分析。

网络结构

卷积层（Convolutional Layers）
- 卷积层使用不同数量的滤波器（Filters）和不同的卷积核大小（Size/Stride）。
- 卷积核大小主要为 \(3 \times 3\) 和 \(1 \times 1\)。
- 每个卷积层后面通常跟随一个最大池化层（Maxpool）。

最大池化层（Maxpool Layers）
- 最大池化层使用 \(2 \times 2\) 的池化窗口，步幅（Stride）为2。
- 最大池化层用于减少特征图的尺寸，同时保留重要的特征。

全局平均池化层（Global Average Pooling Layer）
- 在最后一个卷积层之后，使用全局平均池化层将特征图的每个通道的平均值作为输出。

Softmax层
- 最后使用一个Softmax层进行分类，输出每个类别的概率。

输入输出流程

输入
- 输入图像的尺寸为 \(224 \times 224 \times 3\)（宽度、高度、通道数）。

输出
- 输出为1000个类别的概率分布。

输入输出流程

1. **输入图像**：尺寸为 \(224 \times 224 \times 3\)。
2. **第1层卷积层**：32个 \(3 \times 3\) 滤波器，输出尺寸为 \(224 \times 224 \times 32\)。
3. **第1层最大池化层**： \(2 \times 2\) 池化窗口，步幅为2，输出尺寸为 \(112 \times 112 \times 32\)。
4. **第2层卷积层**：64个 \(3 \times 3\) 滤波器，输出尺寸为 \(112 \times 112 \times 64\)。
5. **第2层最大池化层**： \(2 \times 2\) 池化窗口，步幅为2，输出尺寸为 \(56 \times 56 \times 64\)。
6. **第3层卷积层**：128个 \(3 \times 3\) 滤波器，输出尺寸为 \(56 \times 56 \times 128\)。
7. **第4层卷积层**：64个 \(1 \times 1\) 滤波器，输出尺寸为 \(56 \times 56 \times 64\)。
8. **第5层卷积层**：128个 \(3 \times 3\) 滤波器，输出尺寸为 \(56 \times 56 \times 128\)。
9. **第3层最大池化层**： \(2 \times 2\) 池化窗口，步幅为2，输出尺寸为 \(28 \times 28 \times 128\)。
10. **第6层卷积层**：256个 \(3 \times 3\) 滤波器，输出尺寸为 \(28 \times 28 \times 256\)。
11. **第7层卷积层**：128个 \(1 \times 1\) 滤波器，输出尺寸为 \(28 \times 28 \times 128\)。
12. **第8层卷积层**：256个 \(3 \times 3\) 滤波器，输出尺寸为 \(28 \times 28 \times 256\)。
13. **第4层最大池化层**： \(2 \times 2\) 池化窗口，步幅为2，输出尺寸为 \(14 \times 14 \times 256\)。
14. **第9层卷积层**：512个 \(3 \times 3\) 滤波器，输出尺寸为 \(14 \times 14 \times 512\)。
15. **第10层卷积层**：256个 \(1 \times 1\) 滤波器，输出尺寸为 \(14 \times 14 \times 256\)。
16. **第11层卷积层**：512个 \(3 \times 3\) 滤波器，输出尺寸为 \(14 \times 14 \times 512\)。
17. **第12层卷积层**：256个 \(1 \times 1\) 滤波器，输出尺寸为 \(14 \times 14 \times 256\)。
18. **第13层卷积层**：512个 \(3 \times 3\) 滤波器，输出尺寸为 \(14 \times 14 \times 512\)。
19. **第5层最大池化层**： \(2 \times 2\) 池化窗口，步幅为2，输出尺寸为 \(7 \times 7 \times 512\)。
20. **第14层卷积层**：1024个 \(3 \times 3\) 滤波器，输出尺寸为 \(7 \times 7 \times 1024\)。
21. **第15层卷积层**：512个 \(1 \times 1\) 滤波器，输出尺寸为 \(7 \times 7 \times 512\)。
22. **第16层卷积层**：1024个 \(3 \times 3\) 滤波器，输出尺寸为 \(7 \times 7 \times 1024\)。
23. **第17层卷积层**：512个 \(1 \times 1\) 滤波器，输出尺寸为 \(7 \times 7 \times 512\)。
24. **第18层卷积层**：1024个 \(3 \times 3\) 滤波器，输出尺寸为 \(7 \times 7 \times 1024\)。
25. **第19层卷积层**：1000个 \(1 \times 1\) 滤波器，输出尺寸为 \(7 \times 7 \times 1000\)。
26. **全局平均池化层**：将每个通道的平均值作为输出，输出尺寸为 \(1 \times 1 \times 1000\)。
27. **Softmax层**：输出1000个类别的概率分布。

结论

Darknet-19 是一个深度卷积神经网络，通过多层卷积和池化操作提取图像特征，最后通过全局平均池化和Softmax层进行分类。其设计使得网络能够在保持较高分类精度的同时，具有较高的计算效率。

---

<img width="567" alt="yolov2-fig5" src="https://github.com/isLinXu/issues/assets/59380685/3983f060-37a7-4b51-88db-bb71392f0a0e">

网络结构分析：ImageNet 1k vs WordTree1k

图中展示了两种不同的分类网络结构：ImageNet 1k 和 WordTree1k。它们在处理分类任务时采用了不同的策略。

ImageNet 1k
- **结构**：使用一个大的Softmax层来预测1000个类别的概率分布。
- **特点**：所有类别共享一个Softmax层，直接输出每个类别的概率。
- **优点**：结构简单，易于实现。
- **缺点**：对于类别之间的层次关系没有利用，可能在处理具有层次结构的类别时表现不佳。

WordTree1k
- **结构**：使用多个Softmax层来预测概率分布，每个Softmax层对应一个类别层次。
- **特点**：将类别分成多个层次，每个层次使用一个Softmax层进行预测。
- **优点**：利用了类别之间的层次关系，可以更好地处理具有层次结构的类别。
- **缺点**：结构复杂，实现和训练可能更为困难。

输入输出流程

ImageNet 1k
1. **输入**：图像数据。
2. **特征提取**：通过卷积神经网络提取图像特征。
3. **Softmax层**：将提取的特征输入到一个大的Softmax层，输出1000个类别的概率分布。
4. **输出**：每个类别的概率。

WordTree1k
1. **输入**：图像数据。
2. **特征提取**：通过卷积神经网络提取图像特征。
3. **层次Softmax层**：
   - **第一级Softmax层**：预测大类的概率分布。
   - **第二级Softmax层**：根据第一级的预测结果，进一步预测子类的概率分布。
   - **第三级Softmax层**：根据第二级的预测结果，进一步预测更细粒度的子类的概率分布。
4. **输出**：每个类别的概率，通过多个Softmax层的组合得到最终的类别概率。

结论

- **ImageNet 1k**：适用于类别之间没有明显层次关系的分类任务，结构简单，易于实现和训练。
- **WordTree1k**：适用于类别之间具有层次关系的分类任务，通过多个Softmax层的组合，可以更好地利用类别之间的层次关系，提高分类性能。

在实际应用中，选择哪种结构取决于具体的分类任务和类别的组织方式。如果类别之间具有明显的层次关系，WordTree1k可能会表现更好；如果类别之间没有明显的层次关系，ImageNet 1k则是一个更简单和直接的选择。