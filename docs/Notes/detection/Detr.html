
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>DETR &#8212; 论文阅读笔记</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/plot_directive.css?v=7f9a90b1" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../../_static/documentation_options.js?v=40d2fe7a"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Notes/detection/Detr';</script>
    <link rel="icon" href="../../_static/panda.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="RT-DETR" href="RT-DETR.html" />
    <link rel="prev" title="YOLO-MS" href="YOLO-MS.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="论文阅读笔记 - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="论文阅读笔记 - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">目录</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Method/index.html">论文阅读指南</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Method/efficent_read_paper.html">高效阅读方法及流程</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Method/how_to_read_paper.html">如何阅读论文</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Method/paper_10_question.html">论文速读十问</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Method/read_important_tips.html">读论文与口头报告的几项重点</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Method/reference.html">参考材料</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../List/index.html">论文阅读清单</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../List/basis.html">神经网络基础(basis)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../List/attention.html">注意力部分(attention)</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../List/batch_normalization.html">批量&amp;正则化(batch&amp;normalization)</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../List/classification.html">图像分类(CLAS)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../List/convolutional.html">高级卷积网络知识(Convolutional)</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../List/gan.html">AI合成部分(GAN)</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../List/nlp.html">自然语言处理(NLP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../List/objectdetection.html">目标检测(OBJ)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../List/rnn.html">循环神经网络(RNN)</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../List/segementation.html">图像分割(SEG)</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../List/transformer.html">Transformer</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../List/multimodal.html">多模态(MultiModal Learning)</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../List/llm.html">大语言模型(Large Language Models)</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../index.html">论文阅读笔记</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 has-children"><a class="reference internal" href="../mm-l/index.html">MultiModal Machine Learning</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../mm-l/blip-v1.html">BLIP: Bootstrapping Language-Image Pre-training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mm-l/blip-v2.html">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../llm/index.html">Large Language Models</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../llm/opt.html">OPT: OPT : Open Pre-trained Transformer Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../llm/gpt-v1.html">GPT-v1:Improving Language Understanding by Generative Pre-Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../llm/gpt-v2.html">GPT-v2:Language Models are Unsupervised Multitask Learners</a></li>
<li class="toctree-l3"><a class="reference internal" href="../llm/gpt-v3.html">GPT-v3:Language Models are Few-Shot Learners</a></li>
<li class="toctree-l3"><a class="reference internal" href="../llm/gpt-v4.html">GPT-v4:GPT-4 Technical Report</a></li>
</ul>
</li>
<li class="toctree-l2 current active has-children"><a class="reference internal" href="index.html">Object Detection</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="summary.html">summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="RCNN.html">RCNN</a></li>
<li class="toctree-l3"><a class="reference internal" href="Fast%20R-CNN.html">Fast R-CNN</a></li>
<li class="toctree-l3"><a class="reference internal" href="Faster%20R-CNN.html">Faster R-CNN</a></li>
<li class="toctree-l3"><a class="reference internal" href="Mask%20R-CNN.html">Mask R-CNN</a></li>
<li class="toctree-l3"><a class="reference internal" href="FCN.html">FCN</a></li>
<li class="toctree-l3"><a class="reference internal" href="R-FCN.html">R-FCN</a></li>
<li class="toctree-l3"><a class="reference internal" href="FPN.html">FPN</a></li>
<li class="toctree-l3"><a class="reference internal" href="FCOS.html">FCOS</a></li>
<li class="toctree-l3"><a class="reference internal" href="SSD.html">SSD</a></li>
<li class="toctree-l3"><a class="reference internal" href="Mobilenet-SSDv2.html">Mobilenet-SSDv2</a></li>
<li class="toctree-l3"><a class="reference internal" href="VarifocalNet.html">论文阅读笔记</a></li>

<li class="toctree-l3"><a class="reference internal" href="OneNet.html">OneNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="Mask%20R-CNN.html">Mask R-CNN</a></li>
<li class="toctree-l3"><a class="reference internal" href="Cascade-RCNN.html">Cascade-RCNN</a></li>
<li class="toctree-l3"><a class="reference internal" href="RetinaNet.html">RetinaNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="FemtoDet.html">FemtoDet</a></li>
<li class="toctree-l3"><a class="reference internal" href="SparseInst.html">SparseInst</a></li>
<li class="toctree-l3"><a class="reference internal" href="YOLOv1.html">YOLOv1</a></li>
<li class="toctree-l3"><a class="reference internal" href="YOLOv2.html">YOLOv2</a></li>
<li class="toctree-l3"><a class="reference internal" href="YOLOv3.html">YOLOv3</a></li>
<li class="toctree-l3"><a class="reference internal" href="YOLOv4.html">YOLOv4</a></li>
<li class="toctree-l3"><a class="reference internal" href="Scaled-YOLOv4.html">Scaled-YOLOv4</a></li>
<li class="toctree-l3"><a class="reference internal" href="Edge-YOLO.html">Edge-YOLO</a></li>
<li class="toctree-l3"><a class="reference internal" href="MS-DAYOLO.html">MS-DAYOLO</a></li>
<li class="toctree-l3"><a class="reference internal" href="ASFF.html">ASFF</a></li>
<li class="toctree-l3"><a class="reference internal" href="ATSS.html">ATSS</a></li>
<li class="toctree-l3"><a class="reference internal" href="SABL.html">SABL</a></li>
<li class="toctree-l3"><a class="reference internal" href="SM-NAS.html">SM-NAS</a></li>
<li class="toctree-l3"><a class="reference internal" href="TSD.html">TSD</a></li>
<li class="toctree-l3"><a class="reference internal" href="RDSNet.html">RDSNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="CenterMask.html">CenterMask</a></li>
<li class="toctree-l3"><a class="reference internal" href="EfficientDet.html">EfficientDet</a></li>
<li class="toctree-l3"><a class="reference internal" href="Simple%20Multi-dataset%20Detection.html">Simple Multi-dataset Detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="YOLOX.html">YOLOX</a></li>
<li class="toctree-l3"><a class="reference internal" href="YOLOv6.html">YOLOv6</a></li>
<li class="toctree-l3"><a class="reference internal" href="PP-YOLOv1.html">PP-YOLOv1</a></li>
<li class="toctree-l3"><a class="reference internal" href="PP-YOLOv2.html">PP-YOLOv2</a></li>
<li class="toctree-l3"><a class="reference internal" href="PP-YOLOE.html">PP-YOLOE</a></li>
<li class="toctree-l3"><a class="reference internal" href="YOLOF.html">YOLOF</a></li>
<li class="toctree-l3"><a class="reference internal" href="YOLOP.html">YOLOP</a></li>
<li class="toctree-l3"><a class="reference internal" href="YOLOR.html">YOLOR</a></li>
<li class="toctree-l3"><a class="reference internal" href="YOLOS.html">YOLOS</a></li>
<li class="toctree-l3"><a class="reference internal" href="YOLOv7.html">YOLOv7</a></li>
<li class="toctree-l3"><a class="reference internal" href="Dy-yolov7.html">DY-yolov7</a></li>
<li class="toctree-l3"><a class="reference internal" href="Gold-YOLO.html">Gold-YOLO</a></li>
<li class="toctree-l3"><a class="reference internal" href="YOLOv6_v3.0.html">YOLOv6</a></li>
<li class="toctree-l3"><a class="reference internal" href="DAMO-YOLO.html">DAMO-YOLO</a></li>
<li class="toctree-l3"><a class="reference internal" href="ViT-YOLO.html">ViT-YOLO</a></li>
<li class="toctree-l3"><a class="reference internal" href="YOLO-MS.html">YOLO-MS</a></li>
<li class="toctree-l3 current active"><a class="current reference internal" href="#">DETR</a></li>
<li class="toctree-l3"><a class="reference internal" href="RT-DETR.html">RT-DETR</a></li>
<li class="toctree-l3"><a class="reference internal" href="YOLOv9.html">YOLOv9</a></li>
<li class="toctree-l3"><a class="reference internal" href="YOLOOC.html">YOLOOC</a></li>
<li class="toctree-l3"><a class="reference internal" href="FemtoDet.html">FemtoDet</a></li>
<li class="toctree-l3"><a class="reference internal" href="MS-DAYOLO.html">MS-DAYOLO</a></li>
<li class="toctree-l3"><a class="reference internal" href="OneNet.html">OneNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="Sparse%20R-CNN.html">Sparse R-CNN</a></li>
<li class="toctree-l3"><a class="reference internal" href="SparseInst.html">SparseInst</a></li>
<li class="toctree-l3"><a class="reference internal" href="OWL-ViT.html">OWL-ViT</a></li>
<li class="toctree-l3"><a class="reference internal" href="OWLv2.html">OWLv2</a></li>
<li class="toctree-l3"><a class="reference internal" href="RTMDet.html">RTMDet</a></li>
<li class="toctree-l3"><a class="reference internal" href="YOLO-World.html">YOLO-World</a></li>
<li class="toctree-l3"><a class="reference internal" href="YOLOOC.html">YOLOOC</a></li>
<li class="toctree-l3"><a class="reference internal" href="MDETR.html">MDETR</a></li>
<li class="toctree-l3"><a class="reference internal" href="YOLOv10.html">YOLOv10</a></li>
<li class="toctree-l3"><a class="reference internal" href="%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B020%E5%B9%B4.html"><strong>目标检测二十年：一项综述</strong></a></li>





<li class="toctree-l3"><a class="reference internal" href="yolo%E7%BB%BC%E8%BF%B0.html"><strong>YOLO的全面综述：从YOLOv1到YOLOv8及未来</strong></a></li>




















</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Read/index.html">论文阅读记录</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul class="simple">
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Summary/index.html">论文阅读总结</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul class="simple">
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/isLinXu/paper-read-notes" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/isLinXu/paper-read-notes/edit/main/Notes/detection/Detr.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/isLinXu/paper-read-notes/issues/new?title=Issue%20on%20page%20%2FNotes/detection/Detr.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/Notes/detection/Detr.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>DETR</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">回答问题</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="detr">
<h1>DETR<a class="headerlink" href="#detr" title="Link to this heading">#</a></h1>
<p><strong>标题：</strong> End-to-End Object Detection with Transformers</p>
<p><strong>作者：</strong> Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko</p>
<p><strong>机构：</strong> Facebook AI</p>
<p><strong>摘要：</strong> 本文提出了一种新的对象检测方法，将对象检测视为直接的集合预测问题。该方法简化了检测流程，去除了多个手工设计的组件，如非极大值抑制过程或锚点生成等，这些组件明确编码了我们对任务的先验知识。新框架称为DEtection TRansformer（DETR），其主要成分是一套基于全局损失的集合预测，通过二分图匹配强制进行唯一预测，以及一个基于transformer的编码器-解码器架构。DETR通过固定数量的已学习对象查询，直接并行输出最终的预测集合，简化了检测流程。DETR概念简单，不需要专门的库，与许多其他现代检测器不同。DETR在COCO对象检测数据集上展示了与高度优化的Faster RCNN基线相当的准确性和运行时间性能。此外，DETR可以轻松地推广到以统一的方式产生全景分割，并显著优于竞争基线。训练代码和预训练模型可在GitHub上获得。</p>
<p><strong>1. 工作内容与动机：</strong></p>
<ul class="simple">
<li><p>工作内容：提出了一种端到端的对象检测方法DETR，使用transformers进行集合预测，简化了传统对象检测流程。</p></li>
<li><p>动机：传统对象检测方法依赖于手工设计的组件，这些组件限制了性能并增加了复杂性。DETR旨在通过直接集合预测方法简化这一流程。</p></li>
</ul>
<p><strong>2. 试图解决的问题：</strong></p>
<ul class="simple">
<li><p>解决的问题：传统对象检测方法中的手工设计组件导致性能受限和流程复杂。</p></li>
</ul>
<p><strong>3. 是否是新问题：</strong></p>
<ul class="simple">
<li><p>不是全新的问题，但提出的解决方案是新颖的，将transformers应用于对象检测任务。</p></li>
</ul>
<p><strong>4. 科学假设：</strong></p>
<ul class="simple">
<li><p>假设：transformers能够有效地用于对象检测任务，并且能够通过集合预测简化检测流程。</p></li>
</ul>
<p><strong>5. 相关研究：</strong></p>
<ul class="simple">
<li><p>相关领域包括集合预测、编码器-解码器架构、并行解码和对象检测方法。</p></li>
<li><p>归类：将DETR归类为直接集合预测方法，与传统的基于锚点或提议的方法相对。</p></li>
<li><p>值得关注的研究员：论文作者团队，以及在transformers和对象检测领域有重要贡献的研究员。</p></li>
</ul>
<p><strong>6. 解决方案关键：</strong></p>
<ul class="simple">
<li><p>关键：使用transformer架构进行集合预测，并通过二分图匹配损失进行端到端训练。</p></li>
</ul>
<p><strong>7. 实验设计：</strong></p>
<ul class="simple">
<li><p>实验设计：在COCO数据集上评估DETR，并与传统的Faster RCNN方法进行比较。</p></li>
</ul>
<p><strong>8. 数据集与代码开源：</strong></p>
<ul class="simple">
<li><p>使用的数据集：COCO 2017检测和全景分割数据集。</p></li>
<li><p>代码开源：是的，训练代码和预训练模型可在GitHub上获得。</p></li>
</ul>
<p><strong>9. 实验结果与科学假设：</strong></p>
<ul class="simple">
<li><p>实验结果：DETR在COCO数据集上达到了与Faster RCNN相当的性能，尤其是在大物体检测上表现更好。</p></li>
<li><p>支持假设：结果支持了transformers能够有效进行对象检测的假设。</p></li>
</ul>
<p><strong>10. 论文贡献：</strong></p>
<ul class="simple">
<li><p>提出了一种新的端到端对象检测框架DETR，简化了对象检测流程，并在COCO数据集上展示了竞争性能。</p></li>
<li><p>证明了transformers可以有效地应用于对象检测任务。</p></li>
</ul>
<p><strong>11. 下一步工作：</strong></p>
<ul class="simple">
<li><p>改进DETR在小物体检测上的性能。</p></li>
<li><p>探索DETR在其他视觉任务中的应用。</p></li>
<li><p>进一步优化DETR的训练和推理效率。</p></li>
</ul>
<section id="id1">
<h2>回答问题<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><strong>这篇论文做了什么工作，它的动机是什么？</strong> 论文提出了一种新的端到端对象检测方法DETR，使用transformers进行集合预测，以简化传统对象检测流程并提高性能。</p></li>
<li><p><strong>这篇论文试图解决什么问题？</strong> 论文试图解决传统对象检测方法中手工设计组件导致的性能受限和流程复杂的问题。</p></li>
<li><p><strong>这是否是一个新的问题？</strong> 不是全新的问题，但提出的解决方案是新颖的。</p></li>
<li><p><strong>这篇文章要验证一个什么科学假设？</strong> 验证transformers能够有效地用于对象检测任务，并且能够通过集合预测简化检测流程的假设。</p></li>
<li><p><strong>有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？</strong> 相关研究包括集合预测、编码器-解码器架构、并行解码和对象检测方法。DETR归类为直接集合预测方法。值得关注的研究员包括论文作者团队和在transformers及对象检测领域有重要贡献的研究员。</p></li>
<li><p><strong>论文中提到的解决方案之关键是什么？</strong> 解决方案的关键是使用transformer架构进行集合预测，并通过二分图匹配损失进行端到端训练。</p></li>
<li><p><strong>论文中的实验是如何设计的？</strong> 实验设计是在COCO数据集上评估DETR，并与传统的Faster RCNN方法进行比较。</p></li>
<li><p><strong>用于定量评估的数据集上什么？代码有没有开源？</strong> 使用的数据集是COCO 2017检测和全景分割数据集。代码已经在GitHub上开源。</p></li>
<li><p><strong>论文中的实验及结果有没有很好地支持需要验证的科学假设？</strong> 实验结果支持了transformers能够有效进行对象检测的假设，尤其是在大物体检测上表现更好。</p></li>
<li><p><strong>这篇论文到底有什么贡献？</strong> 论文提出了一种新的端到端对象检测框架DETR，简化了对象检测流程，并在COCO数据集上展示了竞争性能，证明了transformers可以有效地应用于对象检测任务。</p></li>
<li><p><strong>下一步呢？有什么工作可以继续深入？</strong> 下一步的工作可以包括改进DETR在小物体检测上的性能，探索DETR在其他视觉任务中的应用，以及进一步优化DETR的训练和推理效率。</p></li>
</ol>
<hr class="docutils" />
<img width="959" alt="detr-fig1" src="https://github.com/isLinXu/issues/assets/59380685/34e91c32-6193-494d-bf47-228e9adb3a48">
<p>这个图表展示了DETR（Detection Transformer）模型的结构和工作流程。以下是对图表结构的分析和总结：</p>
<p>图表结构分析</p>
<ol class="arabic simple">
<li><p><strong>输入图像</strong>：</p>
<ul class="simple">
<li><p>图表的左侧显示了一张输入图像，包含多个目标。</p></li>
</ul>
</li>
<li><p><strong>CNN特征提取</strong>：</p>
<ul class="simple">
<li><p>输入图像首先通过一个卷积神经网络（CNN），提取出一组图像特征。这些特征表示了图像中的重要信息。</p></li>
</ul>
</li>
<li><p><strong>Transformer编码器-解码器</strong>：</p>
<ul class="simple">
<li><p>提取的图像特征被输入到一个Transformer编码器-解码器结构中。Transformer结构用于处理序列数据，能够捕捉全局上下文信息。</p></li>
<li><p>编码器将图像特征编码成一组高维特征表示，解码器则将这些高维特征表示解码成一组预测框（box predictions）。</p></li>
</ul>
</li>
<li><p><strong>预测框集合</strong>：</p>
<ul class="simple">
<li><p>Transformer解码器输出一组预测框，每个框表示一个可能的目标位置和类别。</p></li>
</ul>
</li>
<li><p><strong>二分匹配损失</strong>：</p>
<ul class="simple">
<li><p>在训练过程中，使用二分匹配（bipartite matching）算法将预测框与真实框（ground truth boxes）进行唯一匹配。</p></li>
<li><p>匹配过程中，如果某个预测框没有匹配到任何真实框，则该预测框被标记为“无目标”（no object），并分配一个“无目标”类别。</p></li>
</ul>
</li>
</ol>
<p>总结</p>
<p>DETR模型通过结合CNN和Transformer架构，直接并行地预测最终的检测结果。具体流程如下：</p>
<ol class="arabic simple">
<li><p><strong>特征提取</strong>：输入图像通过CNN提取出一组图像特征。</p></li>
<li><p><strong>特征编码和解码</strong>：图像特征被输入到Transformer编码器-解码器结构中，编码器将特征编码成高维表示，解码器将高维表示解码成一组预测框。</p></li>
<li><p><strong>预测框输出</strong>：解码器输出一组预测框，每个框表示一个可能的目标位置和类别。</p></li>
<li><p><strong>二分匹配</strong>：在训练过程中，使用二分匹配算法将预测框与真实框进行唯一匹配。未匹配到真实框的预测框被标记为“无目标”类别。</p></li>
</ol>
<p>这种方法的优势在于，DETR模型能够捕捉全局上下文信息，并且通过二分匹配算法有效地处理预测框与真实框的匹配问题，从而提高目标检测的准确性和效率。</p>
<hr class="docutils" />
<img width="952" alt="detr-fig2" src="https://github.com/isLinXu/issues/assets/59380685/6d71a0c3-0f9d-4e8e-953a-3002f62c4725">
<p>这个图表展示了DETR（Detection Transformer）模型的详细结构和工作流程。以下是对图表结构的分析和总结：</p>
<p>结构分析</p>
<ol class="arabic simple">
<li><p><strong>Backbone（主干网络）</strong>：</p>
<ul class="simple">
<li><p>输入图像首先通过一个卷积神经网络（CNN）主干网络，提取出一组二维图像特征。</p></li>
<li><p>提取的图像特征被展平，并补充了位置编码（positional encoding），以保留空间信息。</p></li>
</ul>
</li>
<li><p><strong>Encoder（编码器）</strong>：</p>
<ul class="simple">
<li><p>图像特征和位置编码被输入到Transformer编码器中。编码器由多个Transformer层组成，能够捕捉全局上下文信息，并生成高维特征表示。</p></li>
</ul>
</li>
<li><p><strong>Object Queries（目标查询）</strong>：</p>
<ul class="simple">
<li><p>编码器输出的高维特征表示被传递给Transformer解码器，同时输入一组固定数量的学习到的位置嵌入（positional embeddings），称为目标查询（object queries）。</p></li>
<li><p>目标查询用于引导解码器关注特定的目标位置。</p></li>
</ul>
</li>
<li><p><strong>Decoder（解码器）</strong>：</p>
<ul class="simple">
<li><p>Transformer解码器接收编码器的输出和目标查询，生成一组解码后的特征表示。</p></li>
<li><p>解码器通过关注机制（attention mechanism）进一步处理这些特征表示，生成最终的预测结果。</p></li>
</ul>
</li>
<li><p><strong>Prediction Heads（预测头）</strong>：</p>
<ul class="simple">
<li><p>解码器的每个输出嵌入（embedding）被传递到一个共享的前馈神经网络（FFN）。</p></li>
<li><p>FFN预测每个目标的类别和边界框（class and bounding box），或者预测“无目标”（no object）类别。</p></li>
</ul>
</li>
</ol>
<p>总结</p>
<p>DETR模型通过结合CNN和Transformer架构，直接并行地预测最终的检测结果。具体流程如下：</p>
<ol class="arabic simple">
<li><p><strong>特征提取</strong>：输入图像通过CNN主干网络提取出二维图像特征，并补充位置编码。</p></li>
<li><p><strong>特征编码</strong>：图像特征和位置编码被输入到Transformer编码器中，编码器生成高维特征表示。</p></li>
<li><p><strong>目标查询</strong>：一组固定数量的目标查询被输入到Transformer解码器中，引导解码器关注特定的目标位置。</p></li>
<li><p><strong>特征解码</strong>：解码器通过关注机制处理编码器的输出和目标查询，生成解码后的特征表示。</p></li>
<li><p><strong>预测输出</strong>：解码器的每个输出嵌入被传递到共享的FFN，FFN预测每个目标的类别和边界框，或者预测“无目标”类别。</p></li>
</ol>
<p>这种方法的优势在于，DETR模型能够捕捉全局上下文信息，并通过目标查询机制有效地引导解码器关注特定目标位置，从而提高目标检测的准确性和效率。</p>
<hr class="docutils" />
<img width="964" alt="detr-fig3" src="https://github.com/isLinXu/issues/assets/59380685/a74e4aba-3fc2-4937-912f-7d8460ff36fb">
<p>这个图表展示了DETR（Detection Transformer）模型中编码器的自注意力机制（self-attention）在一组参考点上的表现。以下是对图表结构的分析和总结：</p>
<p>图表结构分析</p>
<ol class="arabic simple">
<li><p><strong>中心图像</strong>：</p>
<ul class="simple">
<li><p>中心图像是一张包含多头牛的场景图像。图像中标记了几个红色的参考点，这些点是自注意力机制关注的关键位置。</p></li>
</ul>
</li>
<li><p><strong>自注意力热图</strong>：</p>
<ul class="simple">
<li><p>中心图像的四周展示了四个自注意力热图（self-attention maps），每个热图对应一个参考点。</p></li>
<li><p>热图显示了编码器在处理输入图像时，对应参考点的自注意力分布情况。颜色越亮的区域表示自注意力权重越高，模型对这些区域的关注度越高。</p></li>
</ul>
</li>
<li><p><strong>参考点和自注意力分布</strong>：</p>
<ul class="simple">
<li><p>每个自注意力热图通过红色虚线与中心图像中的参考点相连，表示该热图是对应参考点的自注意力分布。</p></li>
<li><p>例如，左上角的热图对应中心图像中左侧牛的参考点，显示了模型在该参考点上的自注意力分布情况。</p></li>
</ul>
</li>
</ol>
<p>总结</p>
<p>DETR模型中的编码器自注意力机制能够有效地分离和识别图像中的个体实例。具体表现如下：</p>
<ol class="arabic simple">
<li><p><strong>自注意力机制</strong>：</p>
<ul class="simple">
<li><p>编码器通过自注意力机制，计算输入图像中每个位置与其他位置之间的相关性。自注意力机制能够捕捉全局上下文信息，使模型能够关注图像中的重要区域。</p></li>
</ul>
</li>
<li><p><strong>参考点的自注意力分布</strong>：</p>
<ul class="simple">
<li><p>图表展示了编码器在一组参考点上的自注意力分布情况。每个参考点的自注意力热图显示了模型对该点周围区域的关注度。</p></li>
<li><p>通过自注意力机制，编码器能够有效地分离和识别图像中的个体实例。例如，左上角的热图显示了模型对左侧牛的关注区域，而右上角的热图显示了模型对右侧牛的关注区域。</p></li>
</ul>
</li>
<li><p><strong>实例分离</strong>：</p>
<ul class="simple">
<li><p>自注意力机制使得编码器能够在复杂场景中分离出个体实例，即使这些实例在空间上相互接近或重叠。</p></li>
<li><p>这种能力对于目标检测任务非常重要，因为它能够提高模型在复杂场景中的检测准确性。</p></li>
</ul>
</li>
</ol>
<p>通过自注意力机制，DETR模型的编码器能够有效地分离和识别图像中的个体实例，捕捉全局上下文信息，从而提高目标检测的准确性和效率。</p>
<hr class="docutils" />
<img width="558" alt="detr-fig4" src="https://github.com/isLinXu/issues/assets/59380685/31e389ba-ff4b-4ecb-bce6-9a30701c5c48">
<p>这个图表展示了DETR（Detection Transformer）模型在不同解码器层数下的AP（平均精度）和AP50（IoU阈值为0.5时的平均精度）性能。图表还比较了使用和不使用NMS（非极大值抑制）时的性能表现。以下是对图表结构的分析和总结：</p>
<p>总结</p>
<p>图表展示了DETR模型在不同解码器层数下的AP和AP50性能，并比较了使用和不使用NMS时的表现。具体总结如下：</p>
<ol class="arabic simple">
<li><p><strong>解码器层数的影响</strong>：</p>
<ul class="simple">
<li><p>随着解码器层数的增加，DETR模型的AP和AP50性能逐渐提高。这表明更多的解码器层数能够捕捉到更丰富的特征信息，从而提高检测精度。</p></li>
</ul>
</li>
<li><p><strong>NMS的影响</strong>：</p>
<ul class="simple">
<li><p>不使用NMS时，AP和AP50的性能在解码器层数增加时表现出稳定的提升。</p></li>
<li><p>使用NMS时，AP在初始解码器层数时有所提升，但在后续层数时略有下降。这可能是因为NMS在去除重复预测时，也可能误删了一些正确的预测。</p></li>
<li><p>使用NMS时，AP50在所有解码器层数下均有所提升，但提升幅度较小。这表明NMS在一定程度上能够提高检测精度，但其效果有限。</p></li>
</ul>
</li>
<li><p><strong>DETR模型的设计优势</strong>：</p>
<ul class="simple">
<li><p>图表验证了DETR模型不需要NMS即可获得较高的检测精度。这是因为DETR模型通过自注意力机制和目标查询机制，能够有效地分离和识别图像中的个体实例，从而减少了重复预测的情况。</p></li>
</ul>
</li>
</ol>
<p>总体而言，DETR模型在不同解码器层数下表现出良好的检测性能，并且不依赖于NMS即可获得较高的AP和AP50。这表明DETR模型在目标检测任务中具有较大的优势。</p>
<hr class="docutils" />
<img width="370" alt="detr-fig5" src="https://github.com/isLinXu/issues/assets/59380685/7023c358-34da-4a1b-9501-f17197b08713">
<p>这个图表展示了DETR（Detection Transformer）模型在处理稀有类别的分布外（out of distribution）泛化能力。具体来说，图表显示了DETR模型在一张包含大量长颈鹿的图像上的检测结果。以下是对图表结构的分析和总结：</p>
<p>图表结构分析</p>
<ol class="arabic simple">
<li><p><strong>图像内容</strong>：</p>
<ul class="simple">
<li><p>图像中包含多个长颈鹿，每个长颈鹿都被一个边界框标记出来。</p></li>
<li><p>每个边界框上方都有一个标签，标明了检测到的对象类别（长颈鹿）和置信度分数。</p></li>
</ul>
</li>
<li><p><strong>边界框和标签</strong>：</p>
<ul class="simple">
<li><p>图像中的每个长颈鹿都被成功检测并标记，边界框颜色各异，以区分不同的实例。</p></li>
<li><p>标签显示了每个检测到的长颈鹿的置信度分数，分数越高表示模型对该检测结果的信心越高。</p></li>
</ul>
</li>
<li><p><strong>泛化能力</strong>：</p>
<ul class="simple">
<li><p>图像中包含24个长颈鹿，远超过训练集中每张图像最多包含的13个长颈鹿。</p></li>
<li><p>这表明DETR模型具有很强的泛化能力，能够在训练集中未见过的情况下，成功检测出更多数量的同类对象。</p></li>
</ul>
</li>
</ol>
<p>总结</p>
<p>图表展示了DETR模型在处理稀有类别的分布外泛化能力，具体表现如下：</p>
<ol class="arabic simple">
<li><p><strong>稀有类别的检测</strong>：</p>
<ul class="simple">
<li><p>即使训练集中每张图像最多只包含13个长颈鹿，DETR模型在测试图像中成功检测出了24个长颈鹿。</p></li>
<li><p>这表明DETR模型能够有效地处理稀有类别，并在训练数据不足的情况下，仍能保持较高的检测精度。</p></li>
</ul>
</li>
<li><p><strong>泛化能力</strong>：</p>
<ul class="simple">
<li><p>DETR模型展示了强大的泛化能力，能够在训练集中未见过的情况下，成功检测出更多数量的同类对象。</p></li>
<li><p>这种泛化能力对于实际应用非常重要，因为在现实世界中，目标检测模型需要处理各种不同的场景和对象数量。</p></li>
</ul>
</li>
<li><p><strong>模型鲁棒性</strong>：</p>
<ul class="simple">
<li><p>图表中的检测结果显示，DETR模型对每个长颈鹿的检测置信度都较高，边界框准确。</p></li>
<li><p>这表明DETR模型在处理复杂场景和大量对象时，仍能保持较高的鲁棒性和准确性。</p></li>
</ul>
</li>
</ol>
<p>总体而言，DETR模型在处理稀有类别和分布外泛化方面表现出色，能够在训练数据不足的情况下，成功检测出更多数量的同类对象。这表明DETR模型具有很强的泛化能力和鲁棒性，适用于各种复杂的目标检测任务。</p>
<hr class="docutils" />
<img width="964" alt="detr-fig6" src="https://github.com/isLinXu/issues/assets/59380685/c6808326-3e93-4297-9542-7f25b3ef3d94">
<p>这个图表展示了DETR-DC5模型在COCO验证集图像上的预测结果，并可视化了解码器对每个预测对象的注意力分布。以下是对图表结构的分析和总结：</p>
<p>图表结构分析</p>
<ol class="arabic simple">
<li><p><strong>图像内容</strong>：</p>
<ul class="simple">
<li><p>图表包含两张图像，左侧图像中有两头大象，右侧图像中有两只斑马。</p></li>
<li><p>每个对象都被一个边界框标记出来，并附有标签和置信度分数。</p></li>
</ul>
</li>
<li><p><strong>边界框和标签</strong>：</p>
<ul class="simple">
<li><p>左侧图像中，大象被标记为“elephant 100%”，表示模型对该检测结果的置信度为100%。</p></li>
<li><p>右侧图像中，斑马被标记为“zebra 100%”，表示模型对该检测结果的置信度为100%。</p></li>
</ul>
</li>
<li><p><strong>注意力分布</strong>：</p>
<ul class="simple">
<li><p>图像中使用不同颜色的热图来表示解码器对每个预测对象的注意力分布。</p></li>
<li><p>热图显示了解码器在预测对象时关注的区域，颜色越亮表示注意力权重越高。</p></li>
<li><p>解码器通常关注对象的边缘部分，如腿和头部。</p></li>
</ul>
</li>
</ol>
<p>总结</p>
<p>图表展示了DETR-DC5模型在COCO验证集图像上的预测结果，并可视化了解码器对每个预测对象的注意力分布。具体总结如下：</p>
<ol class="arabic simple">
<li><p><strong>高置信度检测</strong>：</p>
<ul class="simple">
<li><p>模型对每个检测对象的置信度都很高（100%），表明DETR-DC5模型在这些图像上的检测结果非常准确。</p></li>
<li><p>边界框准确地标记了每个对象，显示了模型的高检测精度。</p></li>
</ul>
</li>
<li><p><strong>注意力分布</strong>：</p>
<ul class="simple">
<li><p>解码器的注意力分布显示，模型在预测对象时通常关注对象的边缘部分，如腿和头部。</p></li>
<li><p>这种注意力分布有助于模型更准确地识别和定位对象，因为边缘部分通常包含更多的特征信息。</p></li>
</ul>
</li>
<li><p><strong>模型的鲁棒性</strong>：</p>
<ul class="simple">
<li><p>图表中的检测结果和注意力分布显示，DETR-DC5模型在处理复杂场景和多对象时，仍能保持较高的鲁棒性和准确性。</p></li>
<li><p>模型能够有效地分离和识别图像中的多个对象，并准确地标记它们的位置和类别。</p></li>
</ul>
</li>
<li><p><strong>可视化的价值</strong>：</p>
<ul class="simple">
<li><p>通过可视化解码器的注意力分布，研究人员可以更好地理解模型的工作机制和决策过程。</p></li>
<li><p>这种可视化方法有助于发现模型在处理不同对象时的关注点，从而进一步优化和改进模型。</p></li>
</ul>
</li>
</ol>
<p>总体而言，图表展示了DETR-DC5模型在目标检测任务中的高精度和鲁棒性，并通过可视化解码器的注意力分布，提供了对模型工作机制的深入理解。这表明DETR-DC5模型在处理复杂场景和多对象检测时具有很强的能力。</p>
<hr class="docutils" />
<img width="966" alt="detr-fig7" src="https://github.com/isLinXu/issues/assets/59380685/a2346de9-ea59-4c77-af87-8ab764bec067">
<p>这个图表展示了DETR解码器在COCO 2017验证集所有图像上的边界框预测结果的可视化。具体来说，图表展示了100个预测槽位中的20个，每个预测槽位的边界框预测结果被表示为一个点，点的颜色编码表示不同大小和形状的边界框。以下是对图表结构的分析和总结：</p>
<p>图表结构分析</p>
<ol class="arabic simple">
<li><p><strong>图像内容</strong>：</p>
<ul class="simple">
<li><p>图表包含20个子图，每个子图代表一个预测槽位的边界框预测结果。</p></li>
<li><p>每个子图中的点表示边界框的中心坐标，坐标被归一化到1x1的方格中。</p></li>
</ul>
</li>
<li><p><strong>颜色编码</strong>：</p>
<ul class="simple">
<li><p>绿色点表示小边界框。</p></li>
<li><p>红色点表示大的水平边界框。</p></li>
<li><p>蓝色点表示大的垂直边界框。</p></li>
</ul>
</li>
<li><p><strong>预测槽位的专门化</strong>：</p>
<ul class="simple">
<li><p>每个预测槽位在特定区域和边界框大小上表现出专门化。</p></li>
<li><p>子图中点的分布显示了每个预测槽位在不同区域和边界框大小上的操作模式。</p></li>
</ul>
</li>
</ol>
<p>总结</p>
<p>图表展示了DETR解码器在COCO 2017验证集所有图像上的边界框预测结果的可视化，具体总结如下：</p>
<ol class="arabic simple">
<li><p><strong>预测槽位的专门化</strong>：</p>
<ul class="simple">
<li><p>每个预测槽位在特定区域和边界框大小上表现出专门化。</p></li>
<li><p>这种专门化使得每个槽位能够更有效地处理特定类型的对象和场景，从而提高整体检测性能。</p></li>
</ul>
</li>
<li><p><strong>边界框大小和形状的分布</strong>：</p>
<ul class="simple">
<li><p>绿色点表示小边界框，红色点表示大的水平边界框，蓝色点表示大的垂直边界框。</p></li>
<li><p>这种颜色编码的分布显示了不同预测槽位在处理不同大小和形状的边界框时的偏好。</p></li>
</ul>
</li>
<li><p><strong>常见模式</strong>：</p>
<ul class="simple">
<li><p>几乎所有的预测槽位都有一个预测大图像宽边界框的模式，这在COCO数据集中是常见的。</p></li>
<li><p>这种模式表明DETR模型能够适应数据集中常见的对象大小和形状，从而提高检测精度。</p></li>
</ul>
</li>
<li><p><strong>归一化坐标</strong>：</p>
<ul class="simple">
<li><p>每个边界框的中心坐标被归一化到1x1的方格中，这使得不同图像大小的预测结果可以在同一个尺度上进行比较。</p></li>
<li><p>这种归一化处理有助于更直观地理解预测槽位的专门化和操作模式。</p></li>
</ul>
</li>
</ol>
<p>总体而言，图表展示了DETR解码器在边界框预测上的专门化和操作模式，通过颜色编码和归一化坐标，提供了对模型在处理不同大小和形状的边界框时的偏好的深入理解。这表明DETR模型在目标检测任务中具有很强的适应性和精度。</p>
<hr class="docutils" />
<img width="981" alt="detr-fig8" src="https://github.com/isLinXu/issues/assets/59380685/f73973c2-01b8-42e3-a979-51b5959dbe98">
<p>这个图表展示了全景分割头（panoptic head）的工作流程，具体描述了如何生成每个检测对象的二进制掩码，并通过像素级的argmax操作将这些掩码合并。以下是对图表结构的分析和总结：</p>
<p>图表结构分析</p>
<ol class="arabic simple">
<li><p><strong>输入图像</strong>：</p>
<ul class="simple">
<li><p>输入图像的尺寸为 (3 \times H \times W)，表示RGB图像。</p></li>
</ul>
</li>
<li><p><strong>多头注意力（Multi-head attention）</strong>：</p>
<ul class="simple">
<li><p>输入图像经过编码，生成尺寸为 ((N \times H/32 \times W/32)) 的编码图像。</p></li>
<li><p>多头注意力机制用于生成注意力图（attention maps），尺寸为 ((N \times H/32 \times W/32))。</p></li>
</ul>
</li>
<li><p><strong>ResNet特征提取</strong>：</p>
<ul class="simple">
<li><p>编码图像和注意力图经过FPN（Feature Pyramid Network）风格的卷积神经网络（CNN），生成不同层次的ResNet特征。</p></li>
<li><p>ResNet特征包括Res3、Res4和Res5层的特征图。</p></li>
</ul>
</li>
<li><p><strong>掩码生成</strong>：</p>
<ul class="simple">
<li><p>ResNet特征图经过进一步处理，生成掩码logits，尺寸为 ((N \times H/4 \times W/4))。</p></li>
<li><p>每个检测对象的二进制掩码在并行处理中生成。</p></li>
</ul>
</li>
<li><p><strong>像素级argmax</strong>：</p>
<ul class="simple">
<li><p>所有生成的掩码通过像素级的argmax操作合并，生成最终的全景分割结果。</p></li>
</ul>
</li>
<li><p><strong>输出图像</strong>：</p>
<ul class="simple">
<li><p>最终的全景分割结果展示了每个对象和背景区域的分割掩码。</p></li>
</ul>
</li>
</ol>
<p>总结</p>
<p>图表展示了全景分割头的工作流程，具体总结如下：</p>
<ol class="arabic simple">
<li><p><strong>多头注意力机制</strong>：</p>
<ul class="simple">
<li><p>多头注意力机制用于生成注意力图，帮助模型更好地关注图像中的重要区域。</p></li>
<li><p>注意力图与编码图像一起输入到FPN风格的CNN中，提取多层次的特征。</p></li>
</ul>
</li>
<li><p><strong>ResNet特征提取</strong>：</p>
<ul class="simple">
<li><p>编码图像和注意力图经过FPN风格的CNN，生成不同层次的ResNet特征图。</p></li>
<li><p>这些特征图用于进一步生成掩码logits。</p></li>
</ul>
</li>
<li><p><strong>掩码生成和合并</strong>：</p>
<ul class="simple">
<li><p>每个检测对象的二进制掩码在并行处理中生成，掩码logits的尺寸为 ((N \times H/4 \times W/4))。</p></li>
<li><p>所有生成的掩码通过像素级的argmax操作合并，生成最终的全景分割结果。</p></li>
</ul>
</li>
<li><p><strong>高效的全景分割</strong>：</p>
<ul class="simple">
<li><p>通过多头注意力机制和FPN风格的CNN，模型能够高效地提取特征并生成高质量的掩码。</p></li>
<li><p>像素级的argmax操作确保了最终分割结果的准确性和一致性。</p></li>
</ul>
</li>
</ol>
<p>总体而言，图表展示了全景分割头的详细工作流程，通过多头注意力机制、ResNet特征提取和像素级argmax操作，模型能够高效地生成高质量的全景分割结果。这表明全景分割头在处理复杂场景和多对象分割任务中具有很强的适应性和精度。</p>
<hr class="docutils" />
<p>fig9</p>
<p>这个图表展示了由DETR-R101模型生成的全景分割（panoptic segmentation）结果。全景分割任务包括对图像中的“物体”（things）和“背景”（stuff）进行统一的掩码预测。以下是对图表结构的分析和总结：</p>
<p>图表结构分析</p>
<ol class="arabic simple">
<li><p><strong>图像内容</strong>：</p>
<ul class="simple">
<li><p>图表包含三张图像，每张图像展示了不同场景的全景分割结果。</p></li>
<li><p>每个图像中的对象和背景都被不同颜色的掩码覆盖，并附有标签。</p></li>
</ul>
</li>
<li><p><strong>掩码和标签</strong>：</p>
<ul class="simple">
<li><p>每个对象和背景区域都被一个颜色掩码覆盖，掩码颜色各异，以区分不同的实例和背景。</p></li>
<li><p>标签标明了每个掩码区域的类别，例如“counter”、“bus”、“giraffe”等。</p></li>
</ul>
</li>
<li><p><strong>统一的掩码预测</strong>：</p>
<ul class="simple">
<li><p>DETR-R101模型在同一图像中对“物体”和“背景”进行了统一的掩码预测。</p></li>
<li><p>这种统一的处理方式使得模型能够同时处理图像中的所有元素，而不需要区分“物体”和“背景”。</p></li>
</ul>
</li>
</ol>
<p>总结</p>
<p>图表展示了由DETR-R101模型生成的全景分割结果，具体总结如下：</p>
<ol class="arabic simple">
<li><p><strong>高质量的全景分割</strong>：</p>
<ul class="simple">
<li><p>图像中的每个对象和背景区域都被准确地分割并标记，显示了DETR-R101模型在全景分割任务中的高精度。</p></li>
<li><p>掩码覆盖了对象和背景的边界，显示了模型在处理复杂场景时的鲁棒性。</p></li>
</ul>
</li>
<li><p><strong>统一的掩码预测</strong>：</p>
<ul class="simple">
<li><p>DETR-R101模型能够在同一图像中对“物体”和“背景”进行统一的掩码预测。</p></li>
<li><p>这种统一的处理方式简化了全景分割任务，使得模型能够更高效地处理图像中的所有元素。</p></li>
</ul>
</li>
<li><p><strong>多样化的场景处理</strong>：</p>
<ul class="simple">
<li><p>图表展示了不同场景的全景分割结果，包括室内场景（如厨房）、户外场景（如街道）和自然场景（如动物园）。</p></li>
<li><p>这表明DETR-R101模型在处理各种不同类型的场景时都能保持较高的分割精度。</p></li>
</ul>
</li>
<li><p><strong>标签和掩码的对齐</strong>：</p>
<ul class="simple">
<li><p>每个掩码区域都附有标签，标签与掩码区域准确对齐，显示了模型在对象识别和分割上的一致性。</p></li>
<li><p>这种对齐方式有助于更直观地理解图像中的各个元素及其类别。</p></li>
</ul>
</li>
</ol>
<p>总体而言，图表展示了DETR-R101模型在全景分割任务中的高精度和鲁棒性，通过统一的掩码预测方式，模型能够高效地处理图像中的所有元素，并在各种不同类型的场景中保持一致的分割性能。这表明DETR-R101模型在全景分割任务中具有很强的适应性和实用性。</p>
<hr class="docutils" />
<p>fig10</p>
<p>这个图表展示了DETR（Detection Transformer）模型的Transformer架构，包括编码器（Encoder）和解码器（Decoder）的详细结构。以下是对图表结构的分析和总结：</p>
<p>图表结构分析</p>
<ol class="arabic simple">
<li><p><strong>编码器（Encoder）</strong>：</p>
<ul class="simple">
<li><p><strong>输入</strong>：图像特征（Image features）和空间位置编码（Spatial positional encoding）。</p></li>
<li><p><strong>层次结构</strong>：</p>
<ul>
<li><p>多头自注意力（Multi-Head Self-Attention）：处理输入特征，捕捉全局信息。</p></li>
<li><p>加法和归一化（Add &amp; Norm）：对多头自注意力的输出进行归一化。</p></li>
<li><p>前馈神经网络（FFN）：进一步处理归一化后的特征。</p></li>
<li><p>加法和归一化（Add &amp; Norm）：对FFN的输出进行归一化。</p></li>
</ul>
</li>
<li><p><strong>输出</strong>：编码后的特征。</p></li>
</ul>
</li>
<li><p><strong>解码器（Decoder）</strong>：</p>
<ul class="simple">
<li><p><strong>输入</strong>：编码器的输出特征和对象查询（Object queries）。</p></li>
<li><p><strong>层次结构</strong>：</p>
<ul>
<li><p>多头自注意力（Multi-Head Self-Attention）：处理对象查询，捕捉全局信息。</p></li>
<li><p>加法和归一化（Add &amp; Norm）：对多头自注意力的输出进行归一化。</p></li>
<li><p>多头注意力（Multi-Head Attention）：结合编码器的输出特征和对象查询，捕捉对象与图像特征之间的关系。</p></li>
<li><p>加法和归一化（Add &amp; Norm）：对多头注意力的输出进行归一化。</p></li>
<li><p>前馈神经网络（FFN）：进一步处理归一化后的特征。</p></li>
<li><p>加法和归一化（Add &amp; Norm）：对FFN的输出进行归一化。</p></li>
</ul>
</li>
<li><p><strong>输出</strong>：解码后的特征。</p></li>
</ul>
</li>
<li><p><strong>输出层</strong>：</p>
<ul class="simple">
<li><p><strong>类别预测（Class）</strong>：通过前馈神经网络（FFN）对解码后的特征进行分类，预测对象类别。</p></li>
<li><p><strong>边界框预测（Bounding Box）</strong>：通过前馈神经网络（FFN）对解码后的特征进行回归，预测对象的边界框。</p></li>
</ul>
</li>
</ol>
<p>总结</p>
<p>图表展示了DETR模型的Transformer架构，具体总结如下：</p>
<ol class="arabic simple">
<li><p><strong>编码器-解码器架构</strong>：</p>
<ul class="simple">
<li><p>DETR模型采用了经典的Transformer编码器-解码器架构。</p></li>
<li><p>编码器负责处理输入图像特征，捕捉全局信息。</p></li>
<li><p>解码器结合编码器的输出特征和对象查询，捕捉对象与图像特征之间的关系。</p></li>
</ul>
</li>
<li><p><strong>多头自注意力和多头注意力机制</strong>：</p>
<ul class="simple">
<li><p>编码器和解码器都使用了多头自注意力机制，帮助模型捕捉全局信息和特征之间的关系。</p></li>
<li><p>解码器还使用了多头注意力机制，结合编码器的输出特征和对象查询，进一步增强对象检测的准确性。</p></li>
</ul>
</li>
<li><p><strong>加法和归一化（Add &amp; Norm）</strong>：</p>
<ul class="simple">
<li><p>每个多头自注意力和多头注意力层后都进行了加法和归一化处理，确保特征的稳定性和一致性。</p></li>
<li><p>前馈神经网络（FFN）层后也进行了加法和归一化处理，进一步增强特征的表达能力。</p></li>
</ul>
</li>
<li><p><strong>对象查询（Object queries）</strong>：</p>
<ul class="simple">
<li><p>解码器使用对象查询来捕捉特定对象的信息。</p></li>
<li><p>对象查询与编码器的输出特征结合，通过多头注意力机制捕捉对象与图像特征之间的关系。</p></li>
</ul>
</li>
<li><p><strong>输出层</strong>：</p>
<ul class="simple">
<li><p>解码器的输出特征通过前馈神经网络（FFN）进行分类和回归，分别预测对象的类别和边界框。</p></li>
</ul>
</li>
</ol>
<p>总体而言，图表展示了DETR模型的Transformer架构，通过编码器-解码器结构、多头自注意力和多头注意力机制、加法和归一化处理以及对象查询，模型能够高效地进行对象检测和边界框预测。这表明DETR模型在目标检测任务中具有很强的适应性和精度。</p>
<hr class="docutils" />
<p>fig11</p>
<p>这个图表展示了全景分割模型PanopticFPN和DETR在处理重叠对象时的表现对比。图表分为两部分：(a)展示了重叠对象的失败案例，(b)展示了“物体”掩码在全分辨率下的预测结果。以下是对图表结构的分析和总结：</p>
<p>图表结构分析</p>
<ol class="arabic simple">
<li><p><strong>(a) 重叠对象的失败案例</strong>：</p>
<ul class="simple">
<li><p><strong>左图</strong>：真实标签（Ground truth），展示了图像中所有飞机的正确分割。</p></li>
<li><p><strong>中图</strong>：PanopticFPN的预测结果，显示了模型在处理重叠对象时的失败情况，其中一个飞机完全被遗漏。</p></li>
<li><p><strong>右图</strong>：DETR的预测结果，显示了模型在处理重叠对象时的失败情况，其中三个飞机未能准确分割。</p></li>
</ul>
</li>
<li><p><strong>(b) 全分辨率下的“物体”掩码预测</strong>：</p>
<ul class="simple">
<li><p><strong>左图</strong>：真实标签（Ground truth），展示了图像中所有飞机的正确分割。</p></li>
<li><p><strong>中图</strong>：PanopticFPN的预测结果，显示了模型在全分辨率下的分割效果，边界较模糊。</p></li>
<li><p><strong>右图</strong>：DETR的预测结果，显示了模型在全分辨率下的分割效果，边界较清晰。</p></li>
</ul>
</li>
</ol>
<p>总结</p>
<p>图表展示了PanopticFPN和DETR在全景分割任务中的表现对比，具体总结如下：</p>
<ol class="arabic simple">
<li><p><strong>重叠对象的处理</strong>：</p>
<ul class="simple">
<li><p>在处理重叠对象时，PanopticFPN和DETR都存在一定的失败情况。</p></li>
<li><p>PanopticFPN在重叠对象的分割中遗漏了一个飞机，显示了模型在处理复杂场景时的局限性。</p></li>
<li><p>DETR在重叠对象的分割中未能准确分割三个飞机，显示了模型在处理重叠对象时的挑战。</p></li>
</ul>
</li>
<li><p><strong>全分辨率下的“物体”掩码预测</strong>：</p>
<ul class="simple">
<li><p>在全分辨率下，DETR的分割效果优于PanopticFPN，边界更为清晰。</p></li>
<li><p>PanopticFPN的分割结果边界较模糊，显示了模型在高分辨率下的分割精度不足。</p></li>
<li><p>DETR的分割结果边界较清晰，显示了模型在高分辨率下的分割精度较高。</p></li>
</ul>
</li>
<li><p><strong>模型对比</strong>：</p>
<ul class="simple">
<li><p>PanopticFPN和DETR在全景分割任务中各有优劣。</p></li>
<li><p>PanopticFPN在处理重叠对象时存在遗漏的情况，但在全分辨率下的分割效果较为稳定。</p></li>
<li><p>DETR在处理重叠对象时存在分割不准确的情况，但在全分辨率下的分割效果较为清晰。</p></li>
</ul>
</li>
</ol>
<p>总体而言，图表展示了PanopticFPN和DETR在全景分割任务中的表现对比，揭示了两种模型在处理重叠对象和全分辨率分割时的优劣。PanopticFPN在处理重叠对象时存在遗漏，但在全分辨率下的分割效果较为稳定；DETR在处理重叠对象时存在分割不准确的情况，但在全分辨率下的分割效果较为清晰。这表明在选择全景分割模型时，需要根据具体应用场景和需求进行权衡。</p>
<hr class="docutils" />
<p>fig12</p>
<p>这个图表展示了DETR（Detection Transformer）模型在不同可见实例数量下，漏检（missed instances）各种类别（狗、人物、苹果）实例的百分比。图表分析了随着图像中可见实例数量的增加，DETR模型漏检实例的情况。以下是对图表结构的分析和总结：</p>
<p>图表结构分析</p>
<ol class="arabic simple">
<li><p><strong>横轴（X轴）</strong>：</p>
<ul class="simple">
<li><p>表示图像中可见实例的数量，从0到100。</p></li>
</ul>
</li>
<li><p><strong>纵轴（Y轴）</strong>：</p>
<ul class="simple">
<li><p>表示漏检实例的百分比（% of missed instances），从0%到70%。</p></li>
</ul>
</li>
<li><p><strong>曲线</strong>：</p>
<ul class="simple">
<li><p>三条曲线分别表示不同类别（狗、人物、苹果）的漏检情况。</p></li>
<li><p>曲线的颜色分别为蓝色（狗）、橙色（人物）和绿色（苹果）。</p></li>
<li><p>曲线的阴影部分表示标准差（standard deviation），反映了数据的波动范围。</p></li>
</ul>
</li>
</ol>
<p>总结</p>
<p>图表展示了DETR模型在不同可见实例数量下漏检各种类别实例的情况，具体总结如下：</p>
<ol class="arabic simple">
<li><p><strong>漏检率随可见实例数量增加而增加</strong>：</p>
<ul class="simple">
<li><p>随着图像中可见实例数量的增加，DETR模型漏检实例的百分比也逐渐增加。</p></li>
<li><p>当可见实例数量接近100时，漏检率显著上升，表明模型在处理大量实例时开始饱和，漏检更多的对象。</p></li>
</ul>
</li>
<li><p><strong>不同类别的漏检情况</strong>：</p>
<ul class="simple">
<li><p><strong>狗（dog）</strong>：在可见实例数量较少时，漏检率较低，但随着实例数量增加，漏检率显著上升，尤其在实例数量接近100时，漏检率达到最高。</p></li>
<li><p><strong>人物（person）</strong>：漏检率随实例数量增加而上升的趋势与狗类似，但在实例数量较少时，漏检率稍高于狗。</p></li>
<li><p><strong>苹果（apple）</strong>：漏检率随实例数量增加而上升的趋势与狗和人物类似，但在实例数量较少时，漏检率最低。</p></li>
</ul>
</li>
<li><p><strong>标准差的变化</strong>：</p>
<ul class="simple">
<li><p>曲线的阴影部分表示标准差，反映了漏检率的波动范围。</p></li>
<li><p>随着实例数量增加，标准差也逐渐增大，表明漏检率的波动范围变大，模型在处理大量实例时的稳定性下降。</p></li>
</ul>
</li>
</ol>
<p>结论</p>
<p>图表揭示了DETR模型在处理不同数量可见实例时的漏检情况，表明模型在实例数量较少时表现较好，但随着实例数量增加，漏检率显著上升，尤其在实例数量接近100时，模型开始饱和，漏检更多的对象。不同类别的漏检情况有所不同，狗和人物的漏检率在实例数量较少时较高，而苹果的漏检率在实例数量较少时最低。标准差的变化表明模型在处理大量实例时的稳定性下降。</p>
<p>总体而言，图表展示了DETR模型在处理大量实例时的局限性，提示在实际应用中需要考虑模型的处理能力和稳定性，尤其在处理包含大量实例的图像时。</p>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="YOLO-MS.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">YOLO-MS</p>
      </div>
    </a>
    <a class="right-next"
       href="RT-DETR.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">RT-DETR</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">回答问题</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By isLinXu
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022, isLinXu.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>