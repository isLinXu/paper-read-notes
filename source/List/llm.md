## 大语言模型(Large Language Models)

| Num  | Title                                                        | Field   | Desc                              | Author                                                       | Time | read |
| ---- | ------------------------------------------------------------ | ------- | --------------------------------- | ------------------------------------------------------------ | ---- | ---- |
|      | OPT: OPT : Open Pre-trained Transformer Language Models      |         | 开放预训练的 Transformer 语言模型 | [Susan Zhang](https://paperswithcode.com/author/susan-zhang), [Stephen Roller](https://paperswithcode.com/author/stephen-roller), [Naman Goyal](https://paperswithcode.com/author/naman-goyal), [Mikel Artetxe](https://paperswithcode.com/author/mikel-artetxe), [Moya Chen](https://paperswithcode.com/author/moya-chen), [Shuohui Chen](https://paperswithcode.com/author/shuohui-chen), [Christopher Dewan](https://paperswithcode.com/author/christopher-dewan), [Mona Diab](https://paperswithcode.com/author/mona-diab), [Xian Li](https://paperswithcode.com/author/xian-li), [Xi Victoria Lin](https://paperswithcode.com/author/xi-victoria-lin), [Todor Mihaylov](https://paperswithcode.com/author/todor-mihaylov), [Myle Ott](https://paperswithcode.com/author/myle-ott), [Sam Shleifer](https://paperswithcode.com/author/sam-shleifer), [Kurt Shuster](https://paperswithcode.com/author/kurt-shuster), [Daniel Simig](https://paperswithcode.com/author/daniel-simig), [Punit Singh Koura](https://paperswithcode.com/author/punit-singh-koura), [Anjali Sridhar](https://paperswithcode.com/author/anjali-sridhar), [Tianlu Wang](https://paperswithcode.com/author/tianlu-wang), [Luke Zettlemoyer](https://paperswithcode.com/author/luke-zettlemoyer) |      |      |
|      | GPT-v1:Improving Language Understanding by Generative Pre-Training | GPT&LLM |                                   |                                                              |      |      |
|      | GPT-v2:Language Models are Unsupervised Multitask Learners   | GPT&LLM |                                   |                                                              |      |      |
|      | GPT-v3:Language Models are Few-Shot Learners                 | GPT&LLM |                                   |                                                              |      |      |
|      | GPT-v4:GPT-4 Technical Report                                | GPT&LLM |                                   |                                                              |      |      |
|      |                                                              |         |                                   |                                                              |      |      |
|      |                                                              |         |                                   |                                                              |      |      |



| Date    | keywords             | Institute          | Paper                                                        | Publication |
| ------- | -------------------- | ------------------ | ------------------------------------------------------------ | ----------- |
| 2017-06 | Transformers         | Google             | [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf) | NeurIPS     |
| 2018-06 | GPT 1.0              | OpenAI             | [Improving Language Understanding by Generative Pre-Training](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf) |             |
| 2018-10 | BERT                 | Google             | [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://aclanthology.org/N19-1423.pdf) | NAACL       |
| 2019-02 | GPT 2.0              | OpenAI             | [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) |             |
| 2019-09 | Megatron-LM          | NVIDIA             | [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/pdf/1909.08053.pdf) |             |
| 2019-10 | T5                   | Google             | [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://jmlr.org/papers/v21/20-074.html) | JMLR        |
| 2019-10 | ZeRO                 | Microsoft          | [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/pdf/1910.02054.pdf) | SC          |
| 2020-01 | Scaling Law          | OpenAI             | [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361.pdf) |             |
| 2020-05 | GPT 3.0              | OpenAI             | [Language models are few-shot learners](https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf) | NeurIPS     |
| 2021-01 | Switch Transformers  | Google             | [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/pdf/2101.03961.pdf) | JMLR        |
| 2021-08 | Codex                | OpenAI             | [Evaluating Large Language Models Trained on Code](https://arxiv.org/pdf/2107.03374.pdf) |             |
| 2021-08 | Foundation Models    | Stanford           | [On the Opportunities and Risks of Foundation Models](https://arxiv.org/pdf/2108.07258.pdf) |             |
| 2021-09 | FLAN                 | Google             | [Finetuned Language Models are Zero-Shot Learners](https://openreview.net/forum?id=gEZrGCozdqR) | ICLR        |
| 2021-10 | T0                   | HuggingFace et al. | [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/abs/2110.08207) | ICLR        |
| 2021-12 | GLaM                 | Google             | [GLaM: Efficient Scaling of Language Models with Mixture-of-Experts](https://arxiv.org/pdf/2112.06905.pdf) | ICML        |
| 2021-12 | WebGPT               | OpenAI             | [WebGPT: Improving the Factual Accuracy of Language Models through Web Browsing](https://openai.com/blog/webgpt/) |             |
| 2021-12 | Retro                | DeepMind           | [Improving language models by retrieving from trillions of tokens](https://www.deepmind.com/publications/improving-language-models-by-retrieving-from-trillions-of-tokens) | ICML        |
| 2021-12 | Gopher               | DeepMind           | [Scaling Language Models: Methods, Analysis & Insights from Training Gopher](https://arxiv.org/pdf/2112.11446.pdf) |             |
| 2022-01 | COT                  | Google             | [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903.pdf) | NeurIPS     |
| 2022-01 | LaMDA                | Google             | [LaMDA: Language Models for Dialog Applications](https://arxiv.org/pdf/2201.08239.pdf) |             |
| 2022-01 | Minerva              | Google             | [Solving Quantitative Reasoning Problems with Language Models](https://arxiv.org/abs/2206.14858) | NeurIPS     |
| 2022-01 | Megatron-Turing NLG  | Microsoft&NVIDIA   | [Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model](https://arxiv.org/pdf/2201.11990.pdf) |             |
| 2022-03 | InstructGPT          | OpenAI             | [Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf) |             |
| 2022-04 | PaLM                 | Google             | [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/pdf/2204.02311.pdf) |             |
| 2022-04 | Chinchilla           | DeepMind           | [An empirical analysis of compute-optimal large language model training](https://www.deepmind.com/publications/an-empirical-analysis-of-compute-optimal-large-language-model-training) | NeurIPS     |
| 2022-05 | OPT                  | Meta               | [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/pdf/2205.01068.pdf) |             |
| 2022-06 | Emergent Abilities   | Google             | [Emergent Abilities of Large Language Models](https://openreview.net/pdf?id=yzkSU5zdwD) | TMLR        |
| 2022-06 | BIG-bench            | Google             | [Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models](https://github.com/google/BIG-bench) |             |
| 2022-06 | METALM               | Microsoft          | [Language Models are General-Purpose Interfaces](https://arxiv.org/pdf/2206.06336.pdf) |             |
| 2022-09 | Sparrow              | DeepMind           | [Improving alignment of dialogue agents via targeted human judgements](https://arxiv.org/pdf/2209.14375.pdf) |             |
| 2022-10 | Flan-T5/PaLM         | Google             | [Scaling Instruction-Finetuned Language Models](https://arxiv.org/pdf/2210.11416.pdf) |             |
| 2022-10 | GLM-130B             | Tsinghua           | [GLM-130B: An Open Bilingual Pre-trained Model](https://arxiv.org/pdf/2210.02414.pdf) | ICLR        |
| 2022-11 | HELM                 | Stanford           | [Holistic Evaluation of Language Models](https://arxiv.org/pdf/2211.09110.pdf) |             |
| 2022-11 | BLOOM                | BigScience         | [BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/pdf/2211.05100.pdf) |             |
| 2022-11 | Galactica            | Meta               | [Galactica: A Large Language Model for Science](https://arxiv.org/pdf/2211.09085.pdf) |             |
| 2022-12 | OPT-IML              | Meta               | [OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization](https://arxiv.org/pdf/2212.12017) |             |
| 2023-01 | Flan 2022 Collection | Google             | [The Flan Collection: Designing Data and Methods for Effective Instruction Tuning](https://arxiv.org/pdf/2301.13688.pdf) |             |
| 2023-02 | LLaMA                | Meta               | [LLaMA: Open and Efficient Foundation Language Models](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/) |             |
| 2023-02 | Kosmos-1             | Microsoft          | [Language Is Not All You Need: Aligning Perception with Language Models](https://arxiv.org/abs/2302.14045) |             |
| 2023-03 | PaLM-E               | Google             | [PaLM-E: An Embodied Multimodal Language Model](https://palm-e.github.io/) |             |
| 2023-03 | GPT 4                | OpenAI             | [GPT-4 Technical Report](https://openai.com/research/gpt-4)  |             |

## Foundation Models

|Title|Venue|Date|Code|Demo|
|---|---|---|---|---|
|[**Pixtral-12B**](https://mistral.ai/news/pixtral-12b/)|Mistral|2024-09-17|-|-|
|![Star](https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&label=Star) [**xGen-MM (BLIP-3): A Family of Open Large Multimodal Models**](https://arxiv.org/pdf/2408.08872)|arXiv|2024-08-16|[Github](https://github.com/salesforce/LAVIS/tree/xgen-mm)|-|
|[**The Llama 3 Herd of Models**](https://arxiv.org/pdf/2407.21783)|arXiv|2024-07-31|-|-|
|[**Chameleon: Mixed-Modal Early-Fusion Foundation Models**](https://arxiv.org/pdf/2405.09818)|arXiv|2024-05-16|-|-|
|[**Hello GPT-4o**](https://openai.com/index/hello-gpt-4o/)|OpenAI|2024-05-13|-|-|
|[**The Claude 3 Model Family: Opus, Sonnet, Haiku**](https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf)|Anthropic|2024-03-04|-|-|
|[**Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context**](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf)|Google|2024-02-15|-|-|
|[**Gemini: A Family of Highly Capable Multimodal Models**](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf)|Google|2023-12-06|-|-|
|[**Fuyu-8B: A Multimodal Architecture for AI Agents**](https://www.adept.ai/blog/fuyu-8b)|blog|2023-10-17|[Huggingface](https://huggingface.co/adept/fuyu-8b)|[Demo](https://huggingface.co/adept/fuyu-8b)|
|![Star](https://img.shields.io/github/stars/mshukor/UnIVAL.svg?style=social&label=Star) [**Unified Model for Image, Video, Audio and Language Tasks**](https://arxiv.org/pdf/2307.16184.pdf)|arXiv|2023-07-30|[Github](https://github.com/mshukor/UnIVAL)|[Demo](https://huggingface.co/spaces/mshukor/UnIVAL)|
|[**PaLI-3 Vision Language Models: Smaller, Faster, Stronger**](https://arxiv.org/pdf/2310.09199.pdf)|arXiv|2023-10-13|-|-|
|[**GPT-4V(ision) System Card**](https://cdn.openai.com/papers/GPTV_System_Card.pdf)|OpenAI|2023-09-25|-|-|
|![Star](https://img.shields.io/github/stars/jy0205/LaVIT.svg?style=social&label=Star) [**Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization**](https://arxiv.org/pdf/2309.04669.pdf)|arXiv|2023-09-09|[Github](https://github.com/jy0205/LaVIT)|-|
|[**Multimodal Foundation Models: From Specialists to General-Purpose Assistants**](https://browse.arxiv.org/pdf/2309.10020.pdf)|arXiv|2023-09-18|-|-|
|![Star](https://img.shields.io/github/stars/yiren-jian/BLIText.svg?style=social&label=Star) [**Bootstrapping Vision-Language Learning with Decoupled Language Pre-training**](https://arxiv.org/pdf/2307.07063.pdf)|NeurIPS|2023-07-13|[Github](https://github.com/yiren-jian/BLIText)|-|
|![Star](https://img.shields.io/github/stars/baaivision/Emu.svg?style=social&label=Star) [**Generative Pretraining in Multimodality**](https://arxiv.org/pdf/2307.05222.pdf)|arXiv|2023-07-11|[Github](https://github.com/baaivision/Emu)|[Demo](http://218.91.113.230:9002/)|
|![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star) [**Kosmos-2: Grounding Multimodal Large Language Models to the World**](https://arxiv.org/pdf/2306.14824.pdf)|arXiv|2023-06-26|[Github](https://github.com/microsoft/unilm/tree/master/kosmos-2)|[Demo](https://aka.ms/kosmos-2-demo)|
|![Star](https://img.shields.io/github/stars/VPGTrans/VPGTrans.svg?style=social&label=Star) [**Transfer Visual Prompt Generator across LLMs**](https://arxiv.org/pdf/2305.01278.pdf)|arXiv|2023-05-02|[Github](https://github.com/VPGTrans/VPGTrans)|[Demo](https://3fc7715dbc44234a7f.gradio.live/)|
|[**GPT-4 Technical Report**](https://arxiv.org/pdf/2303.08774.pdf)|arXiv|2023-03-15|-|-|
|[**PaLM-E: An Embodied Multimodal Language Model**](https://arxiv.org/pdf/2303.03378.pdf)|arXiv|2023-03-06|-|[Demo](https://palm-e.github.io/#demo)|
|![Star](https://img.shields.io/github/stars/NVlabs/prismer.svg?style=social&label=Star) [**Prismer: A Vision-Language Model with An Ensemble of Experts**](https://arxiv.org/pdf/2303.02506.pdf)|arXiv|2023-03-04|[Github](https://github.com/NVlabs/prismer)|[Demo](https://huggingface.co/spaces/lorenmt/prismer)|
|![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star) [**Language Is Not All You Need: Aligning Perception with Language Models**](https://arxiv.org/pdf/2302.14045.pdf)|arXiv|2023-02-27|[Github](https://github.com/microsoft/unilm)|-|
|![Star](https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&label=Star) [**BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models**](https://arxiv.org/pdf/2301.12597.pdf)|arXiv|2023-01-30|[Github](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)|[Demo](https://colab.research.google.com/github/salesforce/LAVIS/blob/main/examples/blip2_instructed_generation.ipynb)|
|![Star](https://img.shields.io/github/stars/vimalabs/VIMA.svg?style=social&label=Star) [**VIMA: General Robot Manipulation with Multimodal Prompts**](https://arxiv.org/pdf/2210.03094.pdf)|ICML|2022-10-06|[Github](https://github.com/vimalabs/VIMA)|Local Demo|
|![Star](https://img.shields.io/github/stars/MineDojo/MineDojo.svg?style=social&label=Star) [**MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge**](https://arxiv.org/pdf/2206.08853.pdf)|NeurIPS|2022-06-17|[Github](https://github.com/MineDojo/MineDojo)|-|
|![Star](https://img.shields.io/github/stars/shizhediao/DaVinci.svg?style=social&label=Star) [**Write and Paint: Generative Vision-Language Models are Unified Modal Learners**](https://arxiv.org/pdf/2206.07699.pdf)|ICLR|2022-06-15|[Github](https://github.com/shizhediao/DaVinci)|-|
|![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star) [**Language Models are General-Purpose Interfaces**](https://arxiv.org/pdf/2206.06336.pdf)|arXiv|2022-06-13|[Github](https://github.com/microsoft/unilm)|-|