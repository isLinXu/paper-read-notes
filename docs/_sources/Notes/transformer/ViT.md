# ViT

**标题：** An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale

**作者：** Alexey Dosovitskiy, Lucas Beyer, 等，来自 Google Research。

**摘要：**
- 论文提出了一种新的图像识别方法，使用纯 Transformer 架构直接应用于图像分类任务。
- 研究表明，在大规模数据集上预训练后，Vision Transformer (ViT) 可以在多个图像识别基准测试中取得与最先进的卷积神经网络 (CNN) 相当或更好的结果，同时在训练时需要的计算资源更少。

**1. 工作内容与动机：**
- 动机：Transformer 架构在自然语言处理 (NLP) 领域取得了巨大成功，但在计算机视觉领域的应用受到限制。传统上，注意力机制被用来辅助卷积网络或替代卷积网络的某些部分。本文的动机是探索纯 Transformer 在图像识别任务中的潜力。

**2. 试图解决的问题：**
- 解决的问题是 Transformer 在计算机视觉任务中应用的局限性，尤其是在大规模图像识别任务中。

**3. 是否是新问题：**
- 不完全是新问题，但在图像识别任务中使用纯 Transformer 是一种新的尝试。

**4. 科学假设：**
- 假设纯 Transformer 架构可以直接应用于图像数据，并在大规模数据集上通过预训练达到与 CNN 相媲美的性能。

**5. 相关研究：**
- 相关研究包括在 NLP 中成功的 Transformer 模型，以及在计算机视觉中尝试结合 CNN 和自注意力机制的研究。
- 归类：主要归类于图像识别和模型架构创新。
- 值得关注的研究员：论文作者团队，以及在 NLP 和 CV 领域内对 Transformer 有贡献的研究者。

**6. 解决方案的关键：**
- 关键是将图像分割成固定大小的 patches，并将这些 patches 作为序列输入到 Transformer 模型中。此外，大规模数据集上的预训练也是成功的关键因素。

**7. 实验设计：**
- 实验设计包括在不同规模的数据集（如 ImageNet, ImageNet-21k, JFT-300M）上预训练 ViT，并在多个基准测试（如 ImageNet, CIFAR-100, VTAB 等）上评估其性能。

**8. 定量评估的数据集与代码开源情况：**
- 使用了 ImageNet、CIFAR-100、VTAB 等多个图像识别基准数据集。
- 代码已在 GitHub 上开源：https://github.com/google-research/vision_transformer。

**9. 实验结果与科学假设的支持：**
- 实验结果表明，经过大规模预训练的 ViT 在多个图像识别任务上取得了优异的性能，支持了科学假设。

**10. 论文贡献：**
- 提出了一种新的图像识别方法，证明了纯 Transformer 架构在图像识别任务中的有效性。
- 展示了在大规模数据集上预训练的重要性，并提供了一种相对于传统 CNN 更加节省计算资源的训练方法。

**11. 下一步工作：**
- 将 ViT 应用于其他计算机视觉任务，如目标检测和分割。
- 探索自监督学习方法，以进一步提高模型的性能和泛化能力。
- 继续扩展模型规模，以实现更高的性能。

### 回答问题

1. **这篇论文做了什么工作，它的动机是什么？**
   - 论文提出了一种新的图像识别方法，使用纯 Transformer 架构直接应用于图像分类任务。动机是探索 Transformer 在计算机视觉领域的潜力，并减少对 CNN 的依赖。

2. **这篇论文试图解决什么问题？**
   - 试图解决 Transformer 在计算机视觉任务中应用的局限性，尤其是在大规模图像识别任务中。

3. **这是否是一个新的问题？**
   - 不完全是新问题，但在图像识别任务中使用纯 Transformer 是一种新的尝试。

4. **这篇文章要验证一个什么科学假设？**
   - 验证纯 Transformer 架构可以直接应用于图像数据，并在大规模数据集上通过预训练达到与 CNN 相媲美的性能。

5. **有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？**
   - 相关研究包括在 NLP 中成功的 Transformer 模型，以及在计算机视觉中尝试结合 CNN 和自注意力机制的研究。归类于图像识别和模型架构创新。值得关注的研究员包括论文作者团队和在 NLP 和 CV 领域内对 Transformer 有贡献的研究者。

6. **论文中提到的解决方案之关键是什么？**
   - 解决方案的关键是将图像分割成固定大小的 patches 作为序列输入到 Transformer 模型中，并在大规模数据集上进行预训练。

7. **论文中的实验是如何设计的？**
   - 实验设计包括在不同规模的数据集上预训练 ViT，并在多个基准测试上评估其性能。

8. **用于定量评估的数据集上什么？代码有没有开源？**
   - 使用了 ImageNet、CIFAR-100、VTAB 等多个图像识别基准数据集。代码已在 GitHub 上开源。

9. **论文中的实验及结果有没有很好地支持需要验证的科学假设？**
   - 是的，实验结果表明经过大规模预训练的 ViT 在多个图像识别任务上取得了优异的性能，支持了科学假设。

10. **这篇论文到底有什么贡献？**
    - 提出了一种新的图像识别方法，证明了纯 Transformer 架构在图像识别任务中的有效性，并展示了在大规模数据集上预训练的重要性。

11. **下一步呢？有什么工作可以继续深入？**
    - 将 ViT 应用于其他计算机视觉任务，探索自监督学习方法，以及继续扩展模型规模以实现更高的性能。

---

<img width="837" alt="VIT-fig1" src="https://github.com/isLinXu/issues/assets/59380685/549e73d2-b95e-4448-9d15-b26a9e2fe3ce">

这个图表展示了视觉Transformer（Vision Transformer, ViT）的模型结构，并结合了Transformer编码器的详细结构。以下是对图表结构的详细分析和总结：

视觉Transformer（Vision Transformer, ViT）

1. **输入图像**：
   - 输入图像被分割成固定大小的图像块（patches），每个图像块被线性嵌入（linearly embed）。

2. **位置嵌入（Position Embedding）**：
   - 对每个图像块添加位置嵌入，以保留图像块的位置信息。

3. **线性投影（Linear Projection）**：
   - 将嵌入后的图像块进行线性投影，得到一系列向量序列。

4. **分类标记（Classification Token）**：
   - 在向量序列中添加一个可学习的分类标记（classification token），用于最终的分类任务。

5. **Transformer编码器**：
   - 将处理后的向量序列输入到标准的Transformer编码器中。

6. **MLP头（MLP Head）**：
   - Transformer编码器的输出经过一个多层感知机（MLP）头，用于最终的分类任务。

Transformer编码器

1. **嵌入的图像块（Embedded Patches）**：
   - 输入的嵌入图像块序列。

2. **多头注意力（Multi-Head Attention）**：
   - 输入序列经过多头注意力机制，捕捉不同位置之间的依赖关系。

3. **规范化（Norm）**：
   - 多头注意力的输出经过规范化处理。

4. **多层感知机（MLP）**：
   - 规范化后的输出经过多层感知机进行进一步处理。

5. **层叠（Lx）**：
   - 以上过程在多个层中重复进行。

摘要内容总结

- **图像分割和嵌入**：
  - 输入图像被分割成固定大小的图像块，每个图像块被线性嵌入，并添加位置嵌入。

- **向量序列生成**：
  - 嵌入后的图像块经过线性投影，生成一系列向量序列，并添加一个可学习的分类标记。

- **Transformer编码器处理**：
  - 向量序列输入到标准的Transformer编码器中，经过多头注意力机制和多层感知机的处理。

- **分类任务**：
  - Transformer编码器的输出经过一个多层感知机头，用于最终的分类任务。

结论

视觉Transformer（ViT）通过将输入图像分割成固定大小的图像块，并将这些图像块嵌入到向量序列中，利用Transformer编码器进行处理。通过多头注意力机制和多层感知机的处理，ViT能够捕捉图像中不同位置之间的依赖关系，并最终用于分类任务。ViT的设计灵感来源于Vaswani等人（2017年）提出的Transformer模型，展示了Transformer在计算机视觉任务中的强大应用潜力。


---

<img width="824" alt="vit-fig2" src="https://github.com/isLinXu/issues/assets/59380685/5b31caa8-5b80-4ef3-b7e4-6fca35b2b0d4">

这个图表展示了不同模型在VTAB（Visual Task Adaptation Benchmark）中的表现，具体分为自然任务（Natural tasks）、专门任务（Specialized tasks）和结构化任务（Structured tasks）三个组别。图表通过柱状图的形式对比了四种模型在这些任务组别中的准确率（Accuracy）。以下是对图表结构的详细分析和总结：

图表结构分析

1. **模型对比**：
   - 图表中对比了四种模型的表现：
     - ViT-H/14
     - BiT-L (R152x4)
     - VIVI-Ex-100ft (R50x3)
     - S4L (R50x1)

2. **任务组别**：
   - 图表分为四个子图，分别展示了不同任务组别中的表现：
     - **VTAB（19个任务）**：总体表现
     - **Natural（7个任务）**：自然任务组
     - **Specialized（4个任务）**：专门任务组
     - **Structured（8个任务）**：结构化任务组

3. **准确率（Accuracy）**：
   - Y轴表示准确率（百分比）。
   - 每个子图中的柱状图表示不同模型在对应任务组别中的准确率。

总结

1. **总体表现（VTAB 19个任务）**：
   - ViT-H/14模型在总体任务中表现最佳，准确率接近80%。
   - BiT-L (R152x4)和VIVI-Ex-100ft (R50x3)的表现相近，准确率在75%左右。
   - S4L (R50x1)的表现最差，准确率在65%左右。

2. **自然任务（Natural 7个任务）**：
   - BiT-L (R152x4)在自然任务中表现最佳，准确率接近80%。
   - ViT-H/14和VIVI-Ex-100ft (R50x3)的表现相近，准确率在75%左右。
   - S4L (R50x1)的表现最差，准确率在70%左右。

3. **专门任务（Specialized 4个任务）**：
   - ViT-H/14在专门任务中表现最佳，准确率接近88%。
   - BiT-L (R152x4)和VIVI-Ex-100ft (R50x3)的表现相近，准确率在85%左右。
   - S4L (R50x1)的表现最差，准确率在82%左右。

4. **结构化任务（Structured 8个任务）**：
   - ViT-H/14在结构化任务中表现最佳，准确率接近60%。
   - BiT-L (R152x4)和VIVI-Ex-100ft (R50x3)的表现相近，准确率在55%左右。
   - S4L (R50x1)的表现最差，准确率在50%左右。

结论

从图表中可以看出，ViT-H/14模型在总体任务、专门任务和结构化任务中表现最佳，而BiT-L (R152x4)在自然任务中表现最佳。总体来看，ViT-H/14模型在各个任务组别中的表现都较为优异，显示了其在视觉任务中的强大能力。BiT-L (R152x4)和VIVI-Ex-100ft (R50x3)的表现相对接近，而S4L (R50x1)在所有任务组别中的表现都相对较差。


---

<img width="410" alt="vit-fig3" src="https://github.com/isLinXu/issues/assets/59380685/6b04520d-afe0-41bf-aa7e-afd40c7be55b">

这个图表展示了不同模型在ImageNet数据集上的Top-1准确率（Accuracy），并且对比了这些模型在不同预训练数据集（Pre-training dataset）上的表现。图表通过折线图和散点图的形式展示了不同模型在ImageNet、ImageNet-21k和JFT-300M预训练数据集上的表现。以下是对图表结构的详细分析和总结：

图表结构分析

1. **Y轴：ImageNet Top-1准确率（Accuracy）**：
   - Y轴表示模型在ImageNet数据集上的Top-1准确率，范围从70%到90%。

2. **X轴：预训练数据集（Pre-training dataset）**：
   - X轴表示不同的预训练数据集，包括ImageNet、ImageNet-21k和JFT-300M。

3. **模型对比**：
   - 图表中对比了五种模型的表现：
     - BiT（灰色线和方形标记）
     - ViT-L/32（绿色线和圆形标记）
     - ViT-B/32（蓝色线和圆形标记）
     - ViT-B/16（紫色线和圆形标记）
     - ViT-L/16（橙色线和圆形标记）
     - ViT-H/14（橙色线和圆形标记）

4. **数据点和误差线**：
   - 每个数据点表示模型在对应预训练数据集上的准确率。
   - 误差线表示准确率的标准误差。

总结

1. **小数据集上的表现**：
   - 在ImageNet预训练数据集上，BiT模型的表现优于所有ViT模型，准确率接近80%。
   - ViT模型中，ViT-B/16和ViT-B/32的表现相对较好，准确率在75%左右。
   - ViT-L/32和ViT-H/14的表现较差，准确率在70%左右。

2. **中等数据集上的表现**：
   - 在ImageNet-21k预训练数据集上，BiT模型的表现依然优于大多数ViT模型，但差距有所缩小。
   - ViT-B/16和ViT-L/16的表现有所提升，准确率接近80%。
   - ViT-H/14的表现显著提升，准确率超过80%。

3. **大数据集上的表现**：
   - 在JFT-300M预训练数据集上，ViT模型的表现显著提升，尤其是较大的ViT模型。
   - ViT-H/14的表现最佳，准确率接近90%。
   - ViT-L/16和ViT-B/16的表现也显著提升，准确率在85%左右。
   - BiT模型的表现相对稳定，准确率在80%左右。

结论

从图表中可以看出，ViT模型在小数据集（如ImageNet）上表现不如BiT模型，但随着预训练数据集的增大，ViT模型的表现显著提升，尤其是较大的ViT模型（如ViT-H/14和ViT-L/16）。在大数据集（如JFT-300M）上，ViT模型的表现超过了BiT模型，显示了其在大规模数据集上的强大能力。总体来看，ViT模型在大数据集上的表现优于小数据集，而BiT模型在小数据集上的表现相对稳定。

---

<img width="404" alt="vit-fig4" src="https://github.com/isLinXu/issues/assets/59380685/13a4777e-1335-45af-8ecb-4e92630941bf">

这个图表展示了不同模型在ImageNet数据集上的线性5-shot评估Top-1准确率（Accuracy），并对比了这些模型在不同数量的JFT预训练样本（Number of JFT pre-training samples）上的表现。图表通过折线图的形式展示了不同模型在10M、30M、100M和300M预训练样本上的表现。以下是对图表结构的详细分析和总结：

图表结构分析

1. **Y轴：线性5-shot ImageNet Top-1准确率（Accuracy）**：
   - Y轴表示模型在ImageNet数据集上的线性5-shot评估Top-1准确率，范围从40%到70%。

2. **X轴：JFT预训练样本数量（Number of JFT pre-training samples）**：
   - X轴表示不同数量的JFT预训练样本，包括10M、30M、100M和300M。

3. **模型对比**：
   - 图表中对比了五种模型的表现：
     - ViT-L/16（浅蓝色线和圆形标记）
     - ViT-B/32（深蓝色线和圆形标记）
     - ViT-L/32（绿色线和圆形标记）
     - ViT-b/32（浅绿色线和圆形标记）
     - ResNet50x1 (BiT)（灰色线和方形标记）
     - ResNet152x2 (BiT)（黄色线和方形标记）

总结

1. **小预训练数据集上的表现（10M和30M样本）**：
   - 在10M和30M预训练样本上，ResNet模型（ResNet50x1和ResNet152x2）的表现优于所有ViT模型。
   - ResNet50x1的表现最佳，准确率在10M样本上接近60%，在30M样本上接近65%。
   - ViT模型中，ViT-B/32和ViT-L/32的表现相对较好，但仍低于ResNet模型。

2. **中等预训练数据集上的表现（100M样本）**：
   - 在100M预训练样本上，ResNet模型的表现趋于平稳，ResNet50x1的准确率接近65%，ResNet152x2的准确率在60%左右。
   - ViT模型的表现显著提升，尤其是ViT-L/16和ViT-B/32，准确率接近65%。

3. **大预训练数据集上的表现（300M样本）**：
   - 在300M预训练样本上，ViT模型的表现超过了ResNet模型。
   - ViT-L/16的表现最佳，准确率接近70%。
   - ViT-B/32和ViT-L/32的表现也显著提升，准确率在65%左右。
   - ResNet模型的表现趋于平稳，未见显著提升。

结论

从图表中可以看出，ResNet模型在小预训练数据集（如10M和30M样本）上表现优于ViT模型，但随着预训练数据集的增大，ResNet模型的表现趋于平稳，未见显著提升。而ViT模型在大预训练数据集（如100M和300M样本）上表现显著提升，尤其是较大的ViT模型（如ViT-L/16和ViT-B/32）。总体来看，ViT模型在大规模预训练数据集上的表现优于ResNet模型，显示了其在大数据集上的强大能力。

---

<img width="831" alt="vit-fig5" src="https://github.com/isLinXu/issues/assets/59380685/d0f2bc2d-88d1-4c93-9c88-9119392c0af3">

这个图表展示了不同架构（Vision Transformers、ResNets和Hybrids）在不同预训练计算量（Total pre-training compute）下的迁移准确率（Transfer accuracy）。图表通过散点图的形式展示了这些模型在Average-5和ImageNet数据集上的表现。以下是对图表结构的详细分析和总结：

图表结构分析

1. **Y轴：迁移准确率（Transfer accuracy）**：
   - Y轴表示模型在Average-5和ImageNet数据集上的迁移准确率，范围从75%到90%。

2. **X轴：总预训练计算量（Total pre-training compute）**：
   - X轴表示总预训练计算量，以exaFLOPs为单位，范围从10^2到10^4。

3. **模型对比**：
   - 图表中对比了三种架构的表现：
     - Transformer (ViT)（蓝色圆形标记）
     - ResNet (BiT)（灰色方形标记）
     - Hybrid（橙色菱形标记）

4. **子图**：
   - 图表分为两个子图，分别展示了不同架构在Average-5和ImageNet数据集上的表现：
     - **Average-5**：表示在五个不同任务上的平均迁移准确率。
     - **ImageNet**：表示在ImageNet数据集上的迁移准确率。

总结

1. **Average-5数据集上的表现**：
   - Transformer (ViT)模型在相同计算预算下通常优于ResNet (BiT)模型。
   - Hybrid模型在较小的计算预算下（10^2到10^3 exaFLOPs）表现优于纯Transformer模型，但随着计算预算的增加（超过10^3 exaFLOPs），两者的表现差距逐渐缩小。
   - Transformer (ViT)模型在较大计算预算下（超过10^3 exaFLOPs）表现最佳，迁移准确率接近90%。

2. **ImageNet数据集上的表现**：
   - Transformer (ViT)模型在相同计算预算下通常优于ResNet (BiT)模型。
   - Hybrid模型在较小的计算预算下（10^2到10^3 exaFLOPs）表现优于纯Transformer模型，但随着计算预算的增加（超过10^3 exaFLOPs），两者的表现差距逐渐缩小。
   - Transformer (ViT)模型在较大计算预算下（超过10^3 exaFLOPs）表现最佳，迁移准确率接近90%。

结论

从图表中可以看出，Vision Transformers（ViT）模型在相同计算预算下通常优于ResNet（BiT）模型，显示了其在迁移学习任务中的强大能力。Hybrid模型在较小的计算预算下表现优于纯Transformer模型，但随着计算预算的增加，两者的表现差距逐渐缩小，最终在较大计算预算下（超过10^3 exaFLOPs）表现相近。总体来看，Transformer (ViT)模型在较大计算预算下表现最佳，适用于需要高迁移准确率的任务。


---

<img width="283" alt="vit-fig6" src="https://github.com/isLinXu/issues/assets/59380685/c59c3de2-2990-4333-97b9-6173c93f36da">

从图表中可以看出，模型在处理图像时能够有效地关注到图像中最重要的部分。例如，在狗的图像中，模型主要关注狗的头部和身体；在飞机的图像中，模型主要关注飞机的机身和机翼；在蛇的图像中，模型主要关注蛇的身体和头部。这表明模型的注意力机制能够有效地捕捉到图像中的关键特征，从而提高图像识别的准确性。总体来看，注意力机制在图像处理任务中具有重要作用，能够帮助模型更好地理解和识别图像内容




---

<img width="844" alt="vit-fig7" src="https://github.com/isLinXu/issues/assets/59380685/2446adfb-a2ec-47c6-b52a-b667ca42267c">

这个图表展示了ViT-L/32和ViT-L/16模型在不同方面的表现和特性。图表分为三个部分：左侧展示了RGB嵌入滤波器，中间展示了位置嵌入的相似性，右侧展示了ViT-L/16模型中不同头和网络深度的注意力距离。以下是对图表结构的详细分析和总结：

图表结构分析

1. **左侧（RGB嵌入滤波器）**：
   - **内容**：展示了ViT-L/32模型中初始线性嵌入的RGB值滤波器的前28个主成分。
   - **作用**：这些滤波器用于将输入图像的RGB值嵌入到模型的初始表示中。

2. **中间（位置嵌入相似性）**：
   - **内容**：展示了ViT-L/32模型中位置嵌入的余弦相似性。
   - **图示**：每个方块表示特定行和列的补丁位置嵌入与所有其他补丁位置嵌入的余弦相似性。
   - **作用**：用于展示模型如何理解和表示图像中不同位置的关系。

3. **右侧（ViT-L/16的注意力距离）**：
   - **内容**：展示了ViT-L/16模型中不同头和网络深度的注意力距离。
   - **图示**：每个点表示某一层中16个头之一的平均注意力距离。
   - **颜色**：不同颜色表示不同的头（Head 1、Head 2、Head 3）。
   - **作用**：用于展示模型在不同层和不同头中的注意力分布情况。

总结

1. **RGB嵌入滤波器（左侧）**：
   - 这些滤波器展示了模型如何将输入图像的RGB值嵌入到初始表示中。前28个主成分显示了模型在初始阶段捕捉到的主要特征。

2. **位置嵌入相似性（中间）**：
   - 位置嵌入的余弦相似性图展示了模型如何理解图像中不同位置之间的关系。高相似性表示模型认为这些位置在某种程度上是相似的或相关的。
   - 从图中可以看出，位置嵌入的相似性在对角线上较高，表示模型对相邻位置有较高的相似性理解。

3. **注意力距离（右侧）**：
   - ViT-L/16模型中不同头和网络深度的注意力距离图展示了模型在不同层和不同头中的注意力分布情况。
   - 注意力距离随着网络深度的增加而变化，不同头的注意力距离也有所不同。这表明模型在不同层和不同头中捕捉到的特征和关系是不同的。
   - 颜色区分的不同头展示了每个头在不同层中的注意力分布情况，显示了模型在处理图像时的多样性和复杂性。

结论

从图表中可以看出，ViT模型在处理图像时通过RGB嵌入滤波器、位置嵌入和注意力机制来捕捉和理解图像中的特征和关系。RGB嵌入滤波器展示了模型在初始阶段捕捉到的主要特征，位置嵌入相似性展示了模型对图像中不同位置关系的理解，而注意力距离展示了模型在不同层和不同头中的注意力分布情况。这些机制共同作用，使得ViT模型能够有效地处理和理解图像数据。


---

<img width="838" alt="vit-fig8" src="https://github.com/isLinXu/issues/assets/59380685/329ab285-1c76-475e-95b0-fcb5f5f5f4c6">

这个图表展示了Vision Transformer (ViT)模型在不同维度上的扩展效果。图表通过两个子图展示了不同模型维度（深度、补丁大小、MLP宽度、宽度）在相对计算量（Relative Compute）下的表现。以下是对图表结构的详细分析和总结：

图表结构分析

1. **Y轴**：
   - 左侧子图：表示ImageNet 5-Shot的性能，范围从0.2到0.6。
   - 右侧子图：表示Average Shot的性能，范围从0.4到0.8。

2. **X轴**：
   - 两个子图的X轴均表示相对计算量（Relative Compute），范围从10^0到10^1。

3. **模型维度**：
   - 图表中对比了五种不同模型维度的扩展效果：
     - All（蓝色圆形标记）：所有维度的扩展。
     - Depth（橙色叉形标记）：深度的扩展。
     - Patch size（绿色方形标记）：补丁大小的扩展。
     - Width MLP（红色三角形标记）：MLP宽度的扩展。
     - Width（紫色菱形标记）：宽度的扩展。

总结

1. **ImageNet 5-Shot（左侧子图）**：
   - **All**：所有维度的扩展在相对计算量增加时表现出稳定的性能提升，最终达到约0.6的性能。
   - **Depth**：深度的扩展在相对计算量较低时表现较差，但在相对计算量增加时性能提升显著，最终接近0.6。
   - **Patch size**：补丁大小的扩展在相对计算量增加时性能提升较为平稳，最终达到约0.5的性能。
   - **Width MLP**：MLP宽度的扩展在相对计算量增加时性能提升较为平稳，最终达到约0.5的性能。
   - **Width**：宽度的扩展在相对计算量较低时表现较差，但在相对计算量增加时性能提升显著，最终接近0.6。

2. **Average Shot（右侧子图）**：
   - **All**：所有维度的扩展在相对计算量增加时表现出稳定的性能提升，最终达到约0.8的性能。
   - **Depth**：深度的扩展在相对计算量较低时表现较差，但在相对计算量增加时性能提升显著，最终接近0.8。
   - **Patch size**：补丁大小的扩展在相对计算量增加时性能提升较为平稳，最终达到约0.7的性能。
   - **Width MLP**：MLP宽度的扩展在相对计算量增加时性能提升较为平稳，最终达到约0.7的性能。
   - **Width**：宽度的扩展在相对计算量较低时表现较差，但在相对计算量增加时性能提升显著，最终接近0.8。

结论

从图表中可以看出，Vision Transformer (ViT)模型在不同维度上的扩展效果各不相同。总体来看，所有维度的扩展（All）在相对计算量增加时表现出最稳定和显著的性能提升。深度（Depth）和宽度（Width）的扩展在相对计算量增加时也表现出显著的性能提升，特别是在相对计算量较高时。补丁大小（Patch size）和MLP宽度（Width MLP）的扩展在相对计算量增加时性能提升较为平稳，但最终性能略低于深度和宽度的扩展。

这些结果表明，在扩展ViT模型时，综合考虑所有维度的扩展能够带来最显著的性能提升，而单独扩展深度或宽度也能带来较好的效果。补丁大小和MLP宽度的扩展虽然也能提升性能，但效果相对较弱。


---

<img width="840" alt="vit-fig9" src="https://github.com/isLinXu/issues/assets/59380685/b630e262-b621-44a4-9340-e8ccd30114c9">

这个图表展示了在ImageNet数据集上，使用不同分类器（class-token和global average pooling）以及不同学习率（learning rates）进行训练时的5-shot准确率随训练轮数（epochs of training）的变化情况。以下是对图表结构的详细分析和总结：

图表结构分析

1. **Y轴**：
   - 表示ImageNet 5-shot准确率（百分比），范围从0%到60%。

2. **X轴**：
   - 表示训练轮数（epochs of training），范围从0到7。

3. **曲线**：
   - 图表中有三条曲线，分别表示不同分类器和学习率的组合：
     - **CLS-Token, lr=8e-4**（蓝色曲线）：使用class-token分类器，学习率为8e-4。
     - **GAP, lr=8e-4**（橙色曲线）：使用global average pooling分类器，学习率为8e-4。
     - **GAP, lr=3e-4**（绿色曲线）：使用global average pooling分类器，学习率为3e-4。

总结

1. **CLS-Token, lr=8e-4（蓝色曲线）**：
   - 在训练初期（前1个epoch），准确率迅速上升，随后逐渐平稳增长。
   - 最终在第7个epoch时，准确率接近50%。

2. **GAP, lr=8e-4（橙色曲线）**：
   - 在训练初期（前1个epoch），准确率迅速上升，但随后增长速度减缓，并在第2个epoch后出现波动。
   - 最终在第7个epoch时，准确率约为45%。

3. **GAP, lr=3e-4（绿色曲线）**：
   - 在整个训练过程中，准确率持续稳定增长。
   - 最终在第7个epoch时，准确率接近55%，表现优于其他两种组合。

结论

从图表中可以看出，不同分类器和学习率的组合在训练过程中表现出不同的准确率增长趋势：

1. **分类器对比**：
   - 使用class-token分类器（蓝色曲线）和global average pooling分类器（橙色和绿色曲线）在相同学习率（8e-4）下，class-token分类器的表现略优于global average pooling分类器。
   - 然而，当global average pooling分类器的学习率降低到3e-4时（绿色曲线），其表现显著优于class-token分类器和高学习率的global average pooling分类器。

2. **学习率对比**：
   - 对于global average pooling分类器，较低的学习率（3e-4）显著提高了模型的最终准确率，表现优于较高学习率（8e-4）。

总体来看，虽然class-token和global average pooling分类器在相同学习率下表现相似，但global average pooling分类器在较低学习率下表现更好。这表明在选择分类器和学习率时，需要根据具体情况进行调整，以获得最佳的模型性能。


---

<img width="813" alt="vit-fig10" src="https://github.com/isLinXu/issues/assets/59380685/ce07987e-79e7-4154-ad94-1b61e69fa8ee">

这个图表展示了使用不同超参数训练的模型的位置信嵌入（position embeddings）。图表通过三个子图展示了不同训练轮数、学习率和权重衰减（weight decay）下的位置信嵌入的余弦相似度（cosine similarity）。以下是对图表结构的详细分析和总结：

图表结构分析

1. **子图**：
   - 图表包含三个子图，每个子图表示一种超参数组合下的位置信嵌入：
     - **左侧子图**：ViT-L/16，训练7个epoch，学习率为0.0002，权重衰减为0.01。
     - **中间子图**：ViT-L/16，训练7个epoch，学习率为0.0004，权重衰减为0.1。
     - **右侧子图**：ViT-L/16，训练14个epoch，学习率为0.0004，权重衰减为0.1。

2. **坐标轴**：
   - 每个子图的X轴和Y轴均表示输入补丁的行和列，范围从1到14。
   - 图表中的每个方格表示对应行和列的输入补丁之间的余弦相似度。

3. **颜色表示**：
   - 颜色条从-1（深蓝色）到1（黄色），表示余弦相似度的范围。
   - 颜色越接近黄色，表示余弦相似度越高；颜色越接近深蓝色，表示余弦相似度越低。

总结

1. **左侧子图（7个epoch，学习率0.0002，权重衰减0.01）**：
   - 余弦相似度图显示出较为均匀的分布，颜色主要集中在绿色和黄色之间，表示相似度较高。
   - 这种分布表明模型在训练7个epoch后，位置信嵌入之间的相似度较高。

2. **中间子图（7个epoch，学习率0.0004，权重衰减0.1）**：
   - 余弦相似度图显示出更为均匀的分布，颜色主要集中在绿色和黄色之间，表示相似度较高。
   - 与左侧子图相比，学习率和权重衰减的增加并未显著改变位置信嵌入的相似度分布。

3. **右侧子图（14个epoch，学习率0.0004，权重衰减0.1）**：
   - 余弦相似度图显示出更为均匀和一致的分布，颜色主要集中在绿色和黄色之间，表示相似度较高。
   - 与中间子图相比，增加训练轮数（从7个epoch增加到14个epoch）使得位置信嵌入的相似度分布更加一致。

结论

从图表中可以看出，不同超参数组合对模型的位置信嵌入有一定影响：

1. **训练轮数**：
   - 增加训练轮数（从7个epoch增加到14个epoch）使得位置信嵌入的相似度分布更加一致，表明模型在更多训练轮数下能够更好地学习位置信息。

2. **学习率和权重衰减**：
   - 增加学习率和权重衰减（从0.0002和0.01增加到0.0004和0.1）对位置信嵌入的相似度分布影响不大，表明在一定范围内，学习率和权重衰减的变化对位置信息的学习影响较小。

总体来看，增加训练轮数对位置信息的学习有显著的正面影响，而学习率和权重衰减的变化在一定范围内对位置信息的学习影响较小。


---

<img width="830" alt="vit-fig11" src="https://github.com/isLinXu/issues/assets/59380685/e2f41879-3709-4a21-bfb2-fa1349d7cc07">

这个图表展示了不同网络深度（layer）和注意力头（head）下的平均注意力距离（mean attention distance）。图表通过两个子图展示了两种不同模型（ViT-L/16和R50x1 + ViT-L/16）的注意力距离分布。以下是对图表结构的详细分析和总结：

图表结构分析

1. **子图**：
   - 图表包含两个子图，每个子图表示一种模型的注意力距离分布：
     - **左侧子图**：ViT-L/16模型。
     - **右侧子图**：R50x1 + ViT-L/16模型。

2. **坐标轴**：
   - **X轴**：表示网络深度（layer），范围从0到20。
   - **Y轴**：表示平均注意力距离（mean attention distance），单位为像素，范围从0到120。

3. **数据点**：
   - 每个子图中的数据点表示在某一层的某个注意力头的平均注意力距离。
   - 不同颜色表示不同的注意力头（Head 1、Head 2、Head 3）。

总结

1. **左侧子图（ViT-L/16模型）**：
   - 随着网络深度的增加，平均注意力距离逐渐增加。
   - 在浅层（0到5层），注意力距离较小，约在40到60像素之间。
   - 在深层（15到20层），注意力距离较大，约在100到120像素之间。
   - 不同注意力头的注意力距离分布较为一致，但在某些层次上存在一定的差异。

2. **右侧子图（R50x1 + ViT-L/16模型）**：
   - 随着网络深度的增加，平均注意力距离逐渐增加，但整体分布较为平稳。
   - 在浅层（0到5层），注意力距离较小，约在40到60像素之间。
   - 在深层（15到20层），注意力距离较大，约在100到120像素之间。
   - 不同注意力头的注意力距离分布较为一致，但在某些层次上存在一定的差异。

结论

从图表中可以看出，不同模型在不同网络深度和注意力头下的平均注意力距离分布情况：

1. **网络深度的影响**：
   - 随着网络深度的增加，平均注意力距离逐渐增加。这表明在更深的层次上，模型能够关注到更远的像素区域，从而捕捉到更全局的信息。

2. **模型的影响**：
   - ViT-L/16模型和R50x1 + ViT-L/16模型在注意力距离分布上表现出相似的趋势，但R50x1 + ViT-L/16模型的注意力距离分布更为平稳。这可能是由于R50x1的引入使得模型在浅层就能够捕捉到更多的全局信息，从而在深层的注意力距离分布上表现得更为一致。

3. **注意力头的影响**：
   - 不同注意力头在同一层次上的注意力距离分布较为一致，但在某些层次上存在一定的差异。这表明不同注意力头在捕捉信息时具有一定的多样性，但整体趋势相似。

总体来看，随着网络深度的增加，模型的注意力距离逐渐增加，能够捕捉到更全局的信息。不同模型在注意力距离分布上表现出相似的趋势，但R50x1 + ViT-L/16模型的分布更为平稳。不同注意力头在同一层次上的注意力距离分布较为一致，但存在一定的多样性。


---

<img width="825" alt="vit-fig12" src="https://github.com/isLinXu/issues/assets/59380685/6a3cd205-4176-4ca0-ae85-c5b0153530dd">

这个图表展示了不同架构在不同输入尺寸下的推理速度和每核最大批量大小。图表通过两个子图展示了不同模型（包括ResNet和ViT模型）的性能表现。以下是对图表结构的详细分析和总结：

图表结构分析

1. **子图**：
   - 图表包含两个子图，每个子图表示一种性能指标：
     - **左侧子图**：峰值推理速度（Peak inference speed）。
     - **右侧子图**：每核最大批量大小（Largest per-core batch-size）。

2. **坐标轴**：
   - **左侧子图**：
     - **X轴**：表示输入尺寸（Input size），单位为像素（px），范围从64到512。
     - **Y轴**：表示峰值推理速度（Peak inference speed），单位为图像/秒/核（img/sec/core）。
   - **右侧子图**：
     - **X轴**：表示输入尺寸（Input size），单位为像素（px），范围从64到512。
     - **Y轴**：表示每核最大批量大小（Largest per-core batch-size）。

3. **曲线**：
   - 每个子图中的曲线表示不同模型的性能表现：
     - **R50x1**（灰色虚线）
     - **R50x2**（黑色虚线）
     - **ViT-B/32**（蓝色实线）
     - **ViT-B/16**（红色实线）
     - **ViT-L/32**（紫色实线）
     - **ViT-L/16**（橙色实线）
     - **ViT-H/14**（绿色实线）
     - **R152x4**（棕色虚线）

总结

1. **左侧子图（峰值推理速度）**：
   - 随着输入尺寸的增加，所有模型的峰值推理速度都呈下降趋势。
   - 在较小的输入尺寸（64到128像素）下，ResNet模型（R50x1和R50x2）的推理速度较快。
   - 在较大的输入尺寸（224到512像素）下，ViT模型（如ViT-B/32、ViT-B/16等）的推理速度与ResNet模型相当，甚至在某些情况下更快。

2. **右侧子图（每核最大批量大小）**：
   - 随着输入尺寸的增加，所有模型的每核最大批量大小都呈下降趋势。
   - 在较小的输入尺寸（64到128像素）下，ResNet模型（R50x1和R50x2）的每核最大批量大小较大。
   - 在较大的输入尺寸（224到512像素）下，ViT模型（如ViT-B/32、ViT-B/16等）的每核最大批量大小明显优于ResNet模型，表明ViT模型在内存效率方面表现更好。

结论

从图表中可以看出，不同架构在不同输入尺寸下的推理速度和每核最大批量大小表现情况：

1. **推理速度**：
   - 随着输入尺寸的增加，所有模型的推理速度都下降。
   - 在较小的输入尺寸下，ResNet模型的推理速度较快。
   - 在较大的输入尺寸下，ViT模型的推理速度与ResNet模型相当，甚至在某些情况下更快。

2. **每核最大批量大小**：
   - 随着输入尺寸的增加，所有模型的每核最大批量大小都下降。
   - 在较小的输入尺寸下，ResNet模型的每核最大批量大小较大。
   - 在较大的输入尺寸下，ViT模型的每核最大批量大小明显优于ResNet模型，表明ViT模型在内存效率方面表现更好。

总体来看，ViT模型在较大的输入尺寸下表现出色，不仅在推理速度上与ResNet模型相当，甚至更快，而且在内存效率方面也明显优于ResNet模型。这表明ViT模型在处理大尺寸输入时具有显著的优势。


---

<img width="829" alt="vit-fig13" src="https://github.com/isLinXu/issues/assets/59380685/eda62164-37f0-4b3a-9da1-62447e88f5f1">
这个图表展示了基于轴向注意力（Axial-Attention）模型在ImageNet 5-shot线性任务上的top-1准确率与其计算复杂度（FLOPs）和推理速度的关系。图表通过两个子图展示了不同模型的性能表现。以下是对图表结构的详细分析和总结：

图表结构分析

1. **子图**：
   - 图表包含两个子图，每个子图表示一种性能指标与top-1准确率的关系：
     - **左侧子图**：top-1准确率与总计算量（FLOPs）的关系。
     - **右侧子图**：top-1准确率与峰值推理速度的关系。

2. **坐标轴**：
   - **左侧子图**：
     - **X轴**：表示总计算量（Total compute），单位为GFLOPs（10^9次浮点运算）。
     - **Y轴**：表示top-1准确率（Top-1 accuracy）。
   - **右侧子图**：
     - **X轴**：表示峰值推理速度（Peak inference speed），单位为图像/秒/核（img/sec/core）。
     - **Y轴**：表示top-1准确率（Top-1 accuracy）。

3. **数据点**：
   - 每个子图中的数据点表示不同模型的性能表现：
     - **ViT-B/32**（蓝色圆点）
     - **ViT-B/16**（绿色圆点）
     - **ViT-L/32**（橙色圆点）
     - **ViT-L/16**（棕色圆点）
     - **AxialViT-B/16**（蓝色三角形）
     - **AxialResNet50**（橙色三角形）
     - **ResNet50**（紫色圆点）

总结

1. **左侧子图（top-1准确率与总计算量的关系）**：
   - 随着总计算量的增加，top-1准确率也有所提高。
   - **ResNet50**的计算量最小，但top-1准确率也最低。
   - **ViT-B/32**和**ViT-B/16**的计算量较小，但top-1准确率较高。
   - **AxialViT-B/16**的计算量较大，但top-1准确率最高，表明其在计算量和准确率之间取得了较好的平衡。

2. **右侧子图（top-1准确率与峰值推理速度的关系）**：
   - 随着峰值推理速度的增加，top-1准确率也有所提高。
   - **ResNet50**的推理速度较慢，且top-1准确率最低。
   - **ViT-B/32**和**ViT-B/16**的推理速度较快，且top-1准确率较高。
   - **AxialViT-B/16**的推理速度最快，且top-1准确率最高，表明其在推理速度和准确率之间取得了较好的平衡。

结论

从图表中可以看出，不同模型在计算量、推理速度和top-1准确率之间的关系：

1. **计算量与准确率**：
   - 随着总计算量的增加，top-1准确率有所提高。
   - **AxialViT-B/16**在计算量和准确率之间取得了较好的平衡，计算量较大但准确率最高。

2. **推理速度与准确率**：
   - 随着峰值推理速度的增加，top-1准确率有所提高。
   - **AxialViT-B/16**在推理速度和准确率之间取得了较好的平衡，推理速度最快且准确率最高。

3. **模型比较**：
   - **ResNet50**的计算量和推理速度较低，准确率也最低。
   - **ViT-B/32**和**ViT-B/16**在计算量和推理速度上表现较好，且准确率较高。
   - **AxialViT-B/16**在所有模型中表现最佳，在计算量、推理速度和准确率之间取得了最好的平衡。

总体来看，**AxialViT-B/16**模型在计算量、推理速度和top-1准确率之间表现出色，表明其在处理ImageNet 5-shot线性任务时具有显著的优势。


